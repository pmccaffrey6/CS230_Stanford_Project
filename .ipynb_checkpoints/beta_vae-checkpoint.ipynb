{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1810085c50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vamb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam as Adam\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "EXAMPLE_FASTA_FILE = '2021.01.26_15.46.45_sample_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_inputs_base = os.path.join(BASE_DIR,'example_input_data/new_simulations/camisim_outputs/vamb_inputs')\n",
    "\n",
    "contignames = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'contignames.npz'))\n",
    "\n",
    "lengths = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'lengths.npz'))\n",
    "\n",
    "tnfs = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'tnfs.npz'))\n",
    "    \n",
    "rpkms = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'rpkms.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, mask = vamb.encode.make_dataloader(rpkms, tnfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncontigs, nsamples = dataloader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of contigs to encode: 1342\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of contigs to encode:\", ncontigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DISENTANGLED_VAE(torch.nn.Module):\n",
    "    def __init__(self, nsamples, nhiddens=[512, 512], nlatent=32, alpha=0.15,\n",
    "                 beta=1, dropout=0.2):\n",
    "        \n",
    "\n",
    "        super(DISENTANGLED_VAE, self).__init__()\n",
    "\n",
    "        # Initialize simple attributes\n",
    "        self.usecuda = True\n",
    "        self.nsamples = nsamples\n",
    "        self.ntnf = 103\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.nhiddens = nhiddens\n",
    "        self.nlatent = nlatent\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize lists for holding hidden layers\n",
    "        self.encoderlayers = torch.nn.ModuleList()\n",
    "        self.encodernorms = torch.nn.ModuleList()\n",
    "        self.decoderlayers = torch.nn.ModuleList()\n",
    "        self.decodernorms = torch.nn.ModuleList()\n",
    "\n",
    "        # Add all other hidden layers\n",
    "        for nin, nout in zip([self.nsamples + self.ntnf] + self.nhiddens, self.nhiddens):\n",
    "            self.encoderlayers.append(torch.nn.Linear(nin, nout))\n",
    "            self.encodernorms.append(torch.nn.BatchNorm1d(nout))\n",
    "\n",
    "        # Latent layers\n",
    "        self.mu = torch.nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "        self.logsigma = torch.nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "\n",
    "        # Add first decoding layer\n",
    "        for nin, nout in zip([self.nlatent] + self.nhiddens[::-1], self.nhiddens[::-1]):\n",
    "            self.decoderlayers.append(torch.nn.Linear(nin, nout))\n",
    "            self.decodernorms.append(torch.nn.BatchNorm1d(nout))\n",
    "\n",
    "        # Reconstruction (output) layer\n",
    "        self.outputlayer = torch.nn.Linear(self.nhiddens[0], self.nsamples + self.ntnf)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.dropoutlayer = torch.nn.Dropout(p=self.dropout)\n",
    "\n",
    "        if self.usecuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def _encode(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        # Hidden layers\n",
    "        for encoderlayer, encodernorm in zip(self.encoderlayers, self.encodernorms):\n",
    "            tensor = encodernorm(self.dropoutlayer(self.relu(encoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        # Latent layers\n",
    "        mu = self.mu(tensor)\n",
    "\n",
    "        logsigma = self.softplus(self.logsigma(tensor))\n",
    "\n",
    "        return mu, logsigma\n",
    "\n",
    "    # sample with gaussian noise\n",
    "    def reparameterize(self, mu, logsigma):\n",
    "        epsilon = torch.randn(mu.size(0), mu.size(1))\n",
    "\n",
    "        if self.usecuda:\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        epsilon.requires_grad = True\n",
    "\n",
    "        # See comment above regarding softplus\n",
    "        latent = mu + epsilon * torch.exp(logsigma/2)\n",
    "\n",
    "        return latent\n",
    "\n",
    "    def _decode(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        for decoderlayer, decodernorm in zip(self.decoderlayers, self.decodernorms):\n",
    "            tensor = decodernorm(self.dropoutlayer(self.relu(decoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        reconstruction = self.outputlayer(tensor)\n",
    "\n",
    "        # Decompose reconstruction to depths and tnf signal\n",
    "        depths_out = reconstruction.narrow(1, 0, self.nsamples)\n",
    "        tnf_out = reconstruction.narrow(1, self.nsamples, self.ntnf)\n",
    "\n",
    "        # If multiple samples, apply softmax\n",
    "        if self.nsamples > 1:\n",
    "            depths_out = _softmax(depths_out, dim=1)\n",
    "\n",
    "        return depths_out, tnf_out\n",
    "\n",
    "    def forward(self, depths, tnf):\n",
    "        tensor = torch.cat((depths, tnf), 1)\n",
    "        mu, logsigma = self._encode(tensor)\n",
    "        latent = self.reparameterize(mu, logsigma)\n",
    "        depths_out, tnf_out = self._decode(latent)\n",
    "\n",
    "        return depths_out, tnf_out, mu, logsigma\n",
    "\n",
    "    def calc_loss(self, depths_in, depths_out, tnf_in, tnf_out, mu, logsigma):\n",
    "        # If multiple samples, use cross entropy, else use SSE for abundance\n",
    "        if self.nsamples > 1:\n",
    "            # Add 1e-9 to depths_out to avoid numerical instability.\n",
    "            ce = - ((depths_out + 1e-9).log() * depths_in).sum(dim=1).mean()\n",
    "            ce_weight = (1 - self.alpha) / _log(self.nsamples)\n",
    "        else:\n",
    "            ce = (depths_out - depths_in).pow(2).sum(dim=1).mean()\n",
    "            ce_weight = 1 - self.alpha\n",
    "\n",
    "        sse = (tnf_out - tnf_in).pow(2).sum(dim=1).mean()\n",
    "        kld = -0.5 * (1 + logsigma - mu.pow(2) - logsigma.exp()).sum(dim=1).mean()\n",
    "\n",
    "        sse_weight = self.alpha / self.ntnf\n",
    "        kld_weight = 1 / (self.nlatent * self.beta)\n",
    "        loss = ce * ce_weight + sse * sse_weight + kld * kld_weight\n",
    "\n",
    "        return loss, ce, sse, kld\n",
    "\n",
    "    def trainepoch(self, data_loader, epoch, optimizer, batchsteps, logfile):\n",
    "        self.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_kldloss = 0\n",
    "        epoch_sseloss = 0\n",
    "        epoch_celoss = 0\n",
    "\n",
    "        if epoch in batchsteps:\n",
    "            data_loader = DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size * 2,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True,\n",
    "                                      num_workers=data_loader.num_workers,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        for depths_in, tnf_in in data_loader:\n",
    "            depths_in.requires_grad = True\n",
    "            tnf_in.requires_grad = True\n",
    "\n",
    "            if self.usecuda:\n",
    "                depths_in = depths_in.cuda()\n",
    "                tnf_in = tnf_in.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            depths_out, tnf_out, mu, logsigma = self(depths_in, tnf_in)\n",
    "\n",
    "            loss, ce, sse, kld = self.calc_loss(depths_in, depths_out, tnf_in,\n",
    "                                                  tnf_out, mu, logsigma)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.data.item()\n",
    "            epoch_kldloss += kld.data.item()\n",
    "            epoch_sseloss += sse.data.item()\n",
    "            epoch_celoss += ce.data.item()\n",
    "\n",
    "        print('\\tEpoch: {}\\tLoss: {:.6f}\\tCE: {:.7f}\\tSSE: {:.6f}\\tKLD: {:.4f}\\tBatchsize: {}'.format(\n",
    "              epoch + 1,\n",
    "              epoch_loss / len(data_loader),\n",
    "              epoch_celoss / len(data_loader),\n",
    "              epoch_sseloss / len(data_loader),\n",
    "              epoch_kldloss / len(data_loader),\n",
    "              data_loader.batch_size,\n",
    "              ), file=logfile)\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def encode(self, data_loader):\n",
    "        \"\"\"Encode a data loader to a latent representation with VAE\n",
    "        Input: data_loader: As generated by train_vae\n",
    "        Output: A (n_contigs x n_latent) Numpy array of latent repr.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        new_data_loader = DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False,\n",
    "                                      num_workers=1,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        depths_array, tnf_array = data_loader.dataset.tensors\n",
    "        length = len(depths_array)\n",
    "\n",
    "        # We make a Numpy array instead of a Torch array because, if we create\n",
    "        # a Torch array, then convert it to Numpy, Numpy will believe it doesn't\n",
    "        # own the memory block, and array resizes will not be permitted.\n",
    "        latent = np.empty((length, self.nlatent), dtype=np.float32)\n",
    "\n",
    "        row = 0\n",
    "        with torch.no_grad():\n",
    "            for depths, tnf in new_data_loader:\n",
    "                # Move input to GPU if requested\n",
    "                if self.usecuda:\n",
    "                    depths = depths.cuda()\n",
    "                    tnf = tnf.cuda()\n",
    "\n",
    "                # Evaluate\n",
    "                out_depths, out_tnf, mu, logsigma = self(depths, tnf)\n",
    "\n",
    "                if self.usecuda:\n",
    "                    mu = mu.cpu()\n",
    "\n",
    "                latent[row: row + len(mu)] = mu\n",
    "                row += len(mu)\n",
    "\n",
    "        assert row == length\n",
    "        return latent\n",
    "\n",
    "    def save(self, filehandle):\n",
    "        \"\"\"Saves the VAE to a path or binary opened file. Load with VAE.load\n",
    "        Input: Path or binary opened filehandle\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        state = {'nsamples': self.nsamples,\n",
    "                 'alpha': self.alpha,\n",
    "                 'beta': self.beta,\n",
    "                 'dropout': self.dropout,\n",
    "                 'nhiddens': self.nhiddens,\n",
    "                 'nlatent': self.nlatent,\n",
    "                 'state': self.state_dict(),\n",
    "                }\n",
    "\n",
    "        torch.save(state, filehandle)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, cuda=False, evaluate=True):\n",
    "        \"\"\"Instantiates a VAE from a model file.\n",
    "        Inputs:\n",
    "            path: Path to model file as created by functions VAE.save or\n",
    "                  VAE.trainmodel.\n",
    "            cuda: If network should work on GPU [False]\n",
    "            evaluate: Return network in evaluation mode [True]\n",
    "        Output: VAE with weights and parameters matching the saved network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Forcably load to CPU even if model was saves as GPU model\n",
    "        dictionary = _torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        nsamples = dictionary['nsamples']\n",
    "        alpha = dictionary['alpha']\n",
    "        beta = dictionary['beta']\n",
    "        dropout = dictionary['dropout']\n",
    "        nhiddens = dictionary['nhiddens']\n",
    "        nlatent = dictionary['nlatent']\n",
    "        state = dictionary['state']\n",
    "\n",
    "        vae = cls(nsamples, nhiddens, nlatent, alpha, beta, dropout, cuda)\n",
    "        vae.load_state_dict(state)\n",
    "\n",
    "        if cuda:\n",
    "            vae.cuda()\n",
    "\n",
    "        if evaluate:\n",
    "            vae.eval()\n",
    "\n",
    "        return vae\n",
    "\n",
    "    def trainmodel(self, dataloader, nepochs=500, lrate=1e-3,\n",
    "                   batchsteps=[25, 75, 150, 300], logfile=None, modelfile=None):\n",
    "\n",
    "        if batchsteps is None:\n",
    "            batchsteps_set = set()\n",
    "        else:\n",
    "            # First collect to list in order to allow all element types, then check that\n",
    "            # they are integers\n",
    "            batchsteps = list(batchsteps)\n",
    "            last_batchsize = dataloader.batch_size * 2**len(batchsteps)\n",
    "            batchsteps_set = set(batchsteps)\n",
    "\n",
    "        # Get number of features\n",
    "        ncontigs, nsamples = dataloader.dataset.tensors[0].shape\n",
    "        optimizer = Adam(self.parameters(), lr=lrate)\n",
    "\n",
    "        if logfile is not None:\n",
    "            print('\\tNetwork properties:', file=logfile)\n",
    "            print('\\tCUDA:', self.usecuda, file=logfile)\n",
    "            print('\\tAlpha:', self.alpha, file=logfile)\n",
    "            print('\\tBeta:', self.beta, file=logfile)\n",
    "            print('\\tDropout:', self.dropout, file=logfile)\n",
    "            print('\\tN hidden:', ', '.join(map(str, self.nhiddens)), file=logfile)\n",
    "            print('\\tN latent:', self.nlatent, file=logfile)\n",
    "            print('\\n\\tTraining properties:', file=logfile)\n",
    "            print('\\tN epochs:', nepochs, file=logfile)\n",
    "            print('\\tStarting batch size:', dataloader.batch_size, file=logfile)\n",
    "            batchsteps_string = ', '.join(map(str, sorted(batchsteps))) if batchsteps_set else \"None\"\n",
    "            print('\\tBatchsteps:', batchsteps_string, file=logfile)\n",
    "            print('\\tLearning rate:', lrate, file=logfile)\n",
    "            print('\\tN sequences:', ncontigs, file=logfile)\n",
    "            print('\\tN samples:', nsamples, file=logfile, end='\\n\\n')\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(nepochs):\n",
    "            dataloader = self.trainepoch(dataloader, epoch, optimizer, batchsteps_set, logfile)\n",
    "\n",
    "        # Save weights - Lord forgive me, for I have sinned when catching all exceptions\n",
    "        if modelfile is not None:\n",
    "            try:\n",
    "                self.save(modelfile)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNetwork properties:\n",
      "\tCUDA: True\n",
      "\tAlpha: 0.15\n",
      "\tBeta: 500\n",
      "\tDropout: 0.2\n",
      "\tN hidden: 512, 512\n",
      "\tN latent: 32\n",
      "\n",
      "\tTraining properties:\n",
      "\tN epochs: 300\n",
      "\tStarting batch size: 256\n",
      "\tBatchsteps: None\n",
      "\tLearning rate: 0.001\n",
      "\tN sequences: 1342\n",
      "\tN samples: 1\n",
      "\n",
      "\tEpoch: 1\tLoss: 1.051648\tCE: 1.0207675\tSSE: 125.618611\tKLD: 16.8946\tBatchsize: 256\n",
      "\tEpoch: 2\tLoss: 0.764932\tCE: 0.7391028\tSSE: 92.642772\tKLD: 28.4512\tBatchsize: 256\n",
      "\tEpoch: 3\tLoss: 0.549276\tCE: 0.5172049\tSSE: 73.568968\tKLD: 40.2013\tBatchsize: 256\n",
      "\tEpoch: 4\tLoss: 0.430502\tCE: 0.3867863\tSSE: 67.739410\tKLD: 49.3478\tBatchsize: 256\n",
      "\tEpoch: 5\tLoss: 0.383236\tCE: 0.3369763\tSSE: 63.406117\tKLD: 71.4713\tBatchsize: 256\n",
      "\tEpoch: 6\tLoss: 0.331206\tCE: 0.2884325\tSSE: 56.460612\tKLD: 61.0259\tBatchsize: 256\n",
      "\tEpoch: 7\tLoss: 0.291122\tCE: 0.2460990\tSSE: 53.417184\tKLD: 66.3411\tBatchsize: 256\n",
      "\tEpoch: 8\tLoss: 0.265027\tCE: 0.2204272\tSSE: 50.279342\tKLD: 71.0589\tBatchsize: 256\n",
      "\tEpoch: 9\tLoss: 0.214108\tCE: 0.1627020\tSSE: 48.901179\tKLD: 73.5299\tBatchsize: 256\n",
      "\tEpoch: 10\tLoss: 0.212010\tCE: 0.1635825\tSSE: 46.724119\tKLD: 78.7146\tBatchsize: 256\n",
      "\tEpoch: 11\tLoss: 0.177954\tCE: 0.1251770\tSSE: 45.594366\tKLD: 82.4605\tBatchsize: 256\n",
      "\tEpoch: 12\tLoss: 0.215743\tCE: 0.1723989\tSSE: 43.903335\tKLD: 84.2675\tBatchsize: 256\n",
      "\tEpoch: 13\tLoss: 0.211893\tCE: 0.1680669\tSSE: 43.646901\tKLD: 87.5583\tBatchsize: 256\n",
      "\tEpoch: 14\tLoss: 0.179376\tCE: 0.1313573\tSSE: 42.569405\tKLD: 91.6466\tBatchsize: 256\n",
      "\tEpoch: 15\tLoss: 0.155258\tCE: 0.1036323\tSSE: 42.071816\tKLD: 94.4114\tBatchsize: 256\n",
      "\tEpoch: 16\tLoss: 0.159214\tCE: 0.1092018\tSSE: 41.381896\tKLD: 98.0478\tBatchsize: 256\n",
      "\tEpoch: 17\tLoss: 0.145145\tCE: 0.0931763\tSSE: 40.726237\tKLD: 106.1544\tBatchsize: 256\n",
      "\tEpoch: 18\tLoss: 0.140032\tCE: 0.0882633\tSSE: 40.119580\tKLD: 105.3131\tBatchsize: 256\n",
      "\tEpoch: 19\tLoss: 0.133350\tCE: 0.0818063\tSSE: 39.380202\tKLD: 103.4298\tBatchsize: 256\n",
      "\tEpoch: 20\tLoss: 0.130473\tCE: 0.0790998\tSSE: 38.899605\tKLD: 105.4054\tBatchsize: 256\n",
      "\tEpoch: 21\tLoss: 0.128368\tCE: 0.0762916\tSSE: 39.097742\tKLD: 105.3136\tBatchsize: 256\n",
      "\tEpoch: 22\tLoss: 0.146274\tCE: 0.0989410\tSSE: 38.221220\tKLD: 104.1920\tBatchsize: 256\n",
      "\tEpoch: 23\tLoss: 0.123635\tCE: 0.0722208\tSSE: 38.225324\tKLD: 105.2637\tBatchsize: 256\n",
      "\tEpoch: 24\tLoss: 0.122158\tCE: 0.0712159\tSSE: 37.685445\tKLD: 107.8925\tBatchsize: 256\n",
      "\tEpoch: 25\tLoss: 0.125171\tCE: 0.0756418\tSSE: 37.086515\tKLD: 109.8617\tBatchsize: 256\n",
      "\tEpoch: 26\tLoss: 0.128354\tCE: 0.0795123\tSSE: 36.953294\tKLD: 111.2458\tBatchsize: 256\n",
      "\tEpoch: 27\tLoss: 0.121355\tCE: 0.0722639\tSSE: 36.333788\tKLD: 112.2791\tBatchsize: 256\n",
      "\tEpoch: 28\tLoss: 0.130270\tCE: 0.0827362\tSSE: 36.322935\tKLD: 112.7424\tBatchsize: 256\n",
      "\tEpoch: 29\tLoss: 0.111767\tCE: 0.0614830\tSSE: 35.950389\tKLD: 114.4312\tBatchsize: 256\n",
      "\tEpoch: 30\tLoss: 0.104604\tCE: 0.0544583\tSSE: 35.086458\tKLD: 115.4860\tBatchsize: 256\n",
      "\tEpoch: 31\tLoss: 0.106180\tCE: 0.0550544\tSSE: 35.764227\tKLD: 116.8020\tBatchsize: 256\n",
      "\tEpoch: 32\tLoss: 0.104305\tCE: 0.0533382\tSSE: 35.448484\tKLD: 117.4908\tBatchsize: 256\n",
      "\tEpoch: 33\tLoss: 0.106875\tCE: 0.0570574\tSSE: 34.991065\tKLD: 118.6983\tBatchsize: 256\n",
      "\tEpoch: 34\tLoss: 0.117688\tCE: 0.0690456\tSSE: 35.425095\tKLD: 118.5456\tBatchsize: 256\n",
      "\tEpoch: 35\tLoss: 0.108202\tCE: 0.0590285\tSSE: 34.613766\tKLD: 121.9108\tBatchsize: 256\n",
      "\tEpoch: 36\tLoss: 0.106086\tCE: 0.0563813\tSSE: 34.663721\tKLD: 122.8863\tBatchsize: 256\n",
      "\tEpoch: 37\tLoss: 0.116572\tCE: 0.0700751\tSSE: 34.049809\tKLD: 118.7430\tBatchsize: 256\n",
      "\tEpoch: 38\tLoss: 0.112292\tCE: 0.0647584\tSSE: 34.015888\tKLD: 123.3619\tBatchsize: 256\n",
      "\tEpoch: 39\tLoss: 0.108674\tCE: 0.0609690\tSSE: 33.608217\tKLD: 126.4976\tBatchsize: 256\n",
      "\tEpoch: 40\tLoss: 0.104166\tCE: 0.0545632\tSSE: 34.310552\tKLD: 125.1284\tBatchsize: 256\n",
      "\tEpoch: 41\tLoss: 0.098946\tCE: 0.0493074\tSSE: 33.762672\tKLD: 125.8523\tBatchsize: 256\n",
      "\tEpoch: 42\tLoss: 0.099119\tCE: 0.0496197\tSSE: 33.656845\tKLD: 126.8419\tBatchsize: 256\n",
      "\tEpoch: 43\tLoss: 0.104627\tCE: 0.0564193\tSSE: 33.439019\tKLD: 127.5688\tBatchsize: 256\n",
      "\tEpoch: 44\tLoss: 0.103924\tCE: 0.0557456\tSSE: 33.369499\tKLD: 127.0980\tBatchsize: 256\n",
      "\tEpoch: 45\tLoss: 0.090805\tCE: 0.0404621\tSSE: 33.205451\tKLD: 128.8682\tBatchsize: 256\n",
      "\tEpoch: 46\tLoss: 0.094892\tCE: 0.0456095\tSSE: 33.050773\tKLD: 127.8666\tBatchsize: 256\n",
      "\tEpoch: 47\tLoss: 0.100503\tCE: 0.0533218\tSSE: 32.403329\tKLD: 127.8400\tBatchsize: 256\n",
      "\tEpoch: 48\tLoss: 0.093967\tCE: 0.0447205\tSSE: 32.796923\tKLD: 131.0726\tBatchsize: 256\n",
      "\tEpoch: 49\tLoss: 0.104217\tCE: 0.0571015\tSSE: 32.681359\tKLD: 129.3768\tBatchsize: 256\n",
      "\tEpoch: 50\tLoss: 0.094963\tCE: 0.0465630\tSSE: 32.488585\tKLD: 129.1366\tBatchsize: 256\n",
      "\tEpoch: 51\tLoss: 0.101823\tCE: 0.0540685\tSSE: 32.728074\tKLD: 131.2377\tBatchsize: 256\n",
      "\tEpoch: 52\tLoss: 0.098247\tCE: 0.0511277\tSSE: 32.067776\tKLD: 129.4060\tBatchsize: 256\n",
      "\tEpoch: 53\tLoss: 0.095921\tCE: 0.0480887\tSSE: 32.181287\tKLD: 130.8716\tBatchsize: 256\n",
      "\tEpoch: 54\tLoss: 0.095840\tCE: 0.0477829\tSSE: 32.244619\tKLD: 132.2558\tBatchsize: 256\n",
      "\tEpoch: 55\tLoss: 0.087973\tCE: 0.0396453\tSSE: 31.617136\tKLD: 131.6797\tBatchsize: 256\n",
      "\tEpoch: 56\tLoss: 0.093436\tCE: 0.0455321\tSSE: 31.876748\tKLD: 132.9852\tBatchsize: 256\n",
      "\tEpoch: 57\tLoss: 0.097679\tCE: 0.0505465\tSSE: 31.834254\tKLD: 133.6595\tBatchsize: 256\n",
      "\tEpoch: 58\tLoss: 0.090736\tCE: 0.0427350\tSSE: 31.642733\tKLD: 133.2763\tBatchsize: 256\n",
      "\tEpoch: 59\tLoss: 0.085614\tCE: 0.0369241\tSSE: 31.531364\tKLD: 132.9401\tBatchsize: 256\n",
      "\tEpoch: 60\tLoss: 0.110257\tCE: 0.0660643\tSSE: 31.510151\tKLD: 131.4155\tBatchsize: 256\n",
      "\tEpoch: 61\tLoss: 0.098983\tCE: 0.0520108\tSSE: 31.917092\tKLD: 132.6831\tBatchsize: 256\n",
      "\tEpoch: 62\tLoss: 0.090101\tCE: 0.0419095\tSSE: 31.588730\tKLD: 135.6021\tBatchsize: 256\n",
      "\tEpoch: 63\tLoss: 0.091842\tCE: 0.0441350\tSSE: 31.570147\tKLD: 133.6147\tBatchsize: 256\n",
      "\tEpoch: 64\tLoss: 0.088275\tCE: 0.0406379\tSSE: 31.088234\tKLD: 135.3381\tBatchsize: 256\n",
      "\tEpoch: 65\tLoss: 0.093469\tCE: 0.0460480\tSSE: 31.492715\tKLD: 135.4346\tBatchsize: 256\n",
      "\tEpoch: 66\tLoss: 0.089053\tCE: 0.0421348\tSSE: 30.817764\tKLD: 133.7292\tBatchsize: 256\n",
      "\tEpoch: 67\tLoss: 0.097676\tCE: 0.0518597\tSSE: 31.029871\tKLD: 134.5032\tBatchsize: 256\n",
      "\tEpoch: 68\tLoss: 0.085985\tCE: 0.0382631\tSSE: 30.946396\tKLD: 134.2950\tBatchsize: 256\n",
      "\tEpoch: 69\tLoss: 0.082955\tCE: 0.0353133\tSSE: 30.464813\tKLD: 137.1626\tBatchsize: 256\n",
      "\tEpoch: 70\tLoss: 0.101440\tCE: 0.0565169\tSSE: 30.904013\tKLD: 134.3223\tBatchsize: 256\n",
      "\tEpoch: 71\tLoss: 0.089560\tCE: 0.0425564\tSSE: 30.961037\tKLD: 132.7648\tBatchsize: 256\n",
      "\tEpoch: 72\tLoss: 0.087005\tCE: 0.0398330\tSSE: 30.717072\tKLD: 134.6105\tBatchsize: 256\n",
      "\tEpoch: 73\tLoss: 0.090377\tCE: 0.0438635\tSSE: 30.724170\tKLD: 133.5886\tBatchsize: 256\n",
      "\tEpoch: 74\tLoss: 0.084168\tCE: 0.0368216\tSSE: 30.512210\tKLD: 134.9477\tBatchsize: 256\n",
      "\tEpoch: 75\tLoss: 0.092725\tCE: 0.0452482\tSSE: 31.471388\tKLD: 134.9165\tBatchsize: 256\n",
      "\tEpoch: 76\tLoss: 0.092073\tCE: 0.0460799\tSSE: 30.489251\tKLD: 136.0603\tBatchsize: 256\n",
      "\tEpoch: 77\tLoss: 0.090189\tCE: 0.0427099\tSSE: 31.225879\tKLD: 134.5785\tBatchsize: 256\n",
      "\tEpoch: 78\tLoss: 0.093888\tCE: 0.0488145\tSSE: 30.129884\tKLD: 136.2677\tBatchsize: 256\n",
      "\tEpoch: 79\tLoss: 0.081871\tCE: 0.0348155\tSSE: 30.085827\tKLD: 135.4114\tBatchsize: 256\n",
      "\tEpoch: 80\tLoss: 0.100692\tCE: 0.0571931\tSSE: 30.005363\tKLD: 134.0922\tBatchsize: 256\n",
      "\tEpoch: 81\tLoss: 0.085088\tCE: 0.0391446\tSSE: 29.743926\tKLD: 135.9796\tBatchsize: 256\n",
      "\tEpoch: 82\tLoss: 0.091847\tCE: 0.0465900\tSSE: 30.003585\tKLD: 136.8187\tBatchsize: 256\n",
      "\tEpoch: 83\tLoss: 0.090560\tCE: 0.0447438\tSSE: 30.195790\tKLD: 136.8538\tBatchsize: 256\n",
      "\tEpoch: 84\tLoss: 0.088187\tCE: 0.0423532\tSSE: 29.888560\tKLD: 138.5563\tBatchsize: 256\n",
      "\tEpoch: 85\tLoss: 0.083461\tCE: 0.0369645\tSSE: 29.784480\tKLD: 138.6484\tBatchsize: 256\n",
      "\tEpoch: 86\tLoss: 0.088334\tCE: 0.0429534\tSSE: 29.710048\tKLD: 136.9097\tBatchsize: 256\n",
      "\tEpoch: 87\tLoss: 0.085169\tCE: 0.0391172\tSSE: 29.760965\tKLD: 137.2482\tBatchsize: 256\n",
      "\tEpoch: 88\tLoss: 0.082201\tCE: 0.0359078\tSSE: 29.588667\tKLD: 137.4306\tBatchsize: 256\n",
      "\tEpoch: 89\tLoss: 0.100255\tCE: 0.0574229\tSSE: 29.584265\tKLD: 133.7922\tBatchsize: 256\n",
      "\tEpoch: 90\tLoss: 0.086908\tCE: 0.0412123\tSSE: 29.761532\tKLD: 136.5656\tBatchsize: 256\n",
      "\tEpoch: 91\tLoss: 0.087769\tCE: 0.0430623\tSSE: 29.248645\tKLD: 137.1429\tBatchsize: 256\n",
      "\tEpoch: 92\tLoss: 0.080075\tCE: 0.0342705\tSSE: 29.128387\tKLD: 136.3949\tBatchsize: 256\n",
      "\tEpoch: 93\tLoss: 0.082646\tCE: 0.0366165\tSSE: 29.466661\tKLD: 137.7540\tBatchsize: 256\n",
      "\tEpoch: 94\tLoss: 0.083575\tCE: 0.0373560\tSSE: 29.607133\tKLD: 139.2906\tBatchsize: 256\n",
      "\tEpoch: 95\tLoss: 0.078903\tCE: 0.0328204\tSSE: 29.167944\tKLD: 136.4501\tBatchsize: 256\n",
      "\tEpoch: 96\tLoss: 0.089394\tCE: 0.0446713\tSSE: 29.469986\tKLD: 136.0911\tBatchsize: 256\n",
      "\tEpoch: 97\tLoss: 0.086423\tCE: 0.0412266\tSSE: 29.360056\tKLD: 137.9650\tBatchsize: 256\n",
      "\tEpoch: 98\tLoss: 0.089131\tCE: 0.0447307\tSSE: 29.158442\tKLD: 138.3376\tBatchsize: 256\n",
      "\tEpoch: 99\tLoss: 0.084795\tCE: 0.0393746\tSSE: 29.353479\tKLD: 137.2593\tBatchsize: 256\n",
      "\tEpoch: 100\tLoss: 0.075757\tCE: 0.0288742\tSSE: 29.288227\tKLD: 136.9735\tBatchsize: 256\n",
      "\tEpoch: 101\tLoss: 0.073700\tCE: 0.0265757\tSSE: 29.169367\tKLD: 138.1041\tBatchsize: 256\n",
      "\tEpoch: 102\tLoss: 0.087492\tCE: 0.0429128\tSSE: 29.115222\tKLD: 137.8378\tBatchsize: 256\n",
      "\tEpoch: 103\tLoss: 0.085064\tCE: 0.0395345\tSSE: 29.322356\tKLD: 140.1139\tBatchsize: 256\n",
      "\tEpoch: 104\tLoss: 0.075810\tCE: 0.0288444\tSSE: 29.195004\tKLD: 140.4058\tBatchsize: 256\n",
      "\tEpoch: 105\tLoss: 0.089674\tCE: 0.0463350\tSSE: 28.611515\tKLD: 137.9472\tBatchsize: 256\n",
      "\tEpoch: 106\tLoss: 0.078897\tCE: 0.0332640\tSSE: 28.882169\tKLD: 136.9806\tBatchsize: 256\n",
      "\tEpoch: 107\tLoss: 0.104324\tCE: 0.0630166\tSSE: 28.964865\tKLD: 137.2498\tBatchsize: 256\n",
      "\tEpoch: 108\tLoss: 0.078979\tCE: 0.0337242\tSSE: 28.564980\tKLD: 139.4167\tBatchsize: 256\n",
      "\tEpoch: 109\tLoss: 0.086288\tCE: 0.0418074\tSSE: 28.970611\tKLD: 136.9870\tBatchsize: 256\n",
      "\tEpoch: 110\tLoss: 0.076469\tCE: 0.0307738\tSSE: 28.694481\tKLD: 136.3702\tBatchsize: 256\n",
      "\tEpoch: 111\tLoss: 0.080202\tCE: 0.0350112\tSSE: 28.786535\tKLD: 136.3184\tBatchsize: 256\n",
      "\tEpoch: 112\tLoss: 0.076812\tCE: 0.0309266\tSSE: 28.820054\tKLD: 136.8559\tBatchsize: 256\n",
      "\tEpoch: 113\tLoss: 0.081168\tCE: 0.0363740\tSSE: 28.670327\tKLD: 135.9585\tBatchsize: 256\n",
      "\tEpoch: 114\tLoss: 0.072846\tCE: 0.0267440\tSSE: 28.585130\tKLD: 135.7580\tBatchsize: 256\n",
      "\tEpoch: 115\tLoss: 0.082363\tCE: 0.0385450\tSSE: 28.328223\tKLD: 133.5138\tBatchsize: 256\n",
      "\tEpoch: 116\tLoss: 0.080214\tCE: 0.0361863\tSSE: 28.159634\tKLD: 135.1511\tBatchsize: 256\n",
      "\tEpoch: 117\tLoss: 0.078301\tCE: 0.0336658\tSSE: 28.354871\tKLD: 134.2649\tBatchsize: 256\n",
      "\tEpoch: 118\tLoss: 0.074368\tCE: 0.0297816\tSSE: 27.911341\tKLD: 134.4910\tBatchsize: 256\n",
      "\tEpoch: 119\tLoss: 0.079949\tCE: 0.0359569\tSSE: 28.129781\tKLD: 134.7252\tBatchsize: 256\n",
      "\tEpoch: 120\tLoss: 0.077311\tCE: 0.0327062\tSSE: 28.233758\tKLD: 134.2988\tBatchsize: 256\n",
      "\tEpoch: 121\tLoss: 0.081681\tCE: 0.0382075\tSSE: 28.049797\tKLD: 133.6866\tBatchsize: 256\n",
      "\tEpoch: 122\tLoss: 0.097282\tCE: 0.0563344\tSSE: 28.189371\tKLD: 133.5293\tBatchsize: 256\n",
      "\tEpoch: 123\tLoss: 0.082884\tCE: 0.0398773\tSSE: 27.873116\tKLD: 134.3491\tBatchsize: 256\n",
      "\tEpoch: 124\tLoss: 0.082389\tCE: 0.0395671\tSSE: 27.709446\tKLD: 134.4560\tBatchsize: 256\n",
      "\tEpoch: 125\tLoss: 0.079754\tCE: 0.0358456\tSSE: 28.117027\tKLD: 133.4095\tBatchsize: 256\n",
      "\tEpoch: 126\tLoss: 0.083288\tCE: 0.0395482\tSSE: 28.350357\tKLD: 134.1656\tBatchsize: 256\n",
      "\tEpoch: 127\tLoss: 0.078324\tCE: 0.0340804\tSSE: 28.129668\tKLD: 134.2474\tBatchsize: 256\n",
      "\tEpoch: 128\tLoss: 0.079701\tCE: 0.0360757\tSSE: 28.004388\tKLD: 132.0518\tBatchsize: 256\n",
      "\tEpoch: 129\tLoss: 0.074257\tCE: 0.0298101\tSSE: 27.908039\tKLD: 132.4165\tBatchsize: 256\n",
      "\tEpoch: 130\tLoss: 0.077778\tCE: 0.0342593\tSSE: 27.759175\tKLD: 131.7040\tBatchsize: 256\n",
      "\tEpoch: 131\tLoss: 0.073741\tCE: 0.0289979\tSSE: 28.081065\tKLD: 131.1754\tBatchsize: 256\n",
      "\tEpoch: 132\tLoss: 0.085111\tCE: 0.0421271\tSSE: 28.272996\tKLD: 130.0628\tBatchsize: 256\n",
      "\tEpoch: 133\tLoss: 0.078034\tCE: 0.0344176\tSSE: 27.773042\tKLD: 133.3214\tBatchsize: 256\n",
      "\tEpoch: 134\tLoss: 0.077176\tCE: 0.0336130\tSSE: 27.744326\tKLD: 131.2149\tBatchsize: 256\n",
      "\tEpoch: 135\tLoss: 0.088919\tCE: 0.0466812\tSSE: 28.207026\tKLD: 130.5820\tBatchsize: 256\n",
      "\tEpoch: 136\tLoss: 0.081242\tCE: 0.0382202\tSSE: 27.764658\tKLD: 133.1337\tBatchsize: 256\n",
      "\tEpoch: 137\tLoss: 0.080372\tCE: 0.0380333\tSSE: 27.315903\tKLD: 132.2191\tBatchsize: 256\n",
      "\tEpoch: 138\tLoss: 0.075226\tCE: 0.0320004\tSSE: 27.366252\tKLD: 130.7457\tBatchsize: 256\n",
      "\tEpoch: 139\tLoss: 0.072255\tCE: 0.0280504\tSSE: 27.589993\tKLD: 131.7135\tBatchsize: 256\n",
      "\tEpoch: 140\tLoss: 0.078190\tCE: 0.0353363\tSSE: 27.394852\tKLD: 132.1322\tBatchsize: 256\n",
      "\tEpoch: 141\tLoss: 0.073136\tCE: 0.0294043\tSSE: 27.465375\tKLD: 130.3135\tBatchsize: 256\n",
      "\tEpoch: 142\tLoss: 0.082922\tCE: 0.0409223\tSSE: 27.490005\tKLD: 129.6639\tBatchsize: 256\n",
      "\tEpoch: 143\tLoss: 0.070778\tCE: 0.0263802\tSSE: 27.573249\tKLD: 131.1961\tBatchsize: 256\n",
      "\tEpoch: 144\tLoss: 0.080384\tCE: 0.0376487\tSSE: 27.623097\tKLD: 130.4756\tBatchsize: 256\n",
      "\tEpoch: 145\tLoss: 0.073832\tCE: 0.0300932\tSSE: 27.578930\tKLD: 129.4208\tBatchsize: 256\n",
      "\tEpoch: 146\tLoss: 0.072783\tCE: 0.0288874\tSSE: 27.482303\tKLD: 131.3010\tBatchsize: 256\n",
      "\tEpoch: 147\tLoss: 0.074743\tCE: 0.0310966\tSSE: 27.532814\tKLD: 131.4341\tBatchsize: 256\n",
      "\tEpoch: 148\tLoss: 0.077451\tCE: 0.0347130\tSSE: 27.244238\tKLD: 132.2984\tBatchsize: 256\n",
      "\tEpoch: 149\tLoss: 0.072282\tCE: 0.0285889\tSSE: 27.270669\tKLD: 132.2768\tBatchsize: 256\n",
      "\tEpoch: 150\tLoss: 0.070503\tCE: 0.0272291\tSSE: 26.904306\tKLD: 130.8416\tBatchsize: 256\n",
      "\tEpoch: 151\tLoss: 0.069388\tCE: 0.0263183\tSSE: 26.744661\tKLD: 129.1094\tBatchsize: 256\n",
      "\tEpoch: 152\tLoss: 0.073344\tCE: 0.0297749\tSSE: 27.432772\tKLD: 129.3530\tBatchsize: 256\n",
      "\tEpoch: 153\tLoss: 0.073024\tCE: 0.0301934\tSSE: 26.913588\tKLD: 130.6427\tBatchsize: 256\n",
      "\tEpoch: 154\tLoss: 0.078315\tCE: 0.0348954\tSSE: 27.824986\tKLD: 130.1075\tBatchsize: 256\n",
      "\tEpoch: 155\tLoss: 0.072755\tCE: 0.0299481\tSSE: 26.861679\tKLD: 130.8758\tBatchsize: 256\n",
      "\tEpoch: 156\tLoss: 0.090409\tCE: 0.0507722\tSSE: 26.886090\tKLD: 129.5740\tBatchsize: 256\n",
      "\tEpoch: 157\tLoss: 0.071864\tCE: 0.0289984\tSSE: 26.850635\tKLD: 129.7967\tBatchsize: 256\n",
      "\tEpoch: 158\tLoss: 0.075271\tCE: 0.0326799\tSSE: 26.979498\tKLD: 131.2489\tBatchsize: 256\n",
      "\tEpoch: 159\tLoss: 0.073417\tCE: 0.0304004\tSSE: 27.086099\tKLD: 130.0925\tBatchsize: 256\n",
      "\tEpoch: 160\tLoss: 0.073148\tCE: 0.0311279\tSSE: 26.598346\tKLD: 127.2604\tBatchsize: 256\n",
      "\tEpoch: 161\tLoss: 0.071969\tCE: 0.0297668\tSSE: 26.507264\tKLD: 129.0327\tBatchsize: 256\n",
      "\tEpoch: 162\tLoss: 0.079742\tCE: 0.0376914\tSSE: 27.273475\tKLD: 127.7637\tBatchsize: 256\n",
      "\tEpoch: 163\tLoss: 0.069859\tCE: 0.0273514\tSSE: 26.522623\tKLD: 127.7598\tBatchsize: 256\n",
      "\tEpoch: 164\tLoss: 0.070341\tCE: 0.0271570\tSSE: 26.911900\tKLD: 129.0533\tBatchsize: 256\n",
      "\tEpoch: 165\tLoss: 0.074311\tCE: 0.0311974\tSSE: 27.322539\tKLD: 128.0427\tBatchsize: 256\n",
      "\tEpoch: 166\tLoss: 0.075372\tCE: 0.0335125\tSSE: 26.787860\tKLD: 125.9958\tBatchsize: 256\n",
      "\tEpoch: 167\tLoss: 0.085296\tCE: 0.0449747\tSSE: 26.839213\tKLD: 127.6981\tBatchsize: 256\n",
      "\tEpoch: 168\tLoss: 0.070234\tCE: 0.0274148\tSSE: 26.687335\tKLD: 129.0595\tBatchsize: 256\n",
      "\tEpoch: 169\tLoss: 0.072882\tCE: 0.0310178\tSSE: 26.486967\tKLD: 127.0946\tBatchsize: 256\n",
      "\tEpoch: 170\tLoss: 0.068291\tCE: 0.0254838\tSSE: 26.496507\tKLD: 128.6795\tBatchsize: 256\n",
      "\tEpoch: 171\tLoss: 0.082412\tCE: 0.0422480\tSSE: 26.423095\tKLD: 128.3358\tBatchsize: 256\n",
      "\tEpoch: 172\tLoss: 0.068772\tCE: 0.0260301\tSSE: 26.583393\tKLD: 126.9167\tBatchsize: 256\n",
      "\tEpoch: 173\tLoss: 0.074762\tCE: 0.0334679\tSSE: 26.320187\tKLD: 127.7345\tBatchsize: 256\n",
      "\tEpoch: 174\tLoss: 0.077132\tCE: 0.0362023\tSSE: 26.366181\tKLD: 127.3947\tBatchsize: 256\n",
      "\tEpoch: 175\tLoss: 0.078331\tCE: 0.0369623\tSSE: 26.819428\tKLD: 125.6899\tBatchsize: 256\n",
      "\tEpoch: 176\tLoss: 0.076767\tCE: 0.0358722\tSSE: 26.417564\tKLD: 124.8559\tBatchsize: 256\n",
      "\tEpoch: 177\tLoss: 0.066864\tCE: 0.0248116\tSSE: 26.027800\tKLD: 125.9177\tBatchsize: 256\n",
      "\tEpoch: 178\tLoss: 0.077158\tCE: 0.0361261\tSSE: 26.523215\tKLD: 125.2005\tBatchsize: 256\n",
      "\tEpoch: 179\tLoss: 0.070695\tCE: 0.0286152\tSSE: 26.493296\tKLD: 124.6364\tBatchsize: 256\n",
      "\tEpoch: 180\tLoss: 0.067465\tCE: 0.0251081\tSSE: 26.291235\tKLD: 125.3525\tBatchsize: 256\n",
      "\tEpoch: 181\tLoss: 0.072455\tCE: 0.0314075\tSSE: 25.986351\tKLD: 126.6339\tBatchsize: 256\n",
      "\tEpoch: 182\tLoss: 0.069949\tCE: 0.0281787\tSSE: 26.260669\tKLD: 124.0503\tBatchsize: 256\n",
      "\tEpoch: 183\tLoss: 0.075874\tCE: 0.0355242\tSSE: 26.022028\tKLD: 124.5139\tBatchsize: 256\n",
      "\tEpoch: 184\tLoss: 0.067711\tCE: 0.0255647\tSSE: 26.197701\tKLD: 125.2640\tBatchsize: 256\n",
      "\tEpoch: 185\tLoss: 0.068028\tCE: 0.0260527\tSSE: 26.154971\tKLD: 124.7026\tBatchsize: 256\n",
      "\tEpoch: 186\tLoss: 0.084699\tCE: 0.0457087\tSSE: 26.141997\tKLD: 124.4121\tBatchsize: 256\n",
      "\tEpoch: 187\tLoss: 0.080821\tCE: 0.0407521\tSSE: 26.265911\tKLD: 126.8821\tBatchsize: 256\n",
      "\tEpoch: 188\tLoss: 0.075410\tCE: 0.0346197\tSSE: 26.174101\tKLD: 125.8538\tBatchsize: 256\n",
      "\tEpoch: 189\tLoss: 0.071348\tCE: 0.0302988\tSSE: 25.968385\tKLD: 124.4106\tBatchsize: 256\n",
      "\tEpoch: 190\tLoss: 0.070302\tCE: 0.0290608\tSSE: 25.949631\tKLD: 124.9508\tBatchsize: 256\n",
      "\tEpoch: 191\tLoss: 0.071817\tCE: 0.0311165\tSSE: 25.763670\tKLD: 125.5750\tBatchsize: 256\n",
      "\tEpoch: 192\tLoss: 0.070902\tCE: 0.0295625\tSSE: 26.005059\tKLD: 126.4409\tBatchsize: 256\n",
      "\tEpoch: 193\tLoss: 0.069946\tCE: 0.0279408\tSSE: 26.356091\tKLD: 125.0108\tBatchsize: 256\n",
      "\tEpoch: 194\tLoss: 0.073970\tCE: 0.0330032\tSSE: 26.206683\tKLD: 124.0287\tBatchsize: 256\n",
      "\tEpoch: 195\tLoss: 0.071250\tCE: 0.0305656\tSSE: 25.779963\tKLD: 123.6107\tBatchsize: 256\n",
      "\tEpoch: 196\tLoss: 0.067492\tCE: 0.0259408\tSSE: 25.884966\tKLD: 123.9374\tBatchsize: 256\n",
      "\tEpoch: 197\tLoss: 0.072936\tCE: 0.0322416\tSSE: 25.979773\tKLD: 123.1361\tBatchsize: 256\n",
      "\tEpoch: 198\tLoss: 0.068497\tCE: 0.0278050\tSSE: 25.520655\tKLD: 123.1545\tBatchsize: 256\n",
      "\tEpoch: 199\tLoss: 0.067742\tCE: 0.0262825\tSSE: 25.885447\tKLD: 123.2779\tBatchsize: 256\n",
      "\tEpoch: 200\tLoss: 0.067727\tCE: 0.0261439\tSSE: 25.967890\tKLD: 122.9937\tBatchsize: 256\n",
      "\tEpoch: 201\tLoss: 0.069511\tCE: 0.0285153\tSSE: 25.794195\tKLD: 123.3443\tBatchsize: 256\n",
      "\tEpoch: 202\tLoss: 0.065751\tCE: 0.0244793\tSSE: 25.668124\tKLD: 120.9993\tBatchsize: 256\n",
      "\tEpoch: 203\tLoss: 0.064686\tCE: 0.0222133\tSSE: 26.230748\tKLD: 121.6731\tBatchsize: 256\n",
      "\tEpoch: 204\tLoss: 0.069735\tCE: 0.0287688\tSSE: 25.885152\tKLD: 121.3522\tBatchsize: 256\n",
      "\tEpoch: 205\tLoss: 0.069651\tCE: 0.0293089\tSSE: 25.485475\tKLD: 121.9809\tBatchsize: 256\n",
      "\tEpoch: 206\tLoss: 0.067634\tCE: 0.0265844\tSSE: 25.729409\tKLD: 121.0733\tBatchsize: 256\n",
      "\tEpoch: 207\tLoss: 0.066262\tCE: 0.0248783\tSSE: 25.726772\tKLD: 122.3834\tBatchsize: 256\n",
      "\tEpoch: 208\tLoss: 0.067504\tCE: 0.0266046\tSSE: 25.631656\tKLD: 120.9974\tBatchsize: 256\n",
      "\tEpoch: 209\tLoss: 0.064511\tCE: 0.0238449\tSSE: 25.201278\tKLD: 120.6671\tBatchsize: 256\n",
      "\tEpoch: 210\tLoss: 0.070974\tCE: 0.0305506\tSSE: 25.659860\tKLD: 122.1907\tBatchsize: 256\n",
      "\tEpoch: 211\tLoss: 0.063416\tCE: 0.0213103\tSSE: 25.857433\tKLD: 122.3394\tBatchsize: 256\n",
      "\tEpoch: 212\tLoss: 0.066859\tCE: 0.0259734\tSSE: 25.538648\tKLD: 121.4226\tBatchsize: 256\n",
      "\tEpoch: 213\tLoss: 0.077030\tCE: 0.0384670\tSSE: 25.301794\tKLD: 119.7760\tBatchsize: 256\n",
      "\tEpoch: 214\tLoss: 0.074302\tCE: 0.0347356\tSSE: 25.549545\tKLD: 121.1016\tBatchsize: 256\n",
      "\tEpoch: 215\tLoss: 0.072208\tCE: 0.0322863\tSSE: 25.580830\tKLD: 120.1735\tBatchsize: 256\n",
      "\tEpoch: 216\tLoss: 0.066918\tCE: 0.0262704\tSSE: 25.380696\tKLD: 122.0163\tBatchsize: 256\n",
      "\tEpoch: 217\tLoss: 0.073427\tCE: 0.0338957\tSSE: 25.463360\tKLD: 120.5332\tBatchsize: 256\n",
      "\tEpoch: 218\tLoss: 0.083554\tCE: 0.0463858\tSSE: 25.218351\tKLD: 118.4055\tBatchsize: 256\n",
      "\tEpoch: 219\tLoss: 0.077110\tCE: 0.0386481\tSSE: 25.264771\tKLD: 119.4505\tBatchsize: 256\n",
      "\tEpoch: 220\tLoss: 0.072572\tCE: 0.0323193\tSSE: 25.809773\tKLD: 120.2140\tBatchsize: 256\n",
      "\tEpoch: 221\tLoss: 0.064420\tCE: 0.0237867\tSSE: 25.167553\tKLD: 120.7862\tBatchsize: 256\n",
      "\tEpoch: 222\tLoss: 0.062013\tCE: 0.0213133\tSSE: 24.977512\tKLD: 120.3447\tBatchsize: 256\n",
      "\tEpoch: 223\tLoss: 0.064582\tCE: 0.0235552\tSSE: 25.409939\tKLD: 120.8804\tBatchsize: 256\n",
      "\tEpoch: 224\tLoss: 0.064534\tCE: 0.0240592\tSSE: 25.136854\tKLD: 119.6236\tBatchsize: 256\n",
      "\tEpoch: 225\tLoss: 0.063106\tCE: 0.0227024\tSSE: 24.974401\tKLD: 119.0203\tBatchsize: 256\n",
      "\tEpoch: 226\tLoss: 0.064016\tCE: 0.0236473\tSSE: 24.999965\tKLD: 120.1296\tBatchsize: 256\n",
      "\tEpoch: 227\tLoss: 0.065942\tCE: 0.0261662\tSSE: 24.848348\tKLD: 120.2187\tBatchsize: 256\n",
      "\tEpoch: 228\tLoss: 0.066411\tCE: 0.0262566\tSSE: 25.134930\tKLD: 119.8220\tBatchsize: 256\n",
      "\tEpoch: 229\tLoss: 0.070462\tCE: 0.0311844\tSSE: 25.096774\tKLD: 118.5047\tBatchsize: 256\n",
      "\tEpoch: 230\tLoss: 0.068912\tCE: 0.0289699\tSSE: 25.326366\tKLD: 118.4702\tBatchsize: 256\n",
      "\tEpoch: 231\tLoss: 0.075624\tCE: 0.0375214\tSSE: 24.910550\tKLD: 119.2569\tBatchsize: 256\n",
      "\tEpoch: 232\tLoss: 0.071431\tCE: 0.0320931\tSSE: 25.193707\tKLD: 119.3905\tBatchsize: 256\n",
      "\tEpoch: 233\tLoss: 0.063860\tCE: 0.0235705\tSSE: 24.925695\tKLD: 120.4064\tBatchsize: 256\n",
      "\tEpoch: 234\tLoss: 0.063596\tCE: 0.0232347\tSSE: 24.947130\tKLD: 120.2591\tBatchsize: 256\n",
      "\tEpoch: 235\tLoss: 0.062142\tCE: 0.0201271\tSSE: 25.799781\tKLD: 119.3760\tBatchsize: 256\n",
      "\tEpoch: 236\tLoss: 0.070821\tCE: 0.0317473\tSSE: 25.035475\tKLD: 118.0261\tBatchsize: 256\n",
      "\tEpoch: 237\tLoss: 0.068150\tCE: 0.0282198\tSSE: 25.230799\tKLD: 118.7155\tBatchsize: 256\n",
      "\tEpoch: 238\tLoss: 0.067111\tCE: 0.0267770\tSSE: 25.346462\tKLD: 119.0183\tBatchsize: 256\n",
      "\tEpoch: 239\tLoss: 0.066190\tCE: 0.0266812\tSSE: 24.765346\tKLD: 119.1140\tBatchsize: 256\n",
      "\tEpoch: 240\tLoss: 0.060716\tCE: 0.0196855\tSSE: 25.094341\tKLD: 119.0104\tBatchsize: 256\n",
      "\tEpoch: 241\tLoss: 0.058352\tCE: 0.0176888\tSSE: 24.658322\tKLD: 118.4945\tBatchsize: 256\n",
      "\tEpoch: 242\tLoss: 0.070317\tCE: 0.0314492\tSSE: 24.847963\tKLD: 118.3862\tBatchsize: 256\n",
      "\tEpoch: 243\tLoss: 0.066029\tCE: 0.0268185\tSSE: 24.632396\tKLD: 117.7662\tBatchsize: 256\n",
      "\tEpoch: 244\tLoss: 0.061249\tCE: 0.0210190\tSSE: 24.723869\tKLD: 118.0375\tBatchsize: 256\n",
      "\tEpoch: 245\tLoss: 0.063081\tCE: 0.0228509\tSSE: 24.962713\tKLD: 116.8623\tBatchsize: 256\n",
      "\tEpoch: 246\tLoss: 0.064685\tCE: 0.0254529\tSSE: 24.511509\tKLD: 117.6534\tBatchsize: 256\n",
      "\tEpoch: 247\tLoss: 0.064931\tCE: 0.0256543\tSSE: 24.584102\tKLD: 117.1611\tBatchsize: 256\n",
      "\tEpoch: 248\tLoss: 0.064896\tCE: 0.0250941\tSSE: 24.913567\tKLD: 116.5421\tBatchsize: 256\n",
      "\tEpoch: 249\tLoss: 0.064960\tCE: 0.0252695\tSSE: 24.819558\tKLD: 117.3812\tBatchsize: 256\n",
      "\tEpoch: 250\tLoss: 0.066272\tCE: 0.0267721\tSSE: 24.837140\tKLD: 117.5141\tBatchsize: 256\n",
      "\tEpoch: 251\tLoss: 0.061634\tCE: 0.0218600\tSSE: 24.515717\tKLD: 117.6017\tBatchsize: 256\n",
      "\tEpoch: 252\tLoss: 0.065549\tCE: 0.0259605\tSSE: 24.834685\tKLD: 117.0441\tBatchsize: 256\n",
      "\tEpoch: 253\tLoss: 0.077310\tCE: 0.0403996\tSSE: 24.551024\tKLD: 115.4589\tBatchsize: 256\n",
      "\tEpoch: 254\tLoss: 0.063955\tCE: 0.0243199\tSSE: 24.760477\tKLD: 115.5781\tBatchsize: 256\n",
      "\tEpoch: 255\tLoss: 0.064754\tCE: 0.0260551\tSSE: 24.228269\tKLD: 117.1750\tBatchsize: 256\n",
      "\tEpoch: 256\tLoss: 0.060577\tCE: 0.0209426\tSSE: 24.379701\tKLD: 116.3425\tBatchsize: 256\n",
      "\tEpoch: 257\tLoss: 0.065710\tCE: 0.0269438\tSSE: 24.451082\tKLD: 115.1882\tBatchsize: 256\n",
      "\tEpoch: 258\tLoss: 0.068846\tCE: 0.0303398\tSSE: 24.582679\tKLD: 116.1069\tBatchsize: 256\n",
      "\tEpoch: 259\tLoss: 0.072454\tCE: 0.0343613\tSSE: 24.722388\tKLD: 115.8905\tBatchsize: 256\n",
      "\tEpoch: 260\tLoss: 0.069064\tCE: 0.0306492\tSSE: 24.559793\tKLD: 115.9256\tBatchsize: 256\n",
      "\tEpoch: 261\tLoss: 0.065988\tCE: 0.0274634\tSSE: 24.278897\tKLD: 116.5871\tBatchsize: 256\n",
      "\tEpoch: 262\tLoss: 0.064607\tCE: 0.0255264\tSSE: 24.481974\tKLD: 116.0979\tBatchsize: 256\n",
      "\tEpoch: 263\tLoss: 0.066910\tCE: 0.0276823\tSSE: 24.815625\tKLD: 115.8585\tBatchsize: 256\n",
      "\tEpoch: 264\tLoss: 0.070127\tCE: 0.0320745\tSSE: 24.474860\tKLD: 115.5315\tBatchsize: 256\n",
      "\tEpoch: 265\tLoss: 0.062655\tCE: 0.0234539\tSSE: 24.356634\tKLD: 115.9679\tBatchsize: 256\n",
      "\tEpoch: 266\tLoss: 0.063851\tCE: 0.0245689\tSSE: 24.519662\tKLD: 116.1449\tBatchsize: 256\n",
      "\tEpoch: 267\tLoss: 0.064682\tCE: 0.0257778\tSSE: 24.363575\tKLD: 116.6423\tBatchsize: 256\n",
      "\tEpoch: 268\tLoss: 0.061595\tCE: 0.0220512\tSSE: 24.426254\tKLD: 116.4706\tBatchsize: 256\n",
      "\tEpoch: 269\tLoss: 0.062150\tCE: 0.0231301\tSSE: 24.193307\tKLD: 116.1074\tBatchsize: 256\n",
      "\tEpoch: 270\tLoss: 0.060774\tCE: 0.0205517\tSSE: 24.764261\tKLD: 115.8544\tBatchsize: 256\n",
      "\tEpoch: 271\tLoss: 0.060782\tCE: 0.0215369\tSSE: 24.223480\tKLD: 115.1832\tBatchsize: 256\n",
      "\tEpoch: 272\tLoss: 0.061504\tCE: 0.0216655\tSSE: 24.636248\tKLD: 115.3625\tBatchsize: 256\n",
      "\tEpoch: 273\tLoss: 0.058828\tCE: 0.0196088\tSSE: 23.969715\tKLD: 116.0556\tBatchsize: 256\n",
      "\tEpoch: 274\tLoss: 0.065342\tCE: 0.0268787\tSSE: 24.249230\tKLD: 114.8857\tBatchsize: 256\n",
      "\tEpoch: 275\tLoss: 0.064019\tCE: 0.0246145\tSSE: 24.658649\tKLD: 114.9736\tBatchsize: 256\n",
      "\tEpoch: 276\tLoss: 0.066698\tCE: 0.0285807\tSSE: 24.149874\tKLD: 115.7568\tBatchsize: 256\n",
      "\tEpoch: 277\tLoss: 0.063485\tCE: 0.0252448\tSSE: 23.944376\tKLD: 114.5065\tBatchsize: 256\n",
      "\tEpoch: 278\tLoss: 0.065165\tCE: 0.0264127\tSSE: 24.475339\tKLD: 113.1240\tBatchsize: 256\n",
      "\tEpoch: 279\tLoss: 0.068224\tCE: 0.0305977\tSSE: 24.113475\tKLD: 113.5944\tBatchsize: 256\n",
      "\tEpoch: 280\tLoss: 0.060181\tCE: 0.0207569\tSSE: 24.296087\tKLD: 114.4762\tBatchsize: 256\n",
      "\tEpoch: 281\tLoss: 0.067032\tCE: 0.0290080\tSSE: 24.150451\tKLD: 115.2771\tBatchsize: 256\n",
      "\tEpoch: 282\tLoss: 0.063802\tCE: 0.0248493\tSSE: 24.417424\tKLD: 113.9378\tBatchsize: 256\n",
      "\tEpoch: 283\tLoss: 0.067950\tCE: 0.0298941\tSSE: 24.327123\tKLD: 113.8009\tBatchsize: 256\n",
      "\tEpoch: 284\tLoss: 0.063086\tCE: 0.0245405\tSSE: 24.038643\tKLD: 115.4995\tBatchsize: 256\n",
      "\tEpoch: 285\tLoss: 0.060984\tCE: 0.0215573\tSSE: 24.249352\tKLD: 117.5353\tBatchsize: 256\n",
      "\tEpoch: 286\tLoss: 0.069730\tCE: 0.0315925\tSSE: 24.414471\tKLD: 117.1342\tBatchsize: 256\n",
      "\tEpoch: 287\tLoss: 0.067555\tCE: 0.0292976\tSSE: 24.265883\tKLD: 117.0208\tBatchsize: 256\n",
      "\tEpoch: 288\tLoss: 0.060296\tCE: 0.0208297\tSSE: 24.220934\tKLD: 117.0840\tBatchsize: 256\n",
      "\tEpoch: 289\tLoss: 0.067499\tCE: 0.0300829\tSSE: 23.825565\tKLD: 115.6977\tBatchsize: 256\n",
      "\tEpoch: 290\tLoss: 0.060972\tCE: 0.0223951\tSSE: 23.881236\tKLD: 114.5278\tBatchsize: 256\n",
      "\tEpoch: 291\tLoss: 0.062644\tCE: 0.0239847\tSSE: 24.060976\tKLD: 115.4723\tBatchsize: 256\n",
      "\tEpoch: 292\tLoss: 0.059142\tCE: 0.0202608\tSSE: 23.865445\tKLD: 114.6430\tBatchsize: 256\n",
      "\tEpoch: 293\tLoss: 0.068415\tCE: 0.0306869\tSSE: 24.163322\tKLD: 114.2725\tBatchsize: 256\n",
      "\tEpoch: 294\tLoss: 0.066913\tCE: 0.0289596\tSSE: 24.191209\tKLD: 113.0865\tBatchsize: 256\n",
      "\tEpoch: 295\tLoss: 0.060591\tCE: 0.0212956\tSSE: 24.231515\tKLD: 115.2102\tBatchsize: 256\n",
      "\tEpoch: 296\tLoss: 0.061325\tCE: 0.0224962\tSSE: 24.032265\tKLD: 115.2754\tBatchsize: 256\n",
      "\tEpoch: 297\tLoss: 0.071589\tCE: 0.0348837\tSSE: 23.866284\tKLD: 114.8931\tBatchsize: 256\n",
      "\tEpoch: 298\tLoss: 0.059278\tCE: 0.0204792\tSSE: 23.807143\tKLD: 115.2015\tBatchsize: 256\n",
      "\tEpoch: 299\tLoss: 0.063882\tCE: 0.0256864\tSSE: 23.937059\tKLD: 115.0239\tBatchsize: 256\n",
      "\tEpoch: 300\tLoss: 0.063112\tCE: 0.0252120\tSSE: 23.713012\tKLD: 114.3752\tBatchsize: 256\n"
     ]
    }
   ],
   "source": [
    "dvae = DISENTANGLED_VAE(nsamples=rpkms.shape[1], beta=500)\n",
    "\n",
    "with open('vamb_models/model_disentangled_vae.pt', 'wb') as modelfile:\n",
    "    dvae.trainmodel(dataloader, nepochs=300, modelfile=modelfile, batchsteps=None, logfile=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1342, 32)\n"
     ]
    }
   ],
   "source": [
    "latent = dvae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)\n",
    "\n",
    "latent_output_path = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs/latent_space.npy')\n",
    "with open(latent_output_path, 'wb') as outfile:\n",
    "    np.save(outfile, latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: S0C50940 (of type: <class 'numpy.str_'> )\n",
      "Type of values: <class 'set'>\n",
      "First element of value: S0C50940 of type: <class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "filtered_labels = [n for (n,m) in zip(contignames, mask) if m]\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=filtered_labels)\n",
    "clusters = dict(cluster_iterator)\n",
    "\n",
    "medoid, contigs = next(iter(clusters.items()))\n",
    "print('First key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.7632344 ,  0.9128553 , -0.99542826, ...,  3.4221175 ,\n",
       "         2.475425  , -0.8712864 ],\n",
       "       [-1.8906579 , -0.74544764, -0.12259644, ..., -0.789257  ,\n",
       "        -3.511978  ,  0.4764389 ],\n",
       "       [ 0.21262881, -1.123332  , -0.57075024, ..., -0.7388763 ,\n",
       "        -3.0588045 ,  0.7617666 ],\n",
       "       ...,\n",
       "       [-1.2402482 ,  1.0010657 ,  0.03230138, ..., -1.0659124 ,\n",
       "        -2.618636  ,  1.291593  ],\n",
       "       [ 4.1214113 , -1.9292675 ,  0.835457  , ...,  4.6487226 ,\n",
       "         0.13279133,  0.4252876 ],\n",
       "       [ 1.7570639 , -0.84198314,  1.0179619 , ..., -0.31993234,\n",
       "        -0.5479913 ,  0.8817526 ]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before splitting and filtering: 534\n",
      "Number of bins after splitting and filtering: 10\n"
     ]
    }
   ],
   "source": [
    "def filterclusters(clusters, lengthof):\n",
    "    filtered_bins = dict()\n",
    "    for medoid, contigs in clusters.items():\n",
    "        binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "        if binsize >= 50000:\n",
    "            filtered_bins[medoid] = contigs\n",
    "    \n",
    "    return filtered_bins\n",
    "        \n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "filtered_bins = filterclusters(vamb.vambtools.binsplit(clusters, 'C'), lengthof)\n",
    "print('Number of bins before splitting and filtering:', len(clusters))\n",
    "print('Number of bins after splitting and filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_mapping_table = pd.read_csv(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/gsa_mapping.tsv\"), sep='\\t')\n",
    "\n",
    "contig_mapping_output_path = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs/encoding_mapping.tsv')    \n",
    "\n",
    "contig_mapping_table[contig_mapping_table['#anonymous_contig_id'].isin(contignames)].reset_index().drop('index', axis=1).set_index(\n",
    "    '#anonymous_contig_id').reindex(contignames).to_csv(contig_mapping_output_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_outputs_base = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs')\n",
    "\n",
    "if not os.path.exists(vamb_outputs_base):\n",
    "    os.mkdir(vamb_outputs_base)\n",
    "    \n",
    "\n",
    "# This writes a .tsv file with the clusters and corresponding sequences\n",
    "with open(os.path.join(vamb_outputs_base, 'clusters_dvae.tsv'), 'w') as file:\n",
    "    vamb.vambtools.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "keptcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "\n",
    "\n",
    "# decompress fasta.gz if present\n",
    "fasta_path = os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta.gz\")\n",
    "if os.path.exists(fasta_path) and not os.path.exists(fasta_path.replace('.fasta.gz','.fasta')):\n",
    "    !gzip -dk $fasta_path\n",
    "\n",
    "\n",
    "with open(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta\"), 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=keptcontigs)\n",
    "\n",
    "\n",
    "bindir = os.path.join(vamb_outputs_base, 'dvae_bins')\n",
    "if not os.path.exists(bindir):\n",
    "    os.mkdir(bindir)\n",
    "    \n",
    "    \n",
    "files = glob.glob(os.path.join(bindir,'*'))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "vamb.vambtools.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vamb_env",
   "language": "python",
   "name": "vamb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
