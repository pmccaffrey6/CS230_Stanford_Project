{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff6cd619c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vamb\n",
    "\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam as Adam\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import wandb\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "EXAMPLE_FASTA_FILE = '2021.01.26_15.46.45_sample_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_inputs_base = os.path.join(BASE_DIR,'example_input_data/new_simulations/camisim_outputs/vamb_inputs')\n",
    "\n",
    "contignames = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'contignames.npz'))\n",
    "\n",
    "lengths = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'lengths.npz'))\n",
    "\n",
    "tnfs = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'tnfs.npz'))\n",
    "    \n",
    "rpkms = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'rpkms.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt Through DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthssum = rpkms.sum(axis=1)\n",
    "mask = tnfs.sum(axis=1) != 0\n",
    "mask &= depthssum != 0\n",
    "depthssum = depthssum[mask]\n",
    "\n",
    "rpkm = rpkms[mask].astype(np.float32, copy=False)\n",
    "tnf = tnfs[mask].astype(np.float32, copy=False)\n",
    "\n",
    "## lkj\n",
    "def calculate_z_score(array):\n",
    "    array_mean = array.mean(axis=0)\n",
    "    array_std = array.std(axis=0)\n",
    "\n",
    "    shape = np.copy(array.shape)\n",
    "    shape[0] = 1\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    array_mean.shape = shape\n",
    "    array_mean.shape = shape\n",
    "\n",
    "    array = (array - array_mean) / array_std\n",
    "    \n",
    "    return(array)\n",
    "    \n",
    "rpkm = calculate_z_score(rpkm)\n",
    "tnf = calculate_z_score(tnf)\n",
    "depthstensor = torch.from_numpy(rpkm)\n",
    "tnftensor = torch.from_numpy(tnf)\n",
    "\n",
    "n_workers = 4\n",
    "\n",
    "dataset = TensorDataset(depthstensor, tnftensor)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=256, drop_last=True,\n",
    "                             shuffle=True, num_workers=n_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "ncontigs, nsamples = dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DISENTANGLED_BETA_VAE(torch.nn.Module):\n",
    "    def __init__(self, nsamples, config):\n",
    "        super(DISENTANGLED_BETA_VAE, self).__init__()\n",
    "  \n",
    "        # SET UP AND CONFIGURE THE MODEL\n",
    "        self.ntnf = tnfs.shape[1]\n",
    "        \n",
    "        self.nlatent = config.nlatent\n",
    "        self.dropout = config.dropout\n",
    "        self.learning_rate = config.learning_rate\n",
    "        self.alpha = config.alpha\n",
    "        self.beta = config.beta\n",
    "        self.nepochs = config.nepochs\n",
    "        \n",
    "        nhiddens = [512, 512]\n",
    "        \n",
    "        self.nsamples = nsamples\n",
    "        self.cuda_on = False\n",
    "        \n",
    "        if self.cuda_on:\n",
    "            self.cuda()\n",
    "\n",
    "        self.encoderlayers = torch.nn.ModuleList()\n",
    "        self.encodernorms = torch.nn.ModuleList()\n",
    "        self.decoderlayers = torch.nn.ModuleList()\n",
    "        self.decodernorms = torch.nn.ModuleList()\n",
    "\n",
    "\n",
    "        # ENCODER LAYERS\n",
    "        self.encoderlayers.append( torch.nn.Linear((self.nsamples + self.ntnf), 512) )\n",
    "        self.encodernorms.append( torch.nn.BatchNorm1d(512) )\n",
    "\n",
    "        self.encoderlayers.append( torch.nn.Linear(512, 512) )\n",
    "        self.encodernorms.append( torch.nn.BatchNorm1d(512) )\n",
    "\n",
    "\n",
    "        # LATENT LAYERS\n",
    "        self.mu = torch.nn.Linear(512, self.nlatent)\n",
    "        self.logsigma = torch.nn.Linear(512, self.nlatent)\n",
    "\n",
    "\n",
    "        # DECODER LAYRS\n",
    "        self.decoderlayers.append(torch.nn.Linear(self.nlatent, 512))\n",
    "        self.decodernorms.append(torch.nn.BatchNorm1d(512))\n",
    "\n",
    "        self.decoderlayers.append(torch.nn.Linear(512, 512))\n",
    "        self.decodernorms.append(torch.nn.BatchNorm1d(512))\n",
    "\n",
    "\n",
    "        # RECONSTRUCTION LAYER\n",
    "        self.outputlayer = torch.nn.Linear(512, (self.nsamples + self.ntnf) )\n",
    "\n",
    "\n",
    "        # ACTIVATIONS\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.dropoutlayer = torch.nn.Dropout(p=self.dropout)\n",
    "\n",
    "        \n",
    "    ###\n",
    "    # ENCODE NEW CONTIGS TO LATENT SPACE\n",
    "    ###\n",
    "    def encode(self, data_loader):\n",
    "        self.eval()\n",
    "\n",
    "        new_data_loader = DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False,\n",
    "                                      num_workers=4,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        depths_array, tnf_array = data_loader.dataset.tensors\n",
    "        length = len(depths_array)\n",
    "\n",
    "        latent = np.empty((length, self.nlatent), dtype=np.float32)\n",
    "\n",
    "        row = 0\n",
    "        with torch.no_grad():\n",
    "            for depths, tnf in new_data_loader:\n",
    "                if self.cuda_on:\n",
    "                    depths = depths.cuda()\n",
    "                    tnf = tnf.cuda()\n",
    "\n",
    "                # Evaluate\n",
    "                out_depths, out_tnf, mu, logsigma = self(depths, tnf)\n",
    "\n",
    "                if self.cuda_on:\n",
    "                    mu = mu.cpu()\n",
    "\n",
    "                latent[row: row + len(mu)] = mu\n",
    "                row += len(mu)\n",
    "\n",
    "        assert row == length\n",
    "        return latent\n",
    "    \n",
    "    ###\n",
    "    # SPECIFIC ENCODING AND DECODING FUNCTIONS\n",
    "    ###\n",
    "    # REPARAMATERIZE\n",
    "    def reparameterize(self, mu, logsigma):\n",
    "        epsilon = torch.randn(mu.size(0), mu.size(1))\n",
    "\n",
    "        if self.cuda_on:\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        epsilon.requires_grad = True\n",
    "\n",
    "        # See comment above regarding softplus\n",
    "        latent = mu + epsilon * torch.exp(logsigma/2)\n",
    "\n",
    "        return latent\n",
    "    \n",
    "    \n",
    "    # ENCODE CONTIGS\n",
    "    def encode_contigs(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        # Hidden layers\n",
    "        for encoderlayer, encodernorm in zip(self.encoderlayers, self.encodernorms):\n",
    "            tensor = encodernorm(self.dropoutlayer(self.relu(encoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        # Latent layers\n",
    "        mu = self.mu(tensor)\n",
    "        logsigma = self.softplus(self.logsigma(tensor))\n",
    "\n",
    "        return mu, logsigma\n",
    "    \n",
    "    \n",
    "    # DECODE CONTIGS\n",
    "    def decode_contigs(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        for decoderlayer, decodernorm in zip(self.decoderlayers, self.decodernorms):\n",
    "            tensor = decodernorm(self.dropoutlayer(self.relu(decoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        reconstruction = self.outputlayer(tensor)\n",
    "\n",
    "        # Decompose reconstruction to depths and tnf signal\n",
    "        depths_out = reconstruction.narrow(1, 0, self.nsamples)\n",
    "        tnf_out = reconstruction.narrow(1, self.nsamples, tnfs.shape[1])\n",
    "\n",
    "        return depths_out, tnf_out\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # LOSS CALCULATION\n",
    "    ###\n",
    "    # CALCULATE LOSS\n",
    "    def calc_loss(self, depths_in, depths_out, tnf_in, tnf_out, mu, logsigma):\n",
    "        ce = (depths_out - depths_in).pow(2).sum(dim=1).mean()\n",
    "        ce_weight = 1 - 0.15 # alpha\n",
    "\n",
    "        sse = (tnf_out - tnf_in).pow(2).sum(dim=1).mean()\n",
    "        kld = -0.5 * (1 + logsigma - mu.pow(2) - logsigma.exp()).sum(dim=1).mean()\n",
    "\n",
    "        sse_weight = 0.15 / self.ntnf # alpha / ntnf\n",
    "        # BETA PARAMETER HERE\n",
    "        kld_weight = 1 / (self.nlatent * self.beta)\n",
    "        loss = ce * ce_weight + sse * sse_weight + kld * kld_weight\n",
    "\n",
    "        return loss, ce, sse, kld\n",
    "    \n",
    "\n",
    "    ###\n",
    "    # TRAINING FUNCTIONS\n",
    "    ###\n",
    "    # FORWARD\n",
    "    def forward(self, depths, tnf):\n",
    "        tensor = torch.cat((depths, tnf), 1)\n",
    "        mu, logsigma = self.encode_contigs(tensor)\n",
    "        latent = self.reparameterize(mu, logsigma)\n",
    "        depths_out, tnf_out = self.decode_contigs(latent)\n",
    "\n",
    "        return depths_out, tnf_out, mu, logsigma   \n",
    "        \n",
    "     \n",
    "    \n",
    "    # TRAIN SPECIFIC EPOCH\n",
    "    def trainepoch(self, data_loader, epoch, optimizer, batchsteps):\n",
    "        self.train()\n",
    "\n",
    "        epoch_loss, epoch_kldloss, epoch_sseloss, epoch_celoss = 0, 0, 0, 0\n",
    "\n",
    "        if epoch in batchsteps:\n",
    "            data_loader = DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size * 2,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True,\n",
    "                                      num_workers=data_loader.num_workers,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        for depths_in, tnf_in in data_loader:\n",
    "            depths_in.requires_grad = True\n",
    "            tnf_in.requires_grad = True\n",
    "\n",
    "            # CUDA ENABLING\n",
    "            if self.cuda_on:\n",
    "                depths_in = depths_in.cuda()\n",
    "                tnf_in = tnf_in.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            depths_out, tnf_out, mu, logsigma = self(depths_in, tnf_in)\n",
    "\n",
    "            loss, ce, sse, kld = self.calc_loss(depths_in, depths_out, tnf_in,\n",
    "                                                  tnf_out, mu, logsigma)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss = epoch_loss + loss.data.item()\n",
    "            epoch_kldloss = epoch_kldloss + kld.data.item()\n",
    "            epoch_sseloss = epoch_sseloss + sse.data.item()\n",
    "            epoch_celoss = epoch_celoss + ce.data.item()\n",
    "\n",
    "        print('\\tEpoch: {}\\tLoss: {:.6f}\\tCE: {:.7f}\\tSSE: {:.6f}\\tKLD: {:.4f}\\tBatchsize: {}'.format(\n",
    "              epoch + 1,\n",
    "              epoch_loss / len(data_loader),\n",
    "              epoch_celoss / len(data_loader),\n",
    "              epoch_sseloss / len(data_loader),\n",
    "              epoch_kldloss / len(data_loader),\n",
    "              data_loader.batch_size,\n",
    "              ))\n",
    "        wandb.log({\n",
    "            \"epoch\": (epoch+1), \n",
    "            \"loss\": epoch_loss / len(data_loader),\n",
    "            \"CELoss\": epoch_celoss / len(data_loader),\n",
    "            \"SSELoss\": epoch_sseloss / len(data_loader),\n",
    "            \"KLDLoss\": epoch_kldloss / len(data_loader),\n",
    "            \"Batchsize\": data_loader.batch_size\n",
    "        })\n",
    "\n",
    "        return data_loader\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TRAIN MODEL    \n",
    "    def trainmodel(self, dataloader, batchsteps=[25, 75, 150, 300], modelfile=None):\n",
    "        \n",
    "        batchsteps_set = set()\n",
    "        \n",
    "        ncontigs, nsamples = dataloader.dataset.tensors[0].shape\n",
    "        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        \n",
    "        # TRAIN EPOCH\n",
    "        for epoch in range(self.nepochs):\n",
    "            dataloader = self.trainepoch(dataloader, epoch, optimizer, batchsteps_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: vtxaskgw\n",
      "Sweep URL: https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/vtxaskgw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q5h7addy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepochs: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnlatent: 64\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpmccaffrey6\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lucky-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/vtxaskgw\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/vtxaskgw</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/runs/q5h7addy\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/runs/q5h7addy</a><br/>\n",
       "                Run data is saved locally in <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124805-q5h7addy</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 1\tLoss: 1.535106\tCE: 1.1134370\tSSE: 135.248749\tKLD: 25.0701\tBatchsize: 256\n",
      "\tEpoch: 2\tLoss: 1.361063\tCE: 0.8750011\tSSE: 127.516237\tKLD: 27.6230\tBatchsize: 256\n",
      "\tEpoch: 3\tLoss: 1.155872\tCE: 0.6211215\tSSE: 121.560579\tKLD: 28.8569\tBatchsize: 256\n",
      "\tEpoch: 4\tLoss: 1.055479\tCE: 0.5051520\tSSE: 113.664375\tKLD: 29.4764\tBatchsize: 256\n",
      "\tEpoch: 5\tLoss: 1.044187\tCE: 0.5395366\tSSE: 111.838084\tKLD: 27.0535\tBatchsize: 256\n",
      "\tEpoch: 6\tLoss: 0.917017\tCE: 0.4194311\tSSE: 106.245908\tKLD: 25.9695\tBatchsize: 256\n",
      "\tEpoch: 7\tLoss: 0.871028\tCE: 0.3868858\tSSE: 100.416098\tKLD: 25.3400\tBatchsize: 256\n",
      "\tEpoch: 8\tLoss: 0.812877\tCE: 0.3316474\tSSE: 92.940588\tKLD: 25.3201\tBatchsize: 256\n",
      "\tEpoch: 9\tLoss: 0.779238\tCE: 0.3131797\tSSE: 90.637419\tKLD: 24.3865\tBatchsize: 256\n",
      "\tEpoch: 10\tLoss: 0.750733\tCE: 0.2956007\tSSE: 87.233363\tKLD: 23.8358\tBatchsize: 256\n",
      "\tEpoch: 11\tLoss: 0.742194\tCE: 0.3132914\tSSE: 85.040640\tKLD: 22.5312\tBatchsize: 256\n",
      "\tEpoch: 12\tLoss: 0.712618\tCE: 0.2916597\tSSE: 81.064717\tKLD: 22.1857\tBatchsize: 256\n",
      "\tEpoch: 13\tLoss: 0.717297\tCE: 0.3086326\tSSE: 82.552377\tKLD: 21.4232\tBatchsize: 256\n",
      "\tEpoch: 14\tLoss: 0.652883\tCE: 0.2536414\tSSE: 80.992879\tKLD: 20.4376\tBatchsize: 256\n",
      "\tEpoch: 15\tLoss: 0.640077\tCE: 0.2539775\tSSE: 79.602348\tKLD: 19.7293\tBatchsize: 256\n",
      "\tEpoch: 16\tLoss: 0.623386\tCE: 0.2506988\tSSE: 77.670761\tKLD: 19.0195\tBatchsize: 256\n",
      "\tEpoch: 17\tLoss: 0.605036\tCE: 0.2428762\tSSE: 77.822922\tKLD: 18.2565\tBatchsize: 256\n",
      "\tEpoch: 18\tLoss: 0.582533\tCE: 0.2344614\tSSE: 74.449112\tKLD: 17.5885\tBatchsize: 256\n",
      "\tEpoch: 19\tLoss: 0.559076\tCE: 0.2347659\tSSE: 75.099118\tKLD: 16.0100\tBatchsize: 256\n",
      "\tEpoch: 20\tLoss: 0.540873\tCE: 0.2208882\tSSE: 77.863014\tKLD: 15.3424\tBatchsize: 256\n",
      "\tEpoch: 21\tLoss: 0.534202\tCE: 0.2333773\tSSE: 71.827588\tKLD: 14.7986\tBatchsize: 256\n",
      "\tEpoch: 22\tLoss: 0.520048\tCE: 0.2253583\tSSE: 73.014919\tKLD: 14.2183\tBatchsize: 256\n",
      "\tEpoch: 23\tLoss: 0.513468\tCE: 0.2325357\tSSE: 71.627258\tKLD: 13.5361\tBatchsize: 256\n",
      "\tEpoch: 24\tLoss: 0.493994\tCE: 0.2267355\tSSE: 71.775853\tKLD: 12.5914\tBatchsize: 256\n",
      "\tEpoch: 25\tLoss: 0.485047\tCE: 0.2218781\tSSE: 70.672755\tKLD: 12.3859\tBatchsize: 256\n",
      "\tEpoch: 26\tLoss: 0.466594\tCE: 0.2123911\tSSE: 68.570839\tKLD: 11.9169\tBatchsize: 256\n",
      "\tEpoch: 27\tLoss: 0.462664\tCE: 0.2233903\tSSE: 69.055685\tKLD: 11.0218\tBatchsize: 256\n",
      "\tEpoch: 28\tLoss: 0.463286\tCE: 0.2269233\tSSE: 70.069801\tKLD: 10.7749\tBatchsize: 256\n",
      "\tEpoch: 29\tLoss: 0.413206\tCE: 0.1843957\tSSE: 67.161457\tKLD: 10.1544\tBatchsize: 256\n",
      "\tEpoch: 30\tLoss: 0.447627\tCE: 0.2342606\tSSE: 68.012228\tKLD: 9.5653\tBatchsize: 256\n",
      "\tEpoch: 31\tLoss: 0.420993\tCE: 0.1902182\tSSE: 68.168201\tKLD: 10.2421\tBatchsize: 256\n",
      "\tEpoch: 32\tLoss: 0.425277\tCE: 0.2011094\tSSE: 67.970667\tKLD: 9.9423\tBatchsize: 256\n",
      "\tEpoch: 33\tLoss: 0.412735\tCE: 0.2002333\tSSE: 65.252449\tKLD: 9.4406\tBatchsize: 256\n",
      "\tEpoch: 34\tLoss: 0.414395\tCE: 0.2128216\tSSE: 64.999570\tKLD: 8.8856\tBatchsize: 256\n",
      "\tEpoch: 35\tLoss: 0.381703\tCE: 0.1729830\tSSE: 66.162563\tKLD: 8.8521\tBatchsize: 256\n",
      "\tEpoch: 36\tLoss: 0.391916\tCE: 0.1815966\tSSE: 65.389636\tKLD: 9.1092\tBatchsize: 256\n"
     ]
    }
   ],
   "source": [
    "wandb_on = True\n",
    "sweep_on = True\n",
    "\n",
    "\n",
    "if wandb_on:\n",
    "    \n",
    "    #beta_vae = DISENTANGLED_BETA_VAE(nsamples=rpkms.shape[1], config=config)\n",
    "    #wandb.watch(beta_vae)\n",
    "    #beta_vae.trainmodel(dataloader, batchsteps=None)\n",
    "\n",
    "    if sweep_on:\n",
    "        sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'early_terminate': {\n",
    "          'type': 'hyperband',\n",
    "          'max_iter': 27,\n",
    "          's': 2\n",
    "        },\n",
    "        'metric': {\n",
    "          'name': 'loss',\n",
    "          'goal': 'minimize'   \n",
    "            },\n",
    "            'parameters': {\n",
    "                'nepochs': {\n",
    "                    'values': [300]\n",
    "                },\n",
    "                'dropout': {\n",
    "                    'values': [0.2, 0.4, 0.6]\n",
    "                },\n",
    "                'learning_rate': {\n",
    "                    'values': [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "                },\n",
    "                'alpha': {\n",
    "                    'values': [0.15, 0.25]\n",
    "                },\n",
    "                'beta': {\n",
    "                    'values': [1, 200, 400, 800]\n",
    "                },\n",
    "                'nlatent': {\n",
    "                    'values': [32, 64]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "        def train():\n",
    "            config_defaults = {\n",
    "                'nepochs': 5,\n",
    "                'dropout': 0.2,\n",
    "                'learning_rate': 1e-3,\n",
    "                'alpha': 0.15,\n",
    "                'beta': 200,\n",
    "                'nlatent': 32\n",
    "            }\n",
    "            \n",
    "            wandb.init(project='cs_230_vae', entity='pmccaffrey6', config=config_defaults)\n",
    "            \n",
    "            config = wandb.config\n",
    "            beta_vae = DISENTANGLED_BETA_VAE(nsamples=rpkms.shape[1], config=config)\n",
    "            wandb.watch(beta_vae)\n",
    "            beta_vae.trainmodel(dataloader, batchsteps=None)\n",
    "            print('rundir:', wandb.run.dir)\n",
    "            torch.save(beta_vae.state_dict(), os.path.join(wandb.run.dir, \"model.h5\"))\n",
    "            \n",
    "\n",
    "        sweep_id = wandb.sweep(sweep_config, entity=\"pmccaffrey6\", project=\"cs_230_vae\")\n",
    "        wandb.agent(sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best run volcanic-sweep-7 with 1.3094314575195312% validation accuracy\n",
      "best_params_dict: {'beta': {'value': 200, 'desc': None}, 'alpha': {'value': 0.15, 'desc': None}, 'dropout': {'value': 0.6, 'desc': None}, 'nepochs': {'value': 5, 'desc': None}, 'nlatent': {'value': 32, 'desc': None}, 'learning_rate': {'value': 1e-05, 'desc': None}}\n"
     ]
    }
   ],
   "source": [
    "# GET THE BEST MODEL'S PARAMS FROM THE WANDB API\n",
    "api = wandb.Api()\n",
    "print()\n",
    "sweep = api.sweep(f\"pmccaffrey6/cs_230_vae/{sweep_id}\")\n",
    "\n",
    "runs = sorted(sweep.runs, key=lambda run: run.summary.get(\"loss\", 0), reverse=True)\n",
    "loss = runs[0].summary.get(\"loss\", 0)\n",
    "\n",
    "best_run = runs[0].name\n",
    "\n",
    "print(f\"Best run {best_run} with {loss}% validation accuracy\")\n",
    "best_params_dict = json.loads(runs[0].json_config)\n",
    "print('best_params_dict:', best_params_dict)\n",
    "\n",
    "def convert(dictionary):\n",
    "    return namedtuple('GenericDict', dictionary.keys())(**dictionary)\n",
    "\n",
    "best_params = convert( {key:best_params_dict[key]['value'] for key in best_params_dict.keys()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='./model.h5' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs[0].file(\"model.h5\").download(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape: (1342, 32)\n"
     ]
    }
   ],
   "source": [
    "beta_vae = DISENTANGLED_BETA_VAE(nsamples=rpkms.shape[1], config=best_params)\n",
    "beta_vae.load_state_dict(torch.load('model.h5'))\n",
    "\n",
    "latent = beta_vae.encode(dataloader)\n",
    "print(\"Latent shape:\", latent.shape)\n",
    "\n",
    "latent_output_path = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs/latent_space.npy')\n",
    "with open(latent_output_path, 'wb') as outfile:\n",
    "    np.save(outfile, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leverage Other VAMB Tools for Clustering and Post Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_mapping_table = pd.read_csv(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/gsa_mapping.tsv\"), sep='\\t')\n",
    "\n",
    "contig_mapping_output_path = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs/encoding_mapping.tsv')    \n",
    "\n",
    "contig_mapping_table[contig_mapping_table['#anonymous_contig_id'].isin(contignames)].reset_index().drop('index', axis=1).set_index(\n",
    "    '#anonymous_contig_id').reindex(contignames).to_csv(contig_mapping_output_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: S0C34345 (of type: <class 'numpy.str_'> )\n",
      "Type of values: <class 'set'>\n",
      "First element of value: S0C34345 of type: <class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "filtered_labels = [n for (n,m) in zip(contignames, mask) if m]\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=filtered_labels)\n",
    "clusters = dict(cluster_iterator)\n",
    "\n",
    "medoid, contigs = next(iter(clusters.items()))\n",
    "print('First key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before splitting and filtering: 1163\n",
      "Number of bins after splitting and filtering: 3\n"
     ]
    }
   ],
   "source": [
    "def filterclusters(clusters, lengthof):\n",
    "    filtered_bins = dict()\n",
    "    for medoid, contigs in clusters.items():\n",
    "        binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "        if binsize >= 200000:\n",
    "            filtered_bins[medoid] = contigs\n",
    "    \n",
    "    return filtered_bins\n",
    "        \n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "filtered_bins = filterclusters(vamb.vambtools.binsplit(clusters, 'C'), lengthof)\n",
    "print('Number of bins before splitting and filtering:', len(clusters))\n",
    "print('Number of bins after splitting and filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_outputs_base = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs')\n",
    "\n",
    "if not os.path.exists(vamb_outputs_base):\n",
    "    os.mkdir(vamb_outputs_base)\n",
    "    \n",
    "\n",
    "# This writes a .tsv file with the clusters and corresponding sequences\n",
    "with open(os.path.join(vamb_outputs_base, 'clusters_dvae.tsv'), 'w') as file:\n",
    "    vamb.vambtools.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "keptcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "\n",
    "\n",
    "# decompress fasta.gz if present\n",
    "fasta_path = os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta.gz\")\n",
    "if os.path.exists(fasta_path) and not os.path.exists(fasta_path.replace('.fasta.gz','.fasta')):\n",
    "    !gzip -dk $fasta_path\n",
    "\n",
    "\n",
    "with open(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta\"), 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=keptcontigs)\n",
    "\n",
    "\n",
    "bindir = os.path.join(vamb_outputs_base, 'dvae_bins')\n",
    "if not os.path.exists(bindir):\n",
    "    os.mkdir(bindir)\n",
    "vamb.vambtools.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vamb_env",
   "language": "python",
   "name": "vamb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
