{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb4e409b930>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vamb\n",
    "\n",
    "import numpy as np\n",
    "import torch as _torch\n",
    "import torch.nn as _nn\n",
    "from torch.optim import Adam as _Adam\n",
    "from torch.utils.data import DataLoader as _DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "EXAMPLE_FASTA_FILE = '2021.01.26_15.46.45_sample_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_inputs_base = os.path.join(BASE_DIR,'example_input_data/new_simulations/camisim_outputs/vamb_inputs')\n",
    "\n",
    "contignames = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'contignames.npz'))\n",
    "\n",
    "lengths = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'lengths.npz'))\n",
    "\n",
    "tnfs = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'tnfs.npz'))\n",
    "    \n",
    "rpkms = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'rpkms.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, mask = vamb.encode.make_dataloader(rpkms, tnfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncontigs, nsamples = dataloader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DISENTANGLED_VAE(torch.nn.Module):\n",
    "    def __init__(self, nsamples, nhiddens=[512, 512], nlatent=32, alpha=0.15,\n",
    "                 beta=200, dropout=0.2, cuda=True):\n",
    "        \n",
    "\n",
    "\n",
    "        super(DISENTANGLED_VAE, self).__init__()\n",
    "\n",
    "        # Initialize simple attributes\n",
    "        self.usecuda = cuda\n",
    "        self.nsamples = nsamples\n",
    "        self.ntnf = 103\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.nhiddens = nhiddens\n",
    "        self.nlatent = nlatent\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize lists for holding hidden layers\n",
    "        self.encoderlayers = torch.nn.ModuleList()\n",
    "        self.encodernorms = torch.nn.ModuleList()\n",
    "        self.decoderlayers = torch.nn.ModuleList()\n",
    "        self.decodernorms = torch.nn.ModuleList()\n",
    "\n",
    "        # Add all other hidden layers\n",
    "        for nin, nout in zip([self.nsamples + self.ntnf] + self.nhiddens, self.nhiddens):\n",
    "            self.encoderlayers.append(torch.nn.Linear(nin, nout))\n",
    "            self.encodernorms.append(torch.nn.BatchNorm1d(nout))\n",
    "\n",
    "        # Latent layers\n",
    "        self.mu = torch.nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "        self.logsigma = torch.nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "\n",
    "        # Add first decoding layer\n",
    "        for nin, nout in zip([self.nlatent] + self.nhiddens[::-1], self.nhiddens[::-1]):\n",
    "            self.decoderlayers.append(torch.nn.Linear(nin, nout))\n",
    "            self.decodernorms.append(torch.nn.BatchNorm1d(nout))\n",
    "\n",
    "        # Reconstruction (output) layer\n",
    "        self.outputlayer = torch.nn.Linear(self.nhiddens[0], self.nsamples + self.ntnf)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.dropoutlayer = torch.nn.Dropout(p=self.dropout)\n",
    "\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def _encode(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        # Hidden layers\n",
    "        for encoderlayer, encodernorm in zip(self.encoderlayers, self.encodernorms):\n",
    "            tensor = encodernorm(self.dropoutlayer(self.relu(encoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        # Latent layers\n",
    "        mu = self.mu(tensor)\n",
    "\n",
    "        logsigma = self.softplus(self.logsigma(tensor))\n",
    "\n",
    "        return mu, logsigma\n",
    "\n",
    "    # sample with gaussian noise\n",
    "    def reparameterize(self, mu, logsigma):\n",
    "        epsilon = _torch.randn(mu.size(0), mu.size(1))\n",
    "\n",
    "        if self.usecuda:\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        epsilon.requires_grad = True\n",
    "\n",
    "        # See comment above regarding softplus\n",
    "        latent = mu + epsilon * _torch.exp(logsigma/2)\n",
    "\n",
    "        return latent\n",
    "\n",
    "    def _decode(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        for decoderlayer, decodernorm in zip(self.decoderlayers, self.decodernorms):\n",
    "            tensor = decodernorm(self.dropoutlayer(self.relu(decoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        reconstruction = self.outputlayer(tensor)\n",
    "\n",
    "        # Decompose reconstruction to depths and tnf signal\n",
    "        depths_out = reconstruction.narrow(1, 0, self.nsamples)\n",
    "        tnf_out = reconstruction.narrow(1, self.nsamples, self.ntnf)\n",
    "\n",
    "        # If multiple samples, apply softmax\n",
    "        if self.nsamples > 1:\n",
    "            depths_out = _softmax(depths_out, dim=1)\n",
    "\n",
    "        return depths_out, tnf_out\n",
    "\n",
    "    def forward(self, depths, tnf):\n",
    "        tensor = _torch.cat((depths, tnf), 1)\n",
    "        mu, logsigma = self._encode(tensor)\n",
    "        latent = self.reparameterize(mu, logsigma)\n",
    "        depths_out, tnf_out = self._decode(latent)\n",
    "\n",
    "        return depths_out, tnf_out, mu, logsigma\n",
    "\n",
    "    def calc_loss(self, depths_in, depths_out, tnf_in, tnf_out, mu, logsigma):\n",
    "        # If multiple samples, use cross entropy, else use SSE for abundance\n",
    "        if self.nsamples > 1:\n",
    "            # Add 1e-9 to depths_out to avoid numerical instability.\n",
    "            ce = - ((depths_out + 1e-9).log() * depths_in).sum(dim=1).mean()\n",
    "            ce_weight = (1 - self.alpha) / _log(self.nsamples)\n",
    "        else:\n",
    "            ce = (depths_out - depths_in).pow(2).sum(dim=1).mean()\n",
    "            ce_weight = 1 - self.alpha\n",
    "\n",
    "        sse = (tnf_out - tnf_in).pow(2).sum(dim=1).mean()\n",
    "        kld = -0.5 * (1 + logsigma - mu.pow(2) - logsigma.exp()).sum(dim=1).mean()\n",
    "\n",
    "        sse_weight = self.alpha / self.ntnf\n",
    "        kld_weight = 1 / (self.nlatent * self.beta)\n",
    "        loss = ce * ce_weight + sse * sse_weight + kld * kld_weight\n",
    "\n",
    "        return loss, ce, sse, kld\n",
    "\n",
    "    def trainepoch(self, data_loader, epoch, optimizer, batchsteps, logfile):\n",
    "        self.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_kldloss = 0\n",
    "        epoch_sseloss = 0\n",
    "        epoch_celoss = 0\n",
    "\n",
    "        if epoch in batchsteps:\n",
    "            data_loader = _DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size * 2,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True,\n",
    "                                      num_workers=data_loader.num_workers,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        for depths_in, tnf_in in data_loader:\n",
    "            depths_in.requires_grad = True\n",
    "            tnf_in.requires_grad = True\n",
    "\n",
    "            if self.usecuda:\n",
    "                depths_in = depths_in.cuda()\n",
    "                tnf_in = tnf_in.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            depths_out, tnf_out, mu, logsigma = self(depths_in, tnf_in)\n",
    "\n",
    "            loss, ce, sse, kld = self.calc_loss(depths_in, depths_out, tnf_in,\n",
    "                                                  tnf_out, mu, logsigma)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.data.item()\n",
    "            epoch_kldloss += kld.data.item()\n",
    "            epoch_sseloss += sse.data.item()\n",
    "            epoch_celoss += ce.data.item()\n",
    "\n",
    "        print('\\tEpoch: {}\\tLoss: {:.6f}\\tCE: {:.7f}\\tSSE: {:.6f}\\tKLD: {:.4f}\\tBatchsize: {}'.format(\n",
    "              epoch + 1,\n",
    "              epoch_loss / len(data_loader),\n",
    "              epoch_celoss / len(data_loader),\n",
    "              epoch_sseloss / len(data_loader),\n",
    "              epoch_kldloss / len(data_loader),\n",
    "              data_loader.batch_size,\n",
    "              ), file=logfile)\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def encode(self, data_loader):\n",
    "        \"\"\"Encode a data loader to a latent representation with VAE\n",
    "        Input: data_loader: As generated by train_vae\n",
    "        Output: A (n_contigs x n_latent) Numpy array of latent repr.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        new_data_loader = _DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False,\n",
    "                                      num_workers=1,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        depths_array, tnf_array = data_loader.dataset.tensors\n",
    "        length = len(depths_array)\n",
    "\n",
    "        # We make a Numpy array instead of a Torch array because, if we create\n",
    "        # a Torch array, then convert it to Numpy, Numpy will believe it doesn't\n",
    "        # own the memory block, and array resizes will not be permitted.\n",
    "        latent = np.empty((length, self.nlatent), dtype=np.float32)\n",
    "\n",
    "        row = 0\n",
    "        with _torch.no_grad():\n",
    "            for depths, tnf in new_data_loader:\n",
    "                # Move input to GPU if requested\n",
    "                if self.usecuda:\n",
    "                    depths = depths.cuda()\n",
    "                    tnf = tnf.cuda()\n",
    "\n",
    "                # Evaluate\n",
    "                out_depths, out_tnf, mu, logsigma = self(depths, tnf)\n",
    "\n",
    "                if self.usecuda:\n",
    "                    mu = mu.cpu()\n",
    "\n",
    "                latent[row: row + len(mu)] = mu\n",
    "                row += len(mu)\n",
    "\n",
    "        assert row == length\n",
    "        return latent\n",
    "\n",
    "    def save(self, filehandle):\n",
    "        \"\"\"Saves the VAE to a path or binary opened file. Load with VAE.load\n",
    "        Input: Path or binary opened filehandle\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        state = {'nsamples': self.nsamples,\n",
    "                 'alpha': self.alpha,\n",
    "                 'beta': self.beta,\n",
    "                 'dropout': self.dropout,\n",
    "                 'nhiddens': self.nhiddens,\n",
    "                 'nlatent': self.nlatent,\n",
    "                 'state': self.state_dict(),\n",
    "                }\n",
    "\n",
    "        _torch.save(state, filehandle)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, cuda=False, evaluate=True):\n",
    "        \"\"\"Instantiates a VAE from a model file.\n",
    "        Inputs:\n",
    "            path: Path to model file as created by functions VAE.save or\n",
    "                  VAE.trainmodel.\n",
    "            cuda: If network should work on GPU [False]\n",
    "            evaluate: Return network in evaluation mode [True]\n",
    "        Output: VAE with weights and parameters matching the saved network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Forcably load to CPU even if model was saves as GPU model\n",
    "        dictionary = _torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        nsamples = dictionary['nsamples']\n",
    "        alpha = dictionary['alpha']\n",
    "        beta = dictionary['beta']\n",
    "        dropout = dictionary['dropout']\n",
    "        nhiddens = dictionary['nhiddens']\n",
    "        nlatent = dictionary['nlatent']\n",
    "        state = dictionary['state']\n",
    "\n",
    "        vae = cls(nsamples, nhiddens, nlatent, alpha, beta, dropout, cuda)\n",
    "        vae.load_state_dict(state)\n",
    "\n",
    "        if cuda:\n",
    "            vae.cuda()\n",
    "\n",
    "        if evaluate:\n",
    "            vae.eval()\n",
    "\n",
    "        return vae\n",
    "\n",
    "    def trainmodel(self, dataloader, nepochs=500, lrate=1e-3,\n",
    "                   batchsteps=[25, 75, 150, 300], logfile=None, modelfile=None):\n",
    "\n",
    "        if batchsteps is None:\n",
    "            batchsteps_set = set()\n",
    "        else:\n",
    "            # First collect to list in order to allow all element types, then check that\n",
    "            # they are integers\n",
    "            batchsteps = list(batchsteps)\n",
    "            last_batchsize = dataloader.batch_size * 2**len(batchsteps)\n",
    "            batchsteps_set = set(batchsteps)\n",
    "\n",
    "        # Get number of features\n",
    "        ncontigs, nsamples = dataloader.dataset.tensors[0].shape\n",
    "        optimizer = _Adam(self.parameters(), lr=lrate)\n",
    "\n",
    "        if logfile is not None:\n",
    "            print('\\tNetwork properties:', file=logfile)\n",
    "            print('\\tCUDA:', self.usecuda, file=logfile)\n",
    "            print('\\tAlpha:', self.alpha, file=logfile)\n",
    "            print('\\tBeta:', self.beta, file=logfile)\n",
    "            print('\\tDropout:', self.dropout, file=logfile)\n",
    "            print('\\tN hidden:', ', '.join(map(str, self.nhiddens)), file=logfile)\n",
    "            print('\\tN latent:', self.nlatent, file=logfile)\n",
    "            print('\\n\\tTraining properties:', file=logfile)\n",
    "            print('\\tN epochs:', nepochs, file=logfile)\n",
    "            print('\\tStarting batch size:', dataloader.batch_size, file=logfile)\n",
    "            batchsteps_string = ', '.join(map(str, sorted(batchsteps))) if batchsteps_set else \"None\"\n",
    "            print('\\tBatchsteps:', batchsteps_string, file=logfile)\n",
    "            print('\\tLearning rate:', lrate, file=logfile)\n",
    "            print('\\tN sequences:', ncontigs, file=logfile)\n",
    "            print('\\tN samples:', nsamples, file=logfile, end='\\n\\n')\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(nepochs):\n",
    "            dataloader = self.trainepoch(dataloader, epoch, optimizer, batchsteps_set, logfile)\n",
    "\n",
    "        # Save weights - Lord forgive me, for I have sinned when catching all exceptions\n",
    "        if modelfile is not None:\n",
    "            try:\n",
    "                self.save(modelfile)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNetwork properties:\n",
      "\tCUDA: True\n",
      "\tAlpha: 0.15\n",
      "\tBeta: 500\n",
      "\tDropout: 0.2\n",
      "\tN hidden: 512, 512\n",
      "\tN latent: 32\n",
      "\n",
      "\tTraining properties:\n",
      "\tN epochs: 3\n",
      "\tStarting batch size: 256\n",
      "\tBatchsteps: None\n",
      "\tLearning rate: 0.001\n",
      "\tN sequences: 1342\n",
      "\tN samples: 1\n",
      "\n",
      "\tEpoch: 1\tLoss: 1.108918\tCE: 1.0818523\tSSE: 129.341353\tKLD: 15.7224\tBatchsize: 256\n",
      "\tEpoch: 2\tLoss: 0.754742\tCE: 0.7182180\tSSE: 97.910126\tKLD: 26.7025\tBatchsize: 256\n",
      "\tEpoch: 3\tLoss: 0.559818\tCE: 0.5291639\tSSE: 73.930818\tKLD: 37.8028\tBatchsize: 256\n"
     ]
    }
   ],
   "source": [
    "dvae = DISENTANGLED_VAE(nsamples=rpkms.shape[1], beta=500)\n",
    "\n",
    "with open('vamb_models/model_disentangled_vae.pt', 'wb') as modelfile:\n",
    "    dvae.trainmodel(dataloader, nepochs=3, modelfile=modelfile, batchsteps=None, logfile=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1342, 32)\n"
     ]
    }
   ],
   "source": [
    "latent = dvae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: S0C3114 (of type: <class 'numpy.str_'> )\n",
      "Type of values: <class 'set'>\n",
      "First element of value: S0C34657 of type: <class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "filtered_labels = [n for (n,m) in zip(contignames, mask) if m]\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=filtered_labels)\n",
    "clusters = dict(cluster_iterator)\n",
    "\n",
    "medoid, contigs = next(iter(clusters.items()))\n",
    "print('First key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12732136, -0.74198747, -0.35445487, ..., -0.4132206 ,\n",
       "         2.4283357 , -0.06945899],\n",
       "       [-0.8385641 , -1.0267823 ,  0.97479594, ..., -1.1811213 ,\n",
       "        -0.231461  ,  0.35985753],\n",
       "       [-0.15307364, -0.296144  ,  0.44146183, ..., -0.8366548 ,\n",
       "         0.60108423,  0.06524898],\n",
       "       ...,\n",
       "       [-0.46313658, -0.23217835,  0.5262804 , ..., -0.99592835,\n",
       "         0.31074715,  0.3232081 ],\n",
       "       [-0.03562532, -0.30374518, -0.73882926, ..., -0.4130666 ,\n",
       "         1.0391623 , -0.53838146],\n",
       "       [ 0.35486057,  0.76649994, -0.4772173 , ...,  1.0366642 ,\n",
       "        -0.38499177,  0.0935266 ]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before splitting and filtering: 18\n",
      "Number of bins after splitting and filtering: 4\n"
     ]
    }
   ],
   "source": [
    "def filterclusters(clusters, lengthof):\n",
    "    filtered_bins = dict()\n",
    "    for medoid, contigs in clusters.items():\n",
    "        binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "        if binsize >= 200000:\n",
    "            filtered_bins[medoid] = contigs\n",
    "    \n",
    "    return filtered_bins\n",
    "        \n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "filtered_bins = filterclusters(vamb.vambtools.binsplit(clusters, 'C'), lengthof)\n",
    "print('Number of bins before splitting and filtering:', len(clusters))\n",
    "print('Number of bins after splitting and filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_outputs_base = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs')\n",
    "\n",
    "if not os.path.exists(vamb_outputs_base):\n",
    "    os.mkdir(vamb_outputs_base)\n",
    "    \n",
    "\n",
    "# This writes a .tsv file with the clusters and corresponding sequences\n",
    "with open(os.path.join(vamb_outputs_base, 'clusters_dvae.tsv'), 'w') as file:\n",
    "    vamb.vambtools.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "keptcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "\n",
    "\n",
    "# decompress fasta.gz if present\n",
    "fasta_path = os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta.gz\")\n",
    "if os.path.exists(fasta_path) and not os.path.exists(fasta_path.replace('.fasta.gz','.fasta')):\n",
    "    !gzip -dk $fasta_path\n",
    "\n",
    "\n",
    "with open(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta\"), 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=keptcontigs)\n",
    "\n",
    "\n",
    "bindir = os.path.join(vamb_outputs_base, 'dvae_bins')\n",
    "if not os.path.exists(bindir):\n",
    "    os.mkdir(bindir)\n",
    "vamb.vambtools.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vamb_env",
   "language": "python",
   "name": "vamb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
