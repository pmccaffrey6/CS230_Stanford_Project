{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1810085c50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vamb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam as Adam\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "EXAMPLE_FASTA_FILE = '2021.01.26_15.46.45_sample_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_inputs_base = os.path.join(BASE_DIR,'example_input_data/new_simulations/camisim_outputs/vamb_inputs')\n",
    "\n",
    "contignames = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'contignames.npz'))\n",
    "\n",
    "lengths = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'lengths.npz'))\n",
    "\n",
    "tnfs = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'tnfs.npz'))\n",
    "    \n",
    "rpkms = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'rpkms.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, mask = vamb.encode.make_dataloader(rpkms, tnfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncontigs, nsamples = dataloader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of contigs to encode: 1342\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of contigs to encode:\", ncontigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DISENTANGLED_VAE(torch.nn.Module):\n",
    "    def __init__(self, nsamples, nhiddens=[512, 512], nlatent=32, alpha=0.15,\n",
    "                 beta=1, dropout=0.2):\n",
    "        \n",
    "\n",
    "        super(DISENTANGLED_VAE, self).__init__()\n",
    "\n",
    "        # Initialize simple attributes\n",
    "        self.usecuda = True\n",
    "        self.nsamples = nsamples\n",
    "        self.ntnf = 103\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.nhiddens = nhiddens\n",
    "        self.nlatent = nlatent\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize lists for holding hidden layers\n",
    "        self.encoderlayers = torch.nn.ModuleList()\n",
    "        self.encodernorms = torch.nn.ModuleList()\n",
    "        self.decoderlayers = torch.nn.ModuleList()\n",
    "        self.decodernorms = torch.nn.ModuleList()\n",
    "\n",
    "        # Add all other hidden layers\n",
    "        for nin, nout in zip([self.nsamples + self.ntnf] + self.nhiddens, self.nhiddens):\n",
    "            self.encoderlayers.append(torch.nn.Linear(nin, nout))\n",
    "            self.encodernorms.append(torch.nn.BatchNorm1d(nout))\n",
    "\n",
    "        # Latent layers\n",
    "        self.mu = torch.nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "        self.logsigma = torch.nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "\n",
    "        # Add first decoding layer\n",
    "        for nin, nout in zip([self.nlatent] + self.nhiddens[::-1], self.nhiddens[::-1]):\n",
    "            self.decoderlayers.append(torch.nn.Linear(nin, nout))\n",
    "            self.decodernorms.append(torch.nn.BatchNorm1d(nout))\n",
    "\n",
    "        # Reconstruction (output) layer\n",
    "        self.outputlayer = torch.nn.Linear(self.nhiddens[0], self.nsamples + self.ntnf)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.dropoutlayer = torch.nn.Dropout(p=self.dropout)\n",
    "\n",
    "        if self.usecuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def _encode(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        # Hidden layers\n",
    "        for encoderlayer, encodernorm in zip(self.encoderlayers, self.encodernorms):\n",
    "            tensor = encodernorm(self.dropoutlayer(self.relu(encoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        # Latent layers\n",
    "        mu = self.mu(tensor)\n",
    "\n",
    "        logsigma = self.softplus(self.logsigma(tensor))\n",
    "\n",
    "        return mu, logsigma\n",
    "\n",
    "    # sample with gaussian noise\n",
    "    def reparameterize(self, mu, logsigma):\n",
    "        epsilon = torch.randn(mu.size(0), mu.size(1))\n",
    "\n",
    "        if self.usecuda:\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        epsilon.requires_grad = True\n",
    "\n",
    "        # See comment above regarding softplus\n",
    "        latent = mu + epsilon * torch.exp(logsigma/2)\n",
    "\n",
    "        return latent\n",
    "\n",
    "    def _decode(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        for decoderlayer, decodernorm in zip(self.decoderlayers, self.decodernorms):\n",
    "            tensor = decodernorm(self.dropoutlayer(self.relu(decoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        reconstruction = self.outputlayer(tensor)\n",
    "\n",
    "        # Decompose reconstruction to depths and tnf signal\n",
    "        depths_out = reconstruction.narrow(1, 0, self.nsamples)\n",
    "        tnf_out = reconstruction.narrow(1, self.nsamples, self.ntnf)\n",
    "\n",
    "        # If multiple samples, apply softmax\n",
    "        if self.nsamples > 1:\n",
    "            depths_out = _softmax(depths_out, dim=1)\n",
    "\n",
    "        return depths_out, tnf_out\n",
    "\n",
    "    def forward(self, depths, tnf):\n",
    "        tensor = torch.cat((depths, tnf), 1)\n",
    "        mu, logsigma = self._encode(tensor)\n",
    "        latent = self.reparameterize(mu, logsigma)\n",
    "        depths_out, tnf_out = self._decode(latent)\n",
    "\n",
    "        return depths_out, tnf_out, mu, logsigma\n",
    "\n",
    "    def calc_loss(self, depths_in, depths_out, tnf_in, tnf_out, mu, logsigma):\n",
    "        # If multiple samples, use cross entropy, else use SSE for abundance\n",
    "        if self.nsamples > 1:\n",
    "            # Add 1e-9 to depths_out to avoid numerical instability.\n",
    "            ce = - ((depths_out + 1e-9).log() * depths_in).sum(dim=1).mean()\n",
    "            ce_weight = (1 - self.alpha) / _log(self.nsamples)\n",
    "        else:\n",
    "            ce = (depths_out - depths_in).pow(2).sum(dim=1).mean()\n",
    "            ce_weight = 1 - self.alpha\n",
    "\n",
    "        sse = (tnf_out - tnf_in).pow(2).sum(dim=1).mean()\n",
    "        kld = -0.5 * (1 + logsigma - mu.pow(2) - logsigma.exp()).sum(dim=1).mean()\n",
    "\n",
    "        sse_weight = self.alpha / self.ntnf\n",
    "        kld_weight = 1 / (self.nlatent * self.beta)\n",
    "        loss = ce * ce_weight + sse * sse_weight + kld * kld_weight\n",
    "\n",
    "        return loss, ce, sse, kld\n",
    "\n",
    "    def trainepoch(self, data_loader, epoch, optimizer, batchsteps, logfile):\n",
    "        self.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_kldloss = 0\n",
    "        epoch_sseloss = 0\n",
    "        epoch_celoss = 0\n",
    "\n",
    "        if epoch in batchsteps:\n",
    "            data_loader = DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size * 2,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True,\n",
    "                                      num_workers=data_loader.num_workers,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        for depths_in, tnf_in in data_loader:\n",
    "            depths_in.requires_grad = True\n",
    "            tnf_in.requires_grad = True\n",
    "\n",
    "            if self.usecuda:\n",
    "                depths_in = depths_in.cuda()\n",
    "                tnf_in = tnf_in.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            depths_out, tnf_out, mu, logsigma = self(depths_in, tnf_in)\n",
    "\n",
    "            loss, ce, sse, kld = self.calc_loss(depths_in, depths_out, tnf_in,\n",
    "                                                  tnf_out, mu, logsigma)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.data.item()\n",
    "            epoch_kldloss += kld.data.item()\n",
    "            epoch_sseloss += sse.data.item()\n",
    "            epoch_celoss += ce.data.item()\n",
    "\n",
    "        print('\\tEpoch: {}\\tLoss: {:.6f}\\tCE: {:.7f}\\tSSE: {:.6f}\\tKLD: {:.4f}\\tBatchsize: {}'.format(\n",
    "              epoch + 1,\n",
    "              epoch_loss / len(data_loader),\n",
    "              epoch_celoss / len(data_loader),\n",
    "              epoch_sseloss / len(data_loader),\n",
    "              epoch_kldloss / len(data_loader),\n",
    "              data_loader.batch_size,\n",
    "              ), file=logfile)\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def encode(self, data_loader):\n",
    "        \"\"\"Encode a data loader to a latent representation with VAE\n",
    "        Input: data_loader: As generated by train_vae\n",
    "        Output: A (n_contigs x n_latent) Numpy array of latent repr.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        new_data_loader = DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False,\n",
    "                                      num_workers=1,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        depths_array, tnf_array = data_loader.dataset.tensors\n",
    "        length = len(depths_array)\n",
    "\n",
    "        # We make a Numpy array instead of a Torch array because, if we create\n",
    "        # a Torch array, then convert it to Numpy, Numpy will believe it doesn't\n",
    "        # own the memory block, and array resizes will not be permitted.\n",
    "        latent = np.empty((length, self.nlatent), dtype=np.float32)\n",
    "\n",
    "        row = 0\n",
    "        with torch.no_grad():\n",
    "            for depths, tnf in new_data_loader:\n",
    "                # Move input to GPU if requested\n",
    "                if self.usecuda:\n",
    "                    depths = depths.cuda()\n",
    "                    tnf = tnf.cuda()\n",
    "\n",
    "                # Evaluate\n",
    "                out_depths, out_tnf, mu, logsigma = self(depths, tnf)\n",
    "\n",
    "                if self.usecuda:\n",
    "                    mu = mu.cpu()\n",
    "\n",
    "                latent[row: row + len(mu)] = mu\n",
    "                row += len(mu)\n",
    "\n",
    "        assert row == length\n",
    "        return latent\n",
    "\n",
    "    def save(self, filehandle):\n",
    "        \"\"\"Saves the VAE to a path or binary opened file. Load with VAE.load\n",
    "        Input: Path or binary opened filehandle\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        state = {'nsamples': self.nsamples,\n",
    "                 'alpha': self.alpha,\n",
    "                 'beta': self.beta,\n",
    "                 'dropout': self.dropout,\n",
    "                 'nhiddens': self.nhiddens,\n",
    "                 'nlatent': self.nlatent,\n",
    "                 'state': self.state_dict(),\n",
    "                }\n",
    "\n",
    "        torch.save(state, filehandle)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, cuda=False, evaluate=True):\n",
    "        \"\"\"Instantiates a VAE from a model file.\n",
    "        Inputs:\n",
    "            path: Path to model file as created by functions VAE.save or\n",
    "                  VAE.trainmodel.\n",
    "            cuda: If network should work on GPU [False]\n",
    "            evaluate: Return network in evaluation mode [True]\n",
    "        Output: VAE with weights and parameters matching the saved network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Forcably load to CPU even if model was saves as GPU model\n",
    "        dictionary = _torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        nsamples = dictionary['nsamples']\n",
    "        alpha = dictionary['alpha']\n",
    "        beta = dictionary['beta']\n",
    "        dropout = dictionary['dropout']\n",
    "        nhiddens = dictionary['nhiddens']\n",
    "        nlatent = dictionary['nlatent']\n",
    "        state = dictionary['state']\n",
    "\n",
    "        vae = cls(nsamples, nhiddens, nlatent, alpha, beta, dropout, cuda)\n",
    "        vae.load_state_dict(state)\n",
    "\n",
    "        if cuda:\n",
    "            vae.cuda()\n",
    "\n",
    "        if evaluate:\n",
    "            vae.eval()\n",
    "\n",
    "        return vae\n",
    "\n",
    "    def trainmodel(self, dataloader, nepochs=500, lrate=1e-3,\n",
    "                   batchsteps=[25, 75, 150, 300], logfile=None, modelfile=None):\n",
    "\n",
    "        if batchsteps is None:\n",
    "            batchsteps_set = set()\n",
    "        else:\n",
    "            # First collect to list in order to allow all element types, then check that\n",
    "            # they are integers\n",
    "            batchsteps = list(batchsteps)\n",
    "            last_batchsize = dataloader.batch_size * 2**len(batchsteps)\n",
    "            batchsteps_set = set(batchsteps)\n",
    "\n",
    "        # Get number of features\n",
    "        ncontigs, nsamples = dataloader.dataset.tensors[0].shape\n",
    "        optimizer = Adam(self.parameters(), lr=lrate)\n",
    "\n",
    "        if logfile is not None:\n",
    "            print('\\tNetwork properties:', file=logfile)\n",
    "            print('\\tCUDA:', self.usecuda, file=logfile)\n",
    "            print('\\tAlpha:', self.alpha, file=logfile)\n",
    "            print('\\tBeta:', self.beta, file=logfile)\n",
    "            print('\\tDropout:', self.dropout, file=logfile)\n",
    "            print('\\tN hidden:', ', '.join(map(str, self.nhiddens)), file=logfile)\n",
    "            print('\\tN latent:', self.nlatent, file=logfile)\n",
    "            print('\\n\\tTraining properties:', file=logfile)\n",
    "            print('\\tN epochs:', nepochs, file=logfile)\n",
    "            print('\\tStarting batch size:', dataloader.batch_size, file=logfile)\n",
    "            batchsteps_string = ', '.join(map(str, sorted(batchsteps))) if batchsteps_set else \"None\"\n",
    "            print('\\tBatchsteps:', batchsteps_string, file=logfile)\n",
    "            print('\\tLearning rate:', lrate, file=logfile)\n",
    "            print('\\tN sequences:', ncontigs, file=logfile)\n",
    "            print('\\tN samples:', nsamples, file=logfile, end='\\n\\n')\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(nepochs):\n",
    "            dataloader = self.trainepoch(dataloader, epoch, optimizer, batchsteps_set, logfile)\n",
    "\n",
    "        # Save weights - Lord forgive me, for I have sinned when catching all exceptions\n",
    "        if modelfile is not None:\n",
    "            try:\n",
    "                self.save(modelfile)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNetwork properties:\n",
      "\tCUDA: True\n",
      "\tAlpha: 0.15\n",
      "\tBeta: 500\n",
      "\tDropout: 0.2\n",
      "\tN hidden: 512, 512\n",
      "\tN latent: 32\n",
      "\n",
      "\tTraining properties:\n",
      "\tN epochs: 300\n",
      "\tStarting batch size: 256\n",
      "\tBatchsteps: None\n",
      "\tLearning rate: 0.001\n",
      "\tN sequences: 1342\n",
      "\tN samples: 1\n",
      "\n",
      "\tEpoch: 1\tLoss: 1.106320\tCE: 1.0808577\tSSE: 128.126434\tKLD: 15.9823\tBatchsize: 256\n",
      "\tEpoch: 2\tLoss: 0.713972\tCE: 0.6711483\tSSE: 97.448474\tKLD: 25.2869\tBatchsize: 256\n",
      "\tEpoch: 3\tLoss: 0.497081\tCE: 0.4529748\tSSE: 75.506839\tKLD: 33.4514\tBatchsize: 256\n",
      "\tEpoch: 4\tLoss: 0.410468\tCE: 0.3632411\tSSE: 68.061415\tKLD: 41.5171\tBatchsize: 256\n",
      "\tEpoch: 5\tLoss: 0.348978\tCE: 0.2977701\tSSE: 63.764362\tKLD: 48.2049\tBatchsize: 256\n",
      "\tEpoch: 6\tLoss: 0.295852\tCE: 0.2446490\tSSE: 57.966893\tKLD: 55.7243\tBatchsize: 256\n",
      "\tEpoch: 7\tLoss: 0.247140\tCE: 0.1928451\tSSE: 54.521685\tKLD: 61.1459\tBatchsize: 256\n",
      "\tEpoch: 8\tLoss: 0.232613\tCE: 0.1764359\tSSE: 53.628680\tKLD: 72.6805\tBatchsize: 256\n",
      "\tEpoch: 9\tLoss: 0.222544\tCE: 0.1707169\tSSE: 49.914908\tKLD: 75.8941\tBatchsize: 256\n",
      "\tEpoch: 10\tLoss: 0.247010\tCE: 0.2028533\tSSE: 47.917924\tKLD: 76.8272\tBatchsize: 256\n",
      "\tEpoch: 11\tLoss: 0.220792\tCE: 0.1738255\tSSE: 46.819001\tKLD: 77.7096\tBatchsize: 256\n",
      "\tEpoch: 12\tLoss: 0.192964\tCE: 0.1436583\tSSE: 45.203080\tKLD: 80.3885\tBatchsize: 256\n",
      "\tEpoch: 13\tLoss: 0.180797\tCE: 0.1320269\tSSE: 43.601604\tKLD: 81.2297\tBatchsize: 256\n",
      "\tEpoch: 14\tLoss: 0.161636\tCE: 0.1107466\tSSE: 42.821793\tKLD: 82.2340\tBatchsize: 256\n",
      "\tEpoch: 15\tLoss: 0.173126\tCE: 0.1253940\tSSE: 42.047701\tKLD: 84.8996\tBatchsize: 256\n",
      "\tEpoch: 16\tLoss: 0.146090\tCE: 0.0950564\tSSE: 40.930266\tKLD: 90.9520\tBatchsize: 256\n",
      "\tEpoch: 17\tLoss: 0.137211\tCE: 0.0846562\tSSE: 40.817421\tKLD: 92.9620\tBatchsize: 256\n",
      "\tEpoch: 18\tLoss: 0.145846\tCE: 0.0958310\tSSE: 40.140253\tKLD: 94.9211\tBatchsize: 256\n",
      "\tEpoch: 19\tLoss: 0.134334\tCE: 0.0837696\tSSE: 39.252404\tKLD: 95.4592\tBatchsize: 256\n",
      "\tEpoch: 20\tLoss: 0.142679\tCE: 0.0943403\tSSE: 38.752611\tKLD: 96.8650\tBatchsize: 256\n",
      "\tEpoch: 21\tLoss: 0.120294\tCE: 0.0686888\tSSE: 38.276359\tKLD: 98.6577\tBatchsize: 256\n",
      "\tEpoch: 22\tLoss: 0.124180\tCE: 0.0731137\tSSE: 38.299947\tKLD: 100.1049\tBatchsize: 256\n",
      "\tEpoch: 23\tLoss: 0.127876\tCE: 0.0777878\tSSE: 38.004700\tKLD: 102.5475\tBatchsize: 256\n",
      "\tEpoch: 24\tLoss: 0.121389\tCE: 0.0705210\tSSE: 37.737006\tKLD: 103.8252\tBatchsize: 256\n",
      "\tEpoch: 25\tLoss: 0.131675\tCE: 0.0839243\tSSE: 36.928032\tKLD: 104.9671\tBatchsize: 256\n",
      "\tEpoch: 26\tLoss: 0.112772\tCE: 0.0623885\tSSE: 36.413556\tKLD: 107.3898\tBatchsize: 256\n",
      "\tEpoch: 27\tLoss: 0.114835\tCE: 0.0641476\tSSE: 36.776772\tKLD: 108.0122\tBatchsize: 256\n",
      "\tEpoch: 28\tLoss: 0.113074\tCE: 0.0627647\tSSE: 36.330383\tKLD: 109.0508\tBatchsize: 256\n",
      "\tEpoch: 29\tLoss: 0.109870\tCE: 0.0597170\tSSE: 35.996966\tKLD: 106.9996\tBatchsize: 256\n",
      "\tEpoch: 30\tLoss: 0.107762\tCE: 0.0573225\tSSE: 35.764661\tKLD: 111.2479\tBatchsize: 256\n",
      "\tEpoch: 31\tLoss: 0.123932\tCE: 0.0773967\tSSE: 35.119541\tKLD: 111.9969\tBatchsize: 256\n",
      "\tEpoch: 32\tLoss: 0.118262\tCE: 0.0690378\tSSE: 36.010211\tKLD: 114.2124\tBatchsize: 256\n",
      "\tEpoch: 33\tLoss: 0.114789\tCE: 0.0669717\tSSE: 34.829804\tKLD: 114.2453\tBatchsize: 256\n",
      "\tEpoch: 34\tLoss: 0.110838\tCE: 0.0619964\tSSE: 35.001727\tKLD: 114.6770\tBatchsize: 256\n",
      "\tEpoch: 35\tLoss: 0.108104\tCE: 0.0587991\tSSE: 34.967525\tKLD: 115.2183\tBatchsize: 256\n",
      "\tEpoch: 36\tLoss: 0.108030\tCE: 0.0589082\tSSE: 34.798251\tKLD: 116.4983\tBatchsize: 256\n",
      "\tEpoch: 37\tLoss: 0.113698\tCE: 0.0672887\tSSE: 33.700966\tKLD: 118.7766\tBatchsize: 256\n",
      "\tEpoch: 38\tLoss: 0.122763\tCE: 0.0777170\tSSE: 33.873343\tKLD: 117.9733\tBatchsize: 256\n",
      "\tEpoch: 39\tLoss: 0.098976\tCE: 0.0490581\tSSE: 34.241543\tKLD: 118.5658\tBatchsize: 256\n",
      "\tEpoch: 40\tLoss: 0.113022\tCE: 0.0665846\tSSE: 33.558758\tKLD: 120.8547\tBatchsize: 256\n",
      "\tEpoch: 41\tLoss: 0.121468\tCE: 0.0759081\tSSE: 33.954812\tKLD: 119.9538\tBatchsize: 256\n",
      "\tEpoch: 42\tLoss: 0.107342\tCE: 0.0598630\tSSE: 33.509373\tKLD: 122.5365\tBatchsize: 256\n",
      "\tEpoch: 43\tLoss: 0.107464\tCE: 0.0600417\tSSE: 33.386774\tKLD: 124.9150\tBatchsize: 256\n",
      "\tEpoch: 44\tLoss: 0.106411\tCE: 0.0597634\tSSE: 32.814764\tKLD: 125.1734\tBatchsize: 256\n",
      "\tEpoch: 45\tLoss: 0.093791\tCE: 0.0445288\tSSE: 33.003525\tKLD: 126.0467\tBatchsize: 256\n",
      "\tEpoch: 46\tLoss: 0.097735\tCE: 0.0490999\tSSE: 32.943438\tKLD: 128.3898\tBatchsize: 256\n",
      "\tEpoch: 47\tLoss: 0.103933\tCE: 0.0548028\tSSE: 33.940587\tKLD: 126.7625\tBatchsize: 256\n",
      "\tEpoch: 48\tLoss: 0.091657\tCE: 0.0416223\tSSE: 33.136285\tKLD: 128.3452\tBatchsize: 256\n",
      "\tEpoch: 49\tLoss: 0.091970\tCE: 0.0430377\tSSE: 32.502406\tKLD: 128.8634\tBatchsize: 256\n",
      "\tEpoch: 50\tLoss: 0.092306\tCE: 0.0428298\tSSE: 32.748001\tKLD: 131.3568\tBatchsize: 256\n",
      "\tEpoch: 51\tLoss: 0.094742\tCE: 0.0468602\tSSE: 32.194712\tKLD: 128.4033\tBatchsize: 256\n",
      "\tEpoch: 52\tLoss: 0.091098\tCE: 0.0424019\tSSE: 32.212730\tKLD: 130.3074\tBatchsize: 256\n",
      "\tEpoch: 53\tLoss: 0.090409\tCE: 0.0414829\tSSE: 32.179607\tKLD: 132.5531\tBatchsize: 256\n",
      "\tEpoch: 54\tLoss: 0.093519\tCE: 0.0453028\tSSE: 32.047223\tKLD: 133.4527\tBatchsize: 256\n",
      "\tEpoch: 55\tLoss: 0.093880\tCE: 0.0456168\tSSE: 32.188885\tKLD: 131.6560\tBatchsize: 256\n",
      "\tEpoch: 56\tLoss: 0.090461\tCE: 0.0417808\tSSE: 32.063713\tKLD: 132.0392\tBatchsize: 256\n",
      "\tEpoch: 57\tLoss: 0.088057\tCE: 0.0387778\tSSE: 31.982175\tKLD: 136.3181\tBatchsize: 256\n",
      "\tEpoch: 58\tLoss: 0.100382\tCE: 0.0541900\tSSE: 31.432707\tKLD: 136.7100\tBatchsize: 256\n",
      "\tEpoch: 59\tLoss: 0.089201\tCE: 0.0406668\tSSE: 31.792567\tKLD: 133.3467\tBatchsize: 256\n",
      "\tEpoch: 60\tLoss: 0.090563\tCE: 0.0418749\tSSE: 32.011292\tKLD: 133.6121\tBatchsize: 256\n",
      "\tEpoch: 61\tLoss: 0.087416\tCE: 0.0388443\tSSE: 31.474767\tKLD: 136.9834\tBatchsize: 256\n",
      "\tEpoch: 62\tLoss: 0.088838\tCE: 0.0403965\tSSE: 31.533996\tKLD: 137.2500\tBatchsize: 256\n",
      "\tEpoch: 63\tLoss: 0.107375\tCE: 0.0623908\tSSE: 31.486551\tKLD: 135.8233\tBatchsize: 256\n",
      "\tEpoch: 64\tLoss: 0.089294\tCE: 0.0417173\tSSE: 31.200034\tKLD: 134.3624\tBatchsize: 256\n",
      "\tEpoch: 65\tLoss: 0.089310\tCE: 0.0421970\tSSE: 30.955339\tKLD: 133.7891\tBatchsize: 256\n",
      "\tEpoch: 66\tLoss: 0.088364\tCE: 0.0411157\tSSE: 30.846114\tKLD: 135.8986\tBatchsize: 256\n",
      "\tEpoch: 67\tLoss: 0.090658\tCE: 0.0437927\tSSE: 30.931958\tKLD: 134.2009\tBatchsize: 256\n",
      "\tEpoch: 68\tLoss: 0.091964\tCE: 0.0453887\tSSE: 30.956747\tKLD: 132.8190\tBatchsize: 256\n",
      "\tEpoch: 69\tLoss: 0.095938\tCE: 0.0500181\tSSE: 30.813102\tKLD: 136.7812\tBatchsize: 256\n",
      "\tEpoch: 70\tLoss: 0.086379\tCE: 0.0384172\tSSE: 30.922005\tKLD: 139.0717\tBatchsize: 256\n",
      "\tEpoch: 71\tLoss: 0.095058\tCE: 0.0494697\tSSE: 30.589570\tKLD: 135.3673\tBatchsize: 256\n",
      "\tEpoch: 72\tLoss: 0.090199\tCE: 0.0434448\tSSE: 30.851073\tKLD: 133.4799\tBatchsize: 256\n",
      "\tEpoch: 73\tLoss: 0.095495\tCE: 0.0498094\tSSE: 30.679681\tKLD: 135.6395\tBatchsize: 256\n",
      "\tEpoch: 74\tLoss: 0.096182\tCE: 0.0508588\tSSE: 30.594432\tKLD: 134.3596\tBatchsize: 256\n",
      "\tEpoch: 75\tLoss: 0.092193\tCE: 0.0470995\tSSE: 30.103066\tKLD: 133.0980\tBatchsize: 256\n",
      "\tEpoch: 76\tLoss: 0.085186\tCE: 0.0375220\tSSE: 30.757274\tKLD: 136.0108\tBatchsize: 256\n",
      "\tEpoch: 77\tLoss: 0.086019\tCE: 0.0387274\tSSE: 30.553178\tKLD: 137.6901\tBatchsize: 256\n",
      "\tEpoch: 78\tLoss: 0.080643\tCE: 0.0331202\tSSE: 30.231531\tKLD: 135.4250\tBatchsize: 256\n",
      "\tEpoch: 79\tLoss: 0.082755\tCE: 0.0356508\tSSE: 30.237358\tKLD: 134.6613\tBatchsize: 256\n",
      "\tEpoch: 80\tLoss: 0.100480\tCE: 0.0564858\tSSE: 30.222265\tKLD: 135.2663\tBatchsize: 256\n",
      "\tEpoch: 81\tLoss: 0.086620\tCE: 0.0404914\tSSE: 30.043096\tKLD: 135.2077\tBatchsize: 256\n",
      "\tEpoch: 82\tLoss: 0.087988\tCE: 0.0425397\tSSE: 29.794613\tKLD: 135.0307\tBatchsize: 256\n",
      "\tEpoch: 83\tLoss: 0.082297\tCE: 0.0354213\tSSE: 30.008131\tKLD: 135.7990\tBatchsize: 256\n",
      "\tEpoch: 84\tLoss: 0.089004\tCE: 0.0436472\tSSE: 29.881671\tKLD: 134.1946\tBatchsize: 256\n",
      "\tEpoch: 85\tLoss: 0.082223\tCE: 0.0355028\tSSE: 29.848073\tKLD: 137.2433\tBatchsize: 256\n",
      "\tEpoch: 86\tLoss: 0.083641\tCE: 0.0366910\tSSE: 30.113490\tKLD: 137.5924\tBatchsize: 256\n",
      "\tEpoch: 87\tLoss: 0.086323\tCE: 0.0404533\tSSE: 29.780585\tKLD: 137.0868\tBatchsize: 256\n",
      "\tEpoch: 88\tLoss: 0.080745\tCE: 0.0330387\tSSE: 30.281203\tKLD: 137.0156\tBatchsize: 256\n",
      "\tEpoch: 89\tLoss: 0.084130\tCE: 0.0383255\tSSE: 29.557244\tKLD: 136.1421\tBatchsize: 256\n",
      "\tEpoch: 90\tLoss: 0.087144\tCE: 0.0414007\tSSE: 29.779786\tKLD: 137.3504\tBatchsize: 256\n",
      "\tEpoch: 91\tLoss: 0.081786\tCE: 0.0345730\tSSE: 30.076865\tKLD: 137.5602\tBatchsize: 256\n",
      "\tEpoch: 92\tLoss: 0.077102\tCE: 0.0303760\tSSE: 29.365967\tKLD: 136.2581\tBatchsize: 256\n",
      "\tEpoch: 93\tLoss: 0.081273\tCE: 0.0354488\tSSE: 29.291159\tKLD: 135.7453\tBatchsize: 256\n",
      "\tEpoch: 94\tLoss: 0.075493\tCE: 0.0287549\tSSE: 29.189084\tKLD: 136.6885\tBatchsize: 256\n",
      "\tEpoch: 95\tLoss: 0.079432\tCE: 0.0332392\tSSE: 29.233355\tKLD: 137.6936\tBatchsize: 256\n",
      "\tEpoch: 96\tLoss: 0.088237\tCE: 0.0435430\tSSE: 29.385680\tKLD: 134.8876\tBatchsize: 256\n",
      "\tEpoch: 97\tLoss: 0.092074\tCE: 0.0483002\tSSE: 29.275735\tKLD: 134.1474\tBatchsize: 256\n",
      "\tEpoch: 98\tLoss: 0.088506\tCE: 0.0443910\tSSE: 29.008789\tKLD: 136.4443\tBatchsize: 256\n",
      "\tEpoch: 99\tLoss: 0.080736\tCE: 0.0348625\tSSE: 29.238757\tKLD: 136.3598\tBatchsize: 256\n",
      "\tEpoch: 100\tLoss: 0.091410\tCE: 0.0481062\tSSE: 28.803326\tKLD: 137.1760\tBatchsize: 256\n",
      "\tEpoch: 101\tLoss: 0.081683\tCE: 0.0363635\tSSE: 28.952871\tKLD: 137.7464\tBatchsize: 256\n",
      "\tEpoch: 102\tLoss: 0.079025\tCE: 0.0333337\tSSE: 28.978746\tKLD: 135.8307\tBatchsize: 256\n",
      "\tEpoch: 103\tLoss: 0.081086\tCE: 0.0357278\tSSE: 29.057175\tKLD: 134.4143\tBatchsize: 256\n",
      "\tEpoch: 104\tLoss: 0.098890\tCE: 0.0567901\tSSE: 28.958343\tKLD: 135.1418\tBatchsize: 256\n",
      "\tEpoch: 105\tLoss: 0.085127\tCE: 0.0408168\tSSE: 28.826689\tKLD: 135.2394\tBatchsize: 256\n",
      "\tEpoch: 106\tLoss: 0.090246\tCE: 0.0471499\tSSE: 28.694486\tKLD: 134.0934\tBatchsize: 256\n",
      "\tEpoch: 107\tLoss: 0.085802\tCE: 0.0420821\tSSE: 28.598207\tKLD: 134.1548\tBatchsize: 256\n",
      "\tEpoch: 108\tLoss: 0.076439\tCE: 0.0309356\tSSE: 28.631943\tKLD: 135.1546\tBatchsize: 256\n",
      "\tEpoch: 109\tLoss: 0.083813\tCE: 0.0401000\tSSE: 28.453096\tKLD: 132.6566\tBatchsize: 256\n",
      "\tEpoch: 110\tLoss: 0.080351\tCE: 0.0352888\tSSE: 28.756200\tKLD: 135.6418\tBatchsize: 256\n",
      "\tEpoch: 111\tLoss: 0.083543\tCE: 0.0390006\tSSE: 28.790940\tKLD: 135.4177\tBatchsize: 256\n",
      "\tEpoch: 112\tLoss: 0.078257\tCE: 0.0336026\tSSE: 28.336301\tKLD: 134.8590\tBatchsize: 256\n",
      "\tEpoch: 113\tLoss: 0.078940\tCE: 0.0341625\tSSE: 28.465586\tKLD: 135.1571\tBatchsize: 256\n",
      "\tEpoch: 114\tLoss: 0.076984\tCE: 0.0316322\tSSE: 28.647690\tKLD: 134.0244\tBatchsize: 256\n",
      "\tEpoch: 115\tLoss: 0.080614\tCE: 0.0364123\tSSE: 28.296274\tKLD: 135.2905\tBatchsize: 256\n",
      "\tEpoch: 116\tLoss: 0.075465\tCE: 0.0307240\tSSE: 28.029043\tKLD: 136.4956\tBatchsize: 256\n",
      "\tEpoch: 117\tLoss: 0.078542\tCE: 0.0334067\tSSE: 28.690478\tKLD: 133.8207\tBatchsize: 256\n",
      "\tEpoch: 118\tLoss: 0.078086\tCE: 0.0334159\tSSE: 28.388906\tKLD: 133.4305\tBatchsize: 256\n",
      "\tEpoch: 119\tLoss: 0.080152\tCE: 0.0353908\tSSE: 28.609643\tKLD: 134.4772\tBatchsize: 256\n",
      "\tEpoch: 120\tLoss: 0.082457\tCE: 0.0391201\tSSE: 28.054630\tKLD: 133.5824\tBatchsize: 256\n",
      "\tEpoch: 121\tLoss: 0.081619\tCE: 0.0375708\tSSE: 28.429348\tKLD: 132.5119\tBatchsize: 256\n",
      "\tEpoch: 122\tLoss: 0.078401\tCE: 0.0340449\tSSE: 28.189234\tKLD: 134.5743\tBatchsize: 256\n",
      "\tEpoch: 123\tLoss: 0.076902\tCE: 0.0322834\tSSE: 28.195099\tKLD: 134.4113\tBatchsize: 256\n",
      "\tEpoch: 124\tLoss: 0.081724\tCE: 0.0380063\tSSE: 28.158418\tKLD: 134.5803\tBatchsize: 256\n",
      "\tEpoch: 125\tLoss: 0.081723\tCE: 0.0380328\tSSE: 28.171251\tKLD: 133.9045\tBatchsize: 256\n",
      "\tEpoch: 126\tLoss: 0.072358\tCE: 0.0274474\tSSE: 27.816819\tKLD: 136.2904\tBatchsize: 256\n",
      "\tEpoch: 127\tLoss: 0.074199\tCE: 0.0294272\tSSE: 27.959204\tKLD: 135.4928\tBatchsize: 256\n",
      "\tEpoch: 128\tLoss: 0.074054\tCE: 0.0289919\tSSE: 28.168363\tKLD: 134.2259\tBatchsize: 256\n",
      "\tEpoch: 129\tLoss: 0.074463\tCE: 0.0305142\tSSE: 27.621103\tKLD: 132.8111\tBatchsize: 256\n",
      "\tEpoch: 130\tLoss: 0.075786\tCE: 0.0314636\tSSE: 28.020634\tKLD: 131.7570\tBatchsize: 256\n",
      "\tEpoch: 131\tLoss: 0.073887\tCE: 0.0285450\tSSE: 28.405893\tKLD: 132.0991\tBatchsize: 256\n",
      "\tEpoch: 132\tLoss: 0.071457\tCE: 0.0268062\tSSE: 27.754707\tKLD: 132.0353\tBatchsize: 256\n",
      "\tEpoch: 133\tLoss: 0.075846\tCE: 0.0316803\tSSE: 27.951991\tKLD: 131.3697\tBatchsize: 256\n",
      "\tEpoch: 134\tLoss: 0.075850\tCE: 0.0317259\tSSE: 27.874160\tKLD: 132.6254\tBatchsize: 256\n",
      "\tEpoch: 135\tLoss: 0.075250\tCE: 0.0322012\tSSE: 27.232988\tKLD: 131.5084\tBatchsize: 256\n",
      "\tEpoch: 136\tLoss: 0.070855\tCE: 0.0264465\tSSE: 27.547804\tKLD: 132.1103\tBatchsize: 256\n",
      "\tEpoch: 137\tLoss: 0.072492\tCE: 0.0283474\tSSE: 27.556205\tKLD: 132.2646\tBatchsize: 256\n",
      "\tEpoch: 138\tLoss: 0.071723\tCE: 0.0279228\tSSE: 27.335026\tKLD: 130.8810\tBatchsize: 256\n",
      "\tEpoch: 139\tLoss: 0.074978\tCE: 0.0307266\tSSE: 27.958085\tKLD: 130.3168\tBatchsize: 256\n",
      "\tEpoch: 140\tLoss: 0.070859\tCE: 0.0259842\tSSE: 27.858675\tKLD: 131.2274\tBatchsize: 256\n",
      "\tEpoch: 141\tLoss: 0.068376\tCE: 0.0238971\tSSE: 27.384312\tKLD: 130.9363\tBatchsize: 256\n",
      "\tEpoch: 142\tLoss: 0.074467\tCE: 0.0309830\tSSE: 27.445083\tKLD: 130.6021\tBatchsize: 256\n",
      "\tEpoch: 143\tLoss: 0.074541\tCE: 0.0311534\tSSE: 27.426447\tKLD: 129.9106\tBatchsize: 256\n",
      "\tEpoch: 144\tLoss: 0.081896\tCE: 0.0403277\tSSE: 27.083260\tKLD: 130.8167\tBatchsize: 256\n",
      "\tEpoch: 145\tLoss: 0.075387\tCE: 0.0324812\tSSE: 27.234779\tKLD: 129.8546\tBatchsize: 256\n",
      "\tEpoch: 146\tLoss: 0.072850\tCE: 0.0294026\tSSE: 27.275802\tKLD: 130.1740\tBatchsize: 256\n",
      "\tEpoch: 147\tLoss: 0.074907\tCE: 0.0316576\tSSE: 27.368451\tKLD: 130.2524\tBatchsize: 256\n",
      "\tEpoch: 148\tLoss: 0.069029\tCE: 0.0253200\tSSE: 27.038779\tKLD: 130.0830\tBatchsize: 256\n",
      "\tEpoch: 149\tLoss: 0.068429\tCE: 0.0250062\tSSE: 26.791380\tKLD: 130.5092\tBatchsize: 256\n",
      "\tEpoch: 150\tLoss: 0.075510\tCE: 0.0332767\tSSE: 26.909671\tKLD: 128.5705\tBatchsize: 256\n",
      "\tEpoch: 151\tLoss: 0.071281\tCE: 0.0284836\tSSE: 26.813742\tKLD: 128.3376\tBatchsize: 256\n",
      "\tEpoch: 152\tLoss: 0.068911\tCE: 0.0255715\tSSE: 26.849442\tKLD: 129.1837\tBatchsize: 256\n",
      "\tEpoch: 153\tLoss: 0.072957\tCE: 0.0297600\tSSE: 27.177171\tKLD: 129.3155\tBatchsize: 256\n",
      "\tEpoch: 154\tLoss: 0.081907\tCE: 0.0414137\tSSE: 26.598101\tKLD: 127.5162\tBatchsize: 256\n",
      "\tEpoch: 155\tLoss: 0.075732\tCE: 0.0340279\tSSE: 26.655053\tKLD: 127.8473\tBatchsize: 256\n",
      "\tEpoch: 156\tLoss: 0.071410\tCE: 0.0288192\tSSE: 26.672193\tKLD: 129.1314\tBatchsize: 256\n",
      "\tEpoch: 157\tLoss: 0.069948\tCE: 0.0268723\tSSE: 26.862310\tKLD: 127.7936\tBatchsize: 256\n",
      "\tEpoch: 158\tLoss: 0.068740\tCE: 0.0258085\tSSE: 26.639781\tKLD: 128.1138\tBatchsize: 256\n",
      "\tEpoch: 159\tLoss: 0.070094\tCE: 0.0270668\tSSE: 26.829320\tKLD: 128.2456\tBatchsize: 256\n",
      "\tEpoch: 160\tLoss: 0.074639\tCE: 0.0326062\tSSE: 26.762334\tKLD: 127.1847\tBatchsize: 256\n",
      "\tEpoch: 161\tLoss: 0.073955\tCE: 0.0311564\tSSE: 27.183855\tKLD: 126.1409\tBatchsize: 256\n",
      "\tEpoch: 162\tLoss: 0.066138\tCE: 0.0223490\tSSE: 26.896834\tKLD: 127.5473\tBatchsize: 256\n",
      "\tEpoch: 163\tLoss: 0.066922\tCE: 0.0240422\tSSE: 26.512748\tKLD: 126.0020\tBatchsize: 256\n",
      "\tEpoch: 164\tLoss: 0.072448\tCE: 0.0302725\tSSE: 26.658549\tKLD: 126.2993\tBatchsize: 256\n",
      "\tEpoch: 165\tLoss: 0.071385\tCE: 0.0289981\tSSE: 26.787592\tKLD: 123.6013\tBatchsize: 256\n",
      "\tEpoch: 166\tLoss: 0.067598\tCE: 0.0248578\tSSE: 26.599478\tKLD: 123.7088\tBatchsize: 256\n",
      "\tEpoch: 167\tLoss: 0.065845\tCE: 0.0229147\tSSE: 26.517164\tKLD: 124.0042\tBatchsize: 256\n",
      "\tEpoch: 168\tLoss: 0.070333\tCE: 0.0279506\tSSE: 26.654851\tKLD: 124.1100\tBatchsize: 256\n",
      "\tEpoch: 169\tLoss: 0.072641\tCE: 0.0305789\tSSE: 26.677295\tKLD: 124.7782\tBatchsize: 256\n",
      "\tEpoch: 170\tLoss: 0.069605\tCE: 0.0278909\tSSE: 26.233683\tKLD: 123.1002\tBatchsize: 256\n",
      "\tEpoch: 171\tLoss: 0.069246\tCE: 0.0270627\tSSE: 26.511849\tKLD: 122.1302\tBatchsize: 256\n",
      "\tEpoch: 172\tLoss: 0.072578\tCE: 0.0307781\tSSE: 26.622457\tKLD: 122.3341\tBatchsize: 256\n",
      "\tEpoch: 173\tLoss: 0.071640\tCE: 0.0291448\tSSE: 26.936641\tKLD: 122.2223\tBatchsize: 256\n",
      "\tEpoch: 174\tLoss: 0.071374\tCE: 0.0301562\tSSE: 26.154702\tKLD: 122.4297\tBatchsize: 256\n",
      "\tEpoch: 175\tLoss: 0.072584\tCE: 0.0319028\tSSE: 25.954778\tKLD: 122.6951\tBatchsize: 256\n",
      "\tEpoch: 176\tLoss: 0.076741\tCE: 0.0365553\tSSE: 26.123291\tKLD: 121.9990\tBatchsize: 256\n",
      "\tEpoch: 177\tLoss: 0.069974\tCE: 0.0286887\tSSE: 26.007678\tKLD: 123.4099\tBatchsize: 256\n",
      "\tEpoch: 178\tLoss: 0.074207\tCE: 0.0326975\tSSE: 26.611753\tKLD: 122.5424\tBatchsize: 256\n",
      "\tEpoch: 179\tLoss: 0.068953\tCE: 0.0273669\tSSE: 26.149959\tKLD: 121.7426\tBatchsize: 256\n",
      "\tEpoch: 180\tLoss: 0.064847\tCE: 0.0223313\tSSE: 26.241912\tKLD: 122.3778\tBatchsize: 256\n",
      "\tEpoch: 181\tLoss: 0.085360\tCE: 0.0462628\tSSE: 26.354923\tKLD: 122.4974\tBatchsize: 256\n",
      "\tEpoch: 182\tLoss: 0.068947\tCE: 0.0274814\tSSE: 26.138794\tKLD: 120.3518\tBatchsize: 256\n",
      "\tEpoch: 183\tLoss: 0.067521\tCE: 0.0257388\tSSE: 26.159650\tKLD: 120.7436\tBatchsize: 256\n",
      "\tEpoch: 184\tLoss: 0.068441\tCE: 0.0270249\tSSE: 26.019435\tKLD: 121.2357\tBatchsize: 256\n",
      "\tEpoch: 185\tLoss: 0.066962\tCE: 0.0248562\tSSE: 26.293376\tKLD: 120.6926\tBatchsize: 256\n",
      "\tEpoch: 186\tLoss: 0.104523\tCE: 0.0698934\tSSE: 25.797381\tKLD: 120.7062\tBatchsize: 256\n",
      "\tEpoch: 187\tLoss: 0.071173\tCE: 0.0300253\tSSE: 26.178904\tKLD: 120.4295\tBatchsize: 256\n",
      "\tEpoch: 188\tLoss: 0.065642\tCE: 0.0242929\tSSE: 25.764381\tKLD: 119.5570\tBatchsize: 256\n",
      "\tEpoch: 189\tLoss: 0.068041\tCE: 0.0261587\tSSE: 26.185838\tKLD: 122.7487\tBatchsize: 256\n",
      "\tEpoch: 190\tLoss: 0.081255\tCE: 0.0428196\tSSE: 25.651263\tKLD: 120.0293\tBatchsize: 256\n",
      "\tEpoch: 191\tLoss: 0.073630\tCE: 0.0338674\tSSE: 25.608681\tKLD: 120.7775\tBatchsize: 256\n",
      "\tEpoch: 192\tLoss: 0.064625\tCE: 0.0234107\tSSE: 25.539178\tKLD: 120.5259\tBatchsize: 256\n",
      "\tEpoch: 193\tLoss: 0.067910\tCE: 0.0271948\tSSE: 25.657460\tKLD: 118.8743\tBatchsize: 256\n",
      "\tEpoch: 194\tLoss: 0.070950\tCE: 0.0295135\tSSE: 26.389738\tKLD: 118.9087\tBatchsize: 256\n",
      "\tEpoch: 195\tLoss: 0.073521\tCE: 0.0338476\tSSE: 25.650239\tKLD: 118.3399\tBatchsize: 256\n",
      "\tEpoch: 196\tLoss: 0.079999\tCE: 0.0416668\tSSE: 25.538941\tKLD: 118.2366\tBatchsize: 256\n",
      "\tEpoch: 197\tLoss: 0.065044\tCE: 0.0233555\tSSE: 25.951796\tKLD: 118.3750\tBatchsize: 256\n",
      "\tEpoch: 198\tLoss: 0.065207\tCE: 0.0243342\tSSE: 25.459394\tKLD: 119.1317\tBatchsize: 256\n",
      "\tEpoch: 199\tLoss: 0.066590\tCE: 0.0253509\tSSE: 25.773336\tKLD: 120.1211\tBatchsize: 256\n",
      "\tEpoch: 200\tLoss: 0.064190\tCE: 0.0228990\tSSE: 25.560874\tKLD: 120.0256\tBatchsize: 256\n",
      "\tEpoch: 201\tLoss: 0.072101\tCE: 0.0323277\tSSE: 25.554585\tKLD: 118.5107\tBatchsize: 256\n",
      "\tEpoch: 202\tLoss: 0.065132\tCE: 0.0243793\tSSE: 25.362190\tKLD: 119.5975\tBatchsize: 256\n",
      "\tEpoch: 203\tLoss: 0.069797\tCE: 0.0293406\tSSE: 25.642292\tKLD: 120.2317\tBatchsize: 256\n",
      "\tEpoch: 204\tLoss: 0.072374\tCE: 0.0328320\tSSE: 25.412022\tKLD: 119.3364\tBatchsize: 256\n",
      "\tEpoch: 205\tLoss: 0.069575\tCE: 0.0290392\tSSE: 25.693734\tKLD: 119.5774\tBatchsize: 256\n",
      "\tEpoch: 206\tLoss: 0.068778\tCE: 0.0285957\tSSE: 25.400245\tKLD: 119.6977\tBatchsize: 256\n",
      "\tEpoch: 207\tLoss: 0.065766\tCE: 0.0250323\tSSE: 25.414261\tKLD: 119.6367\tBatchsize: 256\n",
      "\tEpoch: 208\tLoss: 0.069225\tCE: 0.0293632\tSSE: 25.293122\tKLD: 118.9046\tBatchsize: 256\n",
      "\tEpoch: 209\tLoss: 0.073105\tCE: 0.0337047\tSSE: 25.331121\tKLD: 121.0595\tBatchsize: 256\n",
      "\tEpoch: 210\tLoss: 0.067726\tCE: 0.0279624\tSSE: 25.009702\tKLD: 120.5804\tBatchsize: 256\n",
      "\tEpoch: 211\tLoss: 0.078037\tCE: 0.0398068\tSSE: 25.224449\tKLD: 119.4665\tBatchsize: 256\n",
      "\tEpoch: 212\tLoss: 0.068583\tCE: 0.0278239\tSSE: 25.811835\tKLD: 117.4826\tBatchsize: 256\n",
      "\tEpoch: 213\tLoss: 0.086235\tCE: 0.0489935\tSSE: 25.493568\tKLD: 119.4253\tBatchsize: 256\n",
      "\tEpoch: 214\tLoss: 0.069897\tCE: 0.0300080\tSSE: 25.334387\tKLD: 119.9311\tBatchsize: 256\n",
      "\tEpoch: 215\tLoss: 0.067693\tCE: 0.0272161\tSSE: 25.529649\tKLD: 118.0918\tBatchsize: 256\n",
      "\tEpoch: 216\tLoss: 0.065885\tCE: 0.0255577\tSSE: 25.292118\tKLD: 117.2482\tBatchsize: 256\n",
      "\tEpoch: 217\tLoss: 0.073217\tCE: 0.0337581\tSSE: 25.479250\tKLD: 118.6770\tBatchsize: 256\n",
      "\tEpoch: 218\tLoss: 0.073102\tCE: 0.0338414\tSSE: 25.334525\tKLD: 119.0702\tBatchsize: 256\n",
      "\tEpoch: 219\tLoss: 0.067682\tCE: 0.0277633\tSSE: 25.189001\tKLD: 118.3968\tBatchsize: 256\n",
      "\tEpoch: 220\tLoss: 0.070650\tCE: 0.0310366\tSSE: 25.292220\tKLD: 118.9668\tBatchsize: 256\n",
      "\tEpoch: 221\tLoss: 0.069280\tCE: 0.0289836\tSSE: 25.524871\tKLD: 119.5542\tBatchsize: 256\n",
      "\tEpoch: 222\tLoss: 0.062665\tCE: 0.0223938\tSSE: 24.880148\tKLD: 118.3597\tBatchsize: 256\n",
      "\tEpoch: 223\tLoss: 0.068173\tCE: 0.0284887\tSSE: 25.093452\tKLD: 118.6221\tBatchsize: 256\n",
      "\tEpoch: 224\tLoss: 0.063444\tCE: 0.0232364\tSSE: 24.983076\tKLD: 116.9524\tBatchsize: 256\n",
      "\tEpoch: 225\tLoss: 0.061402\tCE: 0.0200193\tSSE: 25.434964\tKLD: 117.5061\tBatchsize: 256\n",
      "\tEpoch: 226\tLoss: 0.071051\tCE: 0.0318305\tSSE: 25.137555\tKLD: 118.1880\tBatchsize: 256\n",
      "\tEpoch: 227\tLoss: 0.068445\tCE: 0.0284995\tSSE: 25.382668\tKLD: 116.0843\tBatchsize: 256\n",
      "\tEpoch: 228\tLoss: 0.062650\tCE: 0.0217624\tSSE: 25.331157\tKLD: 116.1840\tBatchsize: 256\n",
      "\tEpoch: 229\tLoss: 0.062919\tCE: 0.0229549\tSSE: 24.852531\tKLD: 115.4300\tBatchsize: 256\n",
      "\tEpoch: 230\tLoss: 0.069016\tCE: 0.0298220\tSSE: 25.017941\tKLD: 115.7426\tBatchsize: 256\n",
      "\tEpoch: 231\tLoss: 0.065376\tCE: 0.0254192\tSSE: 25.110862\tKLD: 115.2033\tBatchsize: 256\n",
      "\tEpoch: 232\tLoss: 0.063080\tCE: 0.0225451\tSSE: 25.162946\tKLD: 116.3454\tBatchsize: 256\n",
      "\tEpoch: 233\tLoss: 0.071690\tCE: 0.0326459\tSSE: 25.194408\tKLD: 116.0019\tBatchsize: 256\n",
      "\tEpoch: 234\tLoss: 0.066172\tCE: 0.0269954\tSSE: 24.678362\tKLD: 116.5929\tBatchsize: 256\n",
      "\tEpoch: 235\tLoss: 0.078499\tCE: 0.0412545\tSSE: 24.866939\tKLD: 115.4973\tBatchsize: 256\n",
      "\tEpoch: 236\tLoss: 0.071800\tCE: 0.0333915\tSSE: 24.848145\tKLD: 115.6934\tBatchsize: 256\n",
      "\tEpoch: 237\tLoss: 0.060328\tCE: 0.0203518\tSSE: 24.544297\tKLD: 116.5612\tBatchsize: 256\n",
      "\tEpoch: 238\tLoss: 0.077151\tCE: 0.0391701\tSSE: 25.062019\tKLD: 117.7288\tBatchsize: 256\n",
      "\tEpoch: 239\tLoss: 0.070568\tCE: 0.0318208\tSSE: 24.936996\tKLD: 115.2744\tBatchsize: 256\n",
      "\tEpoch: 240\tLoss: 0.064704\tCE: 0.0248090\tSSE: 24.886621\tKLD: 117.9836\tBatchsize: 256\n",
      "\tEpoch: 241\tLoss: 0.079407\tCE: 0.0400743\tSSE: 24.898600\tKLD: 145.3331\tBatchsize: 256\n",
      "\tEpoch: 242\tLoss: 0.075623\tCE: 0.0375155\tSSE: 25.362424\tKLD: 108.7920\tBatchsize: 256\n",
      "\tEpoch: 243\tLoss: 0.078201\tCE: 0.0404063\tSSE: 25.557627\tKLD: 106.1748\tBatchsize: 256\n",
      "\tEpoch: 244\tLoss: 0.082456\tCE: 0.0453140\tSSE: 25.567419\tKLD: 107.2798\tBatchsize: 256\n",
      "\tEpoch: 245\tLoss: 0.079843\tCE: 0.0425442\tSSE: 25.404428\tKLD: 106.9466\tBatchsize: 256\n",
      "\tEpoch: 246\tLoss: 0.076861\tCE: 0.0387245\tSSE: 25.504779\tKLD: 108.8328\tBatchsize: 256\n",
      "\tEpoch: 247\tLoss: 0.074407\tCE: 0.0360229\tSSE: 25.383139\tKLD: 109.1481\tBatchsize: 256\n",
      "\tEpoch: 248\tLoss: 0.072120\tCE: 0.0331600\tSSE: 25.502858\tKLD: 108.7082\tBatchsize: 256\n",
      "\tEpoch: 249\tLoss: 0.075168\tCE: 0.0363575\tSSE: 25.525257\tKLD: 113.4644\tBatchsize: 256\n",
      "\tEpoch: 250\tLoss: 0.073960\tCE: 0.0351159\tSSE: 25.447841\tKLD: 112.8305\tBatchsize: 256\n",
      "\tEpoch: 251\tLoss: 0.084374\tCE: 0.0480003\tSSE: 25.133300\tKLD: 111.5570\tBatchsize: 256\n",
      "\tEpoch: 252\tLoss: 0.068132\tCE: 0.0282615\tSSE: 25.383062\tKLD: 114.3035\tBatchsize: 256\n",
      "\tEpoch: 253\tLoss: 0.068420\tCE: 0.0291962\tSSE: 24.999810\tKLD: 115.1312\tBatchsize: 256\n",
      "\tEpoch: 254\tLoss: 0.078926\tCE: 0.0406121\tSSE: 25.660015\tKLD: 112.5924\tBatchsize: 256\n",
      "\tEpoch: 255\tLoss: 0.065856\tCE: 0.0264667\tSSE: 24.935910\tKLD: 112.7166\tBatchsize: 256\n",
      "\tEpoch: 256\tLoss: 0.066011\tCE: 0.0263471\tSSE: 25.066779\tKLD: 113.7737\tBatchsize: 256\n",
      "\tEpoch: 257\tLoss: 0.061552\tCE: 0.0213589\tSSE: 24.869651\tKLD: 114.8586\tBatchsize: 256\n",
      "\tEpoch: 258\tLoss: 0.062807\tCE: 0.0226859\tSSE: 24.988349\tKLD: 114.1232\tBatchsize: 256\n",
      "\tEpoch: 259\tLoss: 0.079097\tCE: 0.0418476\tSSE: 24.987757\tKLD: 114.1783\tBatchsize: 256\n",
      "\tEpoch: 260\tLoss: 0.065481\tCE: 0.0264467\tSSE: 24.699096\tKLD: 112.5009\tBatchsize: 256\n",
      "\tEpoch: 261\tLoss: 0.075168\tCE: 0.0375882\tSSE: 24.903915\tKLD: 111.2019\tBatchsize: 256\n",
      "\tEpoch: 262\tLoss: 0.074523\tCE: 0.0364867\tSSE: 25.030639\tKLD: 112.9141\tBatchsize: 256\n",
      "\tEpoch: 263\tLoss: 0.072669\tCE: 0.0349921\tSSE: 24.614172\tKLD: 113.2798\tBatchsize: 256\n",
      "\tEpoch: 264\tLoss: 0.067726\tCE: 0.0293228\tSSE: 24.559954\tKLD: 112.5564\tBatchsize: 256\n",
      "\tEpoch: 265\tLoss: 0.063044\tCE: 0.0238273\tSSE: 24.497729\tKLD: 113.8346\tBatchsize: 256\n",
      "\tEpoch: 266\tLoss: 0.061719\tCE: 0.0221419\tSSE: 24.554604\tKLD: 114.2276\tBatchsize: 256\n",
      "\tEpoch: 267\tLoss: 0.063876\tCE: 0.0242480\tSSE: 24.801977\tKLD: 114.3294\tBatchsize: 256\n",
      "\tEpoch: 268\tLoss: 0.067850\tCE: 0.0291639\tSSE: 24.676648\tKLD: 113.9749\tBatchsize: 256\n",
      "\tEpoch: 269\tLoss: 0.062480\tCE: 0.0231873\tSSE: 24.505179\tKLD: 113.3361\tBatchsize: 256\n",
      "\tEpoch: 270\tLoss: 0.069004\tCE: 0.0297726\tSSE: 25.043630\tKLD: 115.6121\tBatchsize: 256\n",
      "\tEpoch: 271\tLoss: 0.064400\tCE: 0.0244045\tSSE: 25.048991\tKLD: 114.8395\tBatchsize: 256\n",
      "\tEpoch: 272\tLoss: 0.065569\tCE: 0.0266324\tSSE: 24.532384\tKLD: 115.2680\tBatchsize: 256\n",
      "\tEpoch: 273\tLoss: 0.066989\tCE: 0.0278269\tSSE: 24.731038\tKLD: 117.1239\tBatchsize: 256\n",
      "\tEpoch: 274\tLoss: 0.073850\tCE: 0.0362605\tSSE: 24.601006\tKLD: 115.2300\tBatchsize: 256\n",
      "\tEpoch: 275\tLoss: 0.062954\tCE: 0.0238831\tSSE: 24.268757\tKLD: 116.9746\tBatchsize: 256\n",
      "\tEpoch: 276\tLoss: 0.075047\tCE: 0.0376478\tSSE: 24.481062\tKLD: 118.3024\tBatchsize: 256\n",
      "\tEpoch: 277\tLoss: 0.061359\tCE: 0.0221590\tSSE: 24.202272\tKLD: 116.4519\tBatchsize: 256\n",
      "\tEpoch: 278\tLoss: 0.066189\tCE: 0.0273658\tSSE: 24.513480\tKLD: 115.6630\tBatchsize: 256\n",
      "\tEpoch: 279\tLoss: 0.069332\tCE: 0.0301545\tSSE: 25.055762\tKLD: 115.3820\tBatchsize: 256\n",
      "\tEpoch: 280\tLoss: 0.061231\tCE: 0.0218145\tSSE: 24.383487\tKLD: 114.8584\tBatchsize: 256\n",
      "\tEpoch: 281\tLoss: 0.067069\tCE: 0.0283784\tSSE: 24.534161\tKLD: 115.4909\tBatchsize: 256\n",
      "\tEpoch: 282\tLoss: 0.062454\tCE: 0.0240890\tSSE: 23.967517\tKLD: 113.1941\tBatchsize: 256\n",
      "\tEpoch: 283\tLoss: 0.065641\tCE: 0.0270561\tSSE: 24.384058\tKLD: 114.1241\tBatchsize: 256\n",
      "\tEpoch: 284\tLoss: 0.067111\tCE: 0.0287908\tSSE: 24.283420\tKLD: 116.4017\tBatchsize: 256\n",
      "\tEpoch: 285\tLoss: 0.074689\tCE: 0.0381893\tSSE: 24.098452\tKLD: 114.1402\tBatchsize: 256\n",
      "\tEpoch: 286\tLoss: 0.077091\tCE: 0.0403229\tSSE: 24.546495\tKLD: 113.1089\tBatchsize: 256\n",
      "\tEpoch: 287\tLoss: 0.082212\tCE: 0.0456006\tSSE: 24.882166\tKLD: 115.4528\tBatchsize: 256\n",
      "\tEpoch: 288\tLoss: 0.074530\tCE: 0.0379464\tSSE: 24.111657\tKLD: 114.5891\tBatchsize: 256\n",
      "\tEpoch: 289\tLoss: 0.063582\tCE: 0.0239619\tSSE: 24.744632\tKLD: 114.8550\tBatchsize: 256\n",
      "\tEpoch: 290\tLoss: 0.066304\tCE: 0.0276267\tSSE: 24.473288\tKLD: 114.8924\tBatchsize: 256\n",
      "\tEpoch: 291\tLoss: 0.062550\tCE: 0.0235905\tSSE: 24.246054\tKLD: 115.0144\tBatchsize: 256\n",
      "\tEpoch: 292\tLoss: 0.063806\tCE: 0.0250956\tSSE: 24.225137\tKLD: 115.1207\tBatchsize: 256\n",
      "\tEpoch: 293\tLoss: 0.067067\tCE: 0.0284555\tSSE: 24.471842\tKLD: 115.8539\tBatchsize: 256\n",
      "\tEpoch: 294\tLoss: 0.064627\tCE: 0.0260756\tSSE: 24.167201\tKLD: 116.2844\tBatchsize: 256\n",
      "\tEpoch: 295\tLoss: 0.061295\tCE: 0.0223514\tSSE: 24.059016\tKLD: 116.1466\tBatchsize: 256\n",
      "\tEpoch: 296\tLoss: 0.059356\tCE: 0.0197055\tSSE: 24.233260\tKLD: 117.0356\tBatchsize: 256\n",
      "\tEpoch: 297\tLoss: 0.064695\tCE: 0.0259514\tSSE: 24.251756\tKLD: 117.0843\tBatchsize: 256\n",
      "\tEpoch: 298\tLoss: 0.072354\tCE: 0.0347688\tSSE: 24.398993\tKLD: 116.2866\tBatchsize: 256\n",
      "\tEpoch: 299\tLoss: 0.062943\tCE: 0.0245421\tSSE: 23.942068\tKLD: 115.4503\tBatchsize: 256\n",
      "\tEpoch: 300\tLoss: 0.060902\tCE: 0.0221742\tSSE: 23.889825\tKLD: 116.2076\tBatchsize: 256\n"
     ]
    }
   ],
   "source": [
    "dvae = DISENTANGLED_VAE(nsamples=rpkms.shape[1], beta=500)\n",
    "\n",
    "with open('vamb_models/model_disentangled_vae.pt', 'wb') as modelfile:\n",
    "    dvae.trainmodel(dataloader, nepochs=300, modelfile=modelfile, batchsteps=None, logfile=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1342, 32)\n"
     ]
    }
   ],
   "source": [
    "latent = dvae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)\n",
    "\n",
    "latent_output_path = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs/latent_space.npy')\n",
    "with open(latent_output_path, 'wb') as outfile:\n",
    "    np.save(outfile, latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: S0C22042 (of type: <class 'numpy.str_'> )\n",
      "Type of values: <class 'set'>\n",
      "First element of value: S0C38307 of type: <class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "filtered_labels = [n for (n,m) in zip(contignames, mask) if m]\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=filtered_labels)\n",
    "clusters = dict(cluster_iterator)\n",
    "\n",
    "medoid, contigs = next(iter(clusters.items()))\n",
    "print('First key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.1950665 , -1.3595657 ,  0.02780213, ...,  4.1902733 ,\n",
       "        -0.37895864, -2.005854  ],\n",
       "       [-6.3082957 , -4.037858  ,  1.8664149 , ..., -0.84718364,\n",
       "        -0.63913274, -1.527633  ],\n",
       "       [-4.793302  ,  0.09538189,  2.4154003 , ..., -1.1848402 ,\n",
       "        -2.937792  ,  0.02344322],\n",
       "       ...,\n",
       "       [-3.4301424 , -0.25371101,  1.0673652 , ..., -3.8149714 ,\n",
       "        -1.0946165 , -1.3135829 ],\n",
       "       [ 1.7739197 , -0.6077919 ,  1.2968498 , ...,  6.5359535 ,\n",
       "        -1.8472372 , -2.1588275 ],\n",
       "       [ 3.5661583 ,  1.167413  ,  1.2282975 , ..., -0.12444055,\n",
       "        -1.3878995 ,  0.3081694 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before splitting and filtering: 460\n",
      "Number of bins after splitting and filtering: 8\n"
     ]
    }
   ],
   "source": [
    "def filterclusters(clusters, lengthof):\n",
    "    filtered_bins = dict()\n",
    "    for medoid, contigs in clusters.items():\n",
    "        binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "        if binsize >= 50000:\n",
    "            filtered_bins[medoid] = contigs\n",
    "    \n",
    "    return filtered_bins\n",
    "        \n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "filtered_bins = filterclusters(vamb.vambtools.binsplit(clusters, 'C'), lengthof)\n",
    "print('Number of bins before splitting and filtering:', len(clusters))\n",
    "print('Number of bins after splitting and filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_mapping_table = pd.read_csv(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/gsa_mapping.tsv\"), sep='\\t')\n",
    "\n",
    "contig_mapping_output_path = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs/encoding_mapping.tsv')    \n",
    "\n",
    "contig_mapping_table[contig_mapping_table['#anonymous_contig_id'].isin(contignames)].reset_index().drop('index', axis=1).set_index(\n",
    "    '#anonymous_contig_id').reindex(contignames).to_csv(contig_mapping_output_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_outputs_base = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs')\n",
    "\n",
    "if not os.path.exists(vamb_outputs_base):\n",
    "    os.mkdir(vamb_outputs_base)\n",
    "    \n",
    "\n",
    "# This writes a .tsv file with the clusters and corresponding sequences\n",
    "with open(os.path.join(vamb_outputs_base, 'clusters_dvae.tsv'), 'w') as file:\n",
    "    vamb.vambtools.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "keptcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "\n",
    "\n",
    "# decompress fasta.gz if present\n",
    "fasta_path = os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta.gz\")\n",
    "if os.path.exists(fasta_path) and not os.path.exists(fasta_path.replace('.fasta.gz','.fasta')):\n",
    "    !gzip -dk $fasta_path\n",
    "\n",
    "\n",
    "with open(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta\"), 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=keptcontigs)\n",
    "\n",
    "\n",
    "bindir = os.path.join(vamb_outputs_base, 'dvae_bins')\n",
    "if not os.path.exists(bindir):\n",
    "    os.mkdir(bindir)\n",
    "    \n",
    "    \n",
    "files = glob.glob(os.path.join(bindir,'*'))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "vamb.vambtools.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vamb_env",
   "language": "python",
   "name": "vamb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
