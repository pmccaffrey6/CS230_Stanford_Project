{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fadb8f52c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vamb\n",
    "\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam as Adam\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import wandb\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "EXAMPLE_FASTA_FILE = '2021.01.26_15.46.45_sample_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_inputs_base = os.path.join(BASE_DIR,'example_input_data/new_simulations/camisim_outputs/vamb_inputs')\n",
    "\n",
    "contignames = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'contignames.npz'))\n",
    "\n",
    "lengths = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'lengths.npz'))\n",
    "\n",
    "tnfs = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'tnfs.npz'))\n",
    "    \n",
    "rpkms = vamb.vambtools.read_npz(os.path.join(vamb_inputs_base, 'rpkms.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt Through DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthssum = rpkms.sum(axis=1)\n",
    "mask = tnfs.sum(axis=1) != 0\n",
    "mask &= depthssum != 0\n",
    "depthssum = depthssum[mask]\n",
    "\n",
    "rpkm = rpkms[mask].astype(np.float32, copy=False)\n",
    "tnf = tnfs[mask].astype(np.float32, copy=False)\n",
    "\n",
    "## lkj\n",
    "def calculate_z_score(array):\n",
    "    array_mean = array.mean(axis=0)\n",
    "    array_std = array.std(axis=0)\n",
    "\n",
    "    shape = np.copy(array.shape)\n",
    "    shape[0] = 1\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    array_mean.shape = shape\n",
    "    array_mean.shape = shape\n",
    "\n",
    "    array = (array - array_mean) / array_std\n",
    "    \n",
    "    return(array)\n",
    "    \n",
    "rpkm = calculate_z_score(rpkm)\n",
    "tnf = calculate_z_score(tnf)\n",
    "depthstensor = torch.from_numpy(rpkm)\n",
    "tnftensor = torch.from_numpy(tnf)\n",
    "\n",
    "n_workers = 4\n",
    "\n",
    "dataset = TensorDataset(depthstensor, tnftensor)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=256, drop_last=True,\n",
    "                             shuffle=True, num_workers=n_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "ncontigs, nsamples = dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DISENTANGLED_BETA_VAE(torch.nn.Module):\n",
    "    def __init__(self, nsamples, config):\n",
    "        super(DISENTANGLED_BETA_VAE, self).__init__()\n",
    "  \n",
    "        # SET UP AND CONFIGURE THE MODEL\n",
    "        self.ntnf = tnfs.shape[1]\n",
    "        \n",
    "        self.nlatent = config.nlatent\n",
    "        self.dropout = config.dropout\n",
    "        self.learning_rate = config.learning_rate\n",
    "        self.alpha = config.alpha\n",
    "        self.beta = config.beta\n",
    "        self.nepochs = config.nepochs\n",
    "        \n",
    "        nhiddens = [512, 512]\n",
    "        \n",
    "        self.nsamples = nsamples\n",
    "        self.cuda_on = False\n",
    "        \n",
    "        if self.cuda_on:\n",
    "            self.cuda()\n",
    "\n",
    "        self.encoderlayers = torch.nn.ModuleList()\n",
    "        self.encodernorms = torch.nn.ModuleList()\n",
    "        self.decoderlayers = torch.nn.ModuleList()\n",
    "        self.decodernorms = torch.nn.ModuleList()\n",
    "\n",
    "\n",
    "        # ENCODER LAYERS\n",
    "        self.encoderlayers.append( torch.nn.Linear((self.nsamples + self.ntnf), 512) )\n",
    "        self.encodernorms.append( torch.nn.BatchNorm1d(512) )\n",
    "\n",
    "        self.encoderlayers.append( torch.nn.Linear(512, 512) )\n",
    "        self.encodernorms.append( torch.nn.BatchNorm1d(512) )\n",
    "\n",
    "\n",
    "        # LATENT LAYERS\n",
    "        self.mu = torch.nn.Linear(512, self.nlatent)\n",
    "        self.logsigma = torch.nn.Linear(512, self.nlatent)\n",
    "\n",
    "\n",
    "        # DECODER LAYRS\n",
    "        self.decoderlayers.append(torch.nn.Linear(self.nlatent, 512))\n",
    "        self.decodernorms.append(torch.nn.BatchNorm1d(512))\n",
    "\n",
    "        self.decoderlayers.append(torch.nn.Linear(512, 512))\n",
    "        self.decodernorms.append(torch.nn.BatchNorm1d(512))\n",
    "\n",
    "\n",
    "        # RECONSTRUCTION LAYER\n",
    "        self.outputlayer = torch.nn.Linear(512, (self.nsamples + self.ntnf) )\n",
    "\n",
    "\n",
    "        # ACTIVATIONS\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.dropoutlayer = torch.nn.Dropout(p=self.dropout)\n",
    "\n",
    "        \n",
    "    ###\n",
    "    # ENCODE NEW CONTIGS TO LATENT SPACE\n",
    "    ###\n",
    "    def encode(self, data_loader):\n",
    "        self.eval()\n",
    "\n",
    "        new_data_loader = DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False,\n",
    "                                      num_workers=4,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        depths_array, tnf_array = data_loader.dataset.tensors\n",
    "        length = len(depths_array)\n",
    "\n",
    "        latent = np.empty((length, self.nlatent), dtype=np.float32)\n",
    "\n",
    "        row = 0\n",
    "        with torch.no_grad():\n",
    "            for depths, tnf in new_data_loader:\n",
    "                if self.cuda_on:\n",
    "                    depths = depths.cuda()\n",
    "                    tnf = tnf.cuda()\n",
    "\n",
    "                # Evaluate\n",
    "                out_depths, out_tnf, mu, logsigma = self(depths, tnf)\n",
    "\n",
    "                if self.cuda_on:\n",
    "                    mu = mu.cpu()\n",
    "\n",
    "                latent[row: row + len(mu)] = mu\n",
    "                row += len(mu)\n",
    "\n",
    "        assert row == length\n",
    "        return latent\n",
    "    \n",
    "    ###\n",
    "    # SPECIFIC ENCODING AND DECODING FUNCTIONS\n",
    "    ###\n",
    "    # REPARAMATERIZE\n",
    "    def reparameterize(self, mu, logsigma):\n",
    "        epsilon = torch.randn(mu.size(0), mu.size(1))\n",
    "\n",
    "        if self.cuda_on:\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        epsilon.requires_grad = True\n",
    "\n",
    "        # See comment above regarding softplus\n",
    "        latent = mu + epsilon * torch.exp(logsigma/2)\n",
    "\n",
    "        return latent\n",
    "    \n",
    "    \n",
    "    # ENCODE CONTIGS\n",
    "    def encode_contigs(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        # Hidden layers\n",
    "        for encoderlayer, encodernorm in zip(self.encoderlayers, self.encodernorms):\n",
    "            tensor = encodernorm(self.dropoutlayer(self.relu(encoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        # Latent layers\n",
    "        mu = self.mu(tensor)\n",
    "        logsigma = self.softplus(self.logsigma(tensor))\n",
    "\n",
    "        return mu, logsigma\n",
    "    \n",
    "    \n",
    "    # DECODE CONTIGS\n",
    "    def decode_contigs(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        for decoderlayer, decodernorm in zip(self.decoderlayers, self.decodernorms):\n",
    "            tensor = decodernorm(self.dropoutlayer(self.relu(decoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        reconstruction = self.outputlayer(tensor)\n",
    "\n",
    "        # Decompose reconstruction to depths and tnf signal\n",
    "        depths_out = reconstruction.narrow(1, 0, self.nsamples)\n",
    "        tnf_out = reconstruction.narrow(1, self.nsamples, tnfs.shape[1])\n",
    "\n",
    "        return depths_out, tnf_out\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # LOSS CALCULATION\n",
    "    ###\n",
    "    # CALCULATE LOSS\n",
    "    def calc_loss(self, depths_in, depths_out, tnf_in, tnf_out, mu, logsigma):\n",
    "        ce = (depths_out - depths_in).pow(2).sum(dim=1).mean()\n",
    "        ce_weight = 1 - 0.15 # alpha\n",
    "\n",
    "        sse = (tnf_out - tnf_in).pow(2).sum(dim=1).mean()\n",
    "        kld = -0.5 * (1 + logsigma - mu.pow(2) - logsigma.exp()).sum(dim=1).mean()\n",
    "\n",
    "        sse_weight = 0.15 / self.ntnf # alpha / ntnf\n",
    "        # BETA PARAMETER HERE\n",
    "        kld_weight = 1 / (self.nlatent * self.beta)\n",
    "        loss = ce * ce_weight + sse * sse_weight + kld * kld_weight\n",
    "\n",
    "        return loss, ce, sse, kld\n",
    "    \n",
    "\n",
    "    ###\n",
    "    # TRAINING FUNCTIONS\n",
    "    ###\n",
    "    # FORWARD\n",
    "    def forward(self, depths, tnf):\n",
    "        tensor = torch.cat((depths, tnf), 1)\n",
    "        mu, logsigma = self.encode_contigs(tensor)\n",
    "        latent = self.reparameterize(mu, logsigma)\n",
    "        depths_out, tnf_out = self.decode_contigs(latent)\n",
    "\n",
    "        return depths_out, tnf_out, mu, logsigma   \n",
    "        \n",
    "     \n",
    "    \n",
    "    # TRAIN SPECIFIC EPOCH\n",
    "    def trainepoch(self, data_loader, epoch, optimizer, batchsteps):\n",
    "        self.train()\n",
    "\n",
    "        epoch_loss, epoch_kldloss, epoch_sseloss, epoch_celoss = 0, 0, 0, 0\n",
    "\n",
    "        if epoch in batchsteps:\n",
    "            data_loader = DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size * 2,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True,\n",
    "                                      num_workers=data_loader.num_workers,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        for depths_in, tnf_in in data_loader:\n",
    "            depths_in.requires_grad = True\n",
    "            tnf_in.requires_grad = True\n",
    "\n",
    "            # CUDA ENABLING\n",
    "            if self.cuda_on:\n",
    "                depths_in = depths_in.cuda()\n",
    "                tnf_in = tnf_in.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            depths_out, tnf_out, mu, logsigma = self(depths_in, tnf_in)\n",
    "\n",
    "            loss, ce, sse, kld = self.calc_loss(depths_in, depths_out, tnf_in,\n",
    "                                                  tnf_out, mu, logsigma)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss = epoch_loss + loss.data.item()\n",
    "            epoch_kldloss = epoch_kldloss + kld.data.item()\n",
    "            epoch_sseloss = epoch_sseloss + sse.data.item()\n",
    "            epoch_celoss = epoch_celoss + ce.data.item()\n",
    "\n",
    "        print('\\tEpoch: {}\\tLoss: {:.6f}\\tCE: {:.7f}\\tSSE: {:.6f}\\tKLD: {:.4f}\\tBatchsize: {}'.format(\n",
    "              epoch + 1,\n",
    "              epoch_loss / len(data_loader),\n",
    "              epoch_celoss / len(data_loader),\n",
    "              epoch_sseloss / len(data_loader),\n",
    "              epoch_kldloss / len(data_loader),\n",
    "              data_loader.batch_size,\n",
    "              ))\n",
    "        wandb.log({\n",
    "            \"epoch\": (epoch+1), \n",
    "            \"loss\": epoch_loss / len(data_loader),\n",
    "            \"CELoss\": epoch_celoss / len(data_loader),\n",
    "            \"SSELoss\": epoch_sseloss / len(data_loader),\n",
    "            \"KLDLoss\": epoch_kldloss / len(data_loader),\n",
    "            \"Batchsize\": data_loader.batch_size\n",
    "        })\n",
    "\n",
    "        return data_loader\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TRAIN MODEL    \n",
    "    def trainmodel(self, dataloader, batchsteps=[25, 75, 150, 300], modelfile=None):\n",
    "        \n",
    "        batchsteps_set = set()\n",
    "        \n",
    "        ncontigs, nsamples = dataloader.dataset.tensors[0].shape\n",
    "        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        \n",
    "        # TRAIN EPOCH\n",
    "        for epoch in range(self.nepochs):\n",
    "            dataloader = self.trainepoch(dataloader, epoch, optimizer, batchsteps_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 4t8iicmb\n",
      "Sweep URL: https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4v085q82 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnlatent: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">amber-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/runs/4v085q82\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/runs/4v085q82</a><br/>\n",
       "                Run data is saved locally in <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124043-4v085q82</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 1\tLoss: 7.789081\tCE: 6.8864641\tSSE: 136.909744\tKLD: 22223.3975\tBatchsize: 256\n",
      "\tEpoch: 2\tLoss: 1.811571\tCE: 1.4561278\tSSE: 128.357265\tKLD: 4952.7538\tBatchsize: 256\n",
      "\tEpoch: 3\tLoss: 1.086201\tCE: 1.0144000\tSSE: 93.712982\tKLD: 1119.8180\tBatchsize: 256\n",
      "\tEpoch: 4\tLoss: 0.942016\tCE: 0.9212229\tSSE: 83.734911\tKLD: 474.0213\tBatchsize: 256\n",
      "\tEpoch: 5\tLoss: 0.759025\tCE: 0.6718147\tSSE: 71.402379\tKLD: 1075.1774\tBatchsize: 256\n",
      "rundir: /home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124043-4v085q82/files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34037<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5954296142dc464395f6d4429ed6d344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124043-4v085q82/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124043-4v085q82/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>0.75902</td></tr><tr><td>CELoss</td><td>0.67181</td></tr><tr><td>SSELoss</td><td>71.40238</td></tr><tr><td>KLDLoss</td><td>1075.17744</td></tr><tr><td>Batchsize</td><td>256</td></tr><tr><td>_runtime</td><td>5</td></tr><tr><td>_timestamp</td><td>1615574448</td></tr><tr><td>_step</td><td>4</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▂▁▁▁</td></tr><tr><td>CELoss</td><td>█▂▁▁▁</td></tr><tr><td>SSELoss</td><td>█▇▃▂▁</td></tr><tr><td>KLDLoss</td><td>█▂▁▁▁</td></tr><tr><td>Batchsize</td><td>▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▅▅██</td></tr><tr><td>_timestamp</td><td>▁▅▅██</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">amber-sweep-1</strong>: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/runs/4v085q82\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/runs/4v085q82</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pzq1gak3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnlatent: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpmccaffrey6\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">daily-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/runs/pzq1gak3\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/runs/pzq1gak3</a><br/>\n",
       "                Run data is saved locally in <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124054-pzq1gak3</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 1\tLoss: 1.382310\tCE: 1.3895778\tSSE: 136.772055\tKLD: 25.4240\tBatchsize: 256\n",
      "\tEpoch: 2\tLoss: 1.286904\tCE: 1.2762968\tSSE: 137.376575\tKLD: 25.4554\tBatchsize: 256\n",
      "\tEpoch: 3\tLoss: 1.320336\tCE: 1.3168200\tSSE: 136.684402\tKLD: 25.3900\tBatchsize: 256\n",
      "\tEpoch: 4\tLoss: 1.252808\tCE: 1.2375339\tSSE: 136.582260\tKLD: 25.5730\tBatchsize: 256\n",
      "\tEpoch: 5\tLoss: 1.307951\tCE: 1.3029193\tSSE: 136.285968\tKLD: 25.5372\tBatchsize: 256\n",
      "rundir: /home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124054-pzq1gak3/files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34167<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167dd684ddff4875a547a91035a4021f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124054-pzq1gak3/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124054-pzq1gak3/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>1.30795</td></tr><tr><td>CELoss</td><td>1.30292</td></tr><tr><td>SSELoss</td><td>136.28597</td></tr><tr><td>KLDLoss</td><td>25.53721</td></tr><tr><td>Batchsize</td><td>256</td></tr><tr><td>_runtime</td><td>4</td></tr><tr><td>_timestamp</td><td>1615574458</td></tr><tr><td>_step</td><td>4</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▃▅▁▄</td></tr><tr><td>CELoss</td><td>█▃▅▁▄</td></tr><tr><td>SSELoss</td><td>▄█▄▃▁</td></tr><tr><td>KLDLoss</td><td>▂▄▁█▇</td></tr><tr><td>Batchsize</td><td>▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▅▅▅█</td></tr><tr><td>_timestamp</td><td>▁▅▅▅█</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">daily-sweep-2</strong>: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/runs/pzq1gak3\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/runs/pzq1gak3</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mx66bv25 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnlatent: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">tough-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/runs/mx66bv25\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/runs/mx66bv25</a><br/>\n",
       "                Run data is saved locally in <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124103-mx66bv25</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 1\tLoss: 1.359319\tCE: 1.3629724\tSSE: 136.523907\tKLD: 25.2294\tBatchsize: 256\n",
      "\tEpoch: 2\tLoss: 1.284897\tCE: 1.2755204\tSSE: 136.459439\tKLD: 25.3030\tBatchsize: 256\n",
      "\tEpoch: 3\tLoss: 1.356670\tCE: 1.3596638\tSSE: 136.636572\tKLD: 25.2163\tBatchsize: 256\n",
      "\tEpoch: 4\tLoss: 1.368580\tCE: 1.3750954\tSSE: 135.811417\tKLD: 25.1574\tBatchsize: 256\n",
      "\tEpoch: 5\tLoss: 1.308759\tCE: 1.3027259\tSSE: 136.972925\tKLD: 25.1814\tBatchsize: 256\n",
      "rundir: /home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124103-mx66bv25/files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34289<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4761c777f1414eb01aee4e2546806a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124103-mx66bv25/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124103-mx66bv25/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>1.30876</td></tr><tr><td>CELoss</td><td>1.30273</td></tr><tr><td>SSELoss</td><td>136.97292</td></tr><tr><td>KLDLoss</td><td>25.18136</td></tr><tr><td>Batchsize</td><td>256</td></tr><tr><td>_runtime</td><td>3</td></tr><tr><td>_timestamp</td><td>1615574466</td></tr><tr><td>_step</td><td>4</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>▇▁▇█▃</td></tr><tr><td>CELoss</td><td>▇▁▇█▃</td></tr><tr><td>SSELoss</td><td>▅▅▆▁█</td></tr><tr><td>KLDLoss</td><td>▄█▄▁▂</td></tr><tr><td>Batchsize</td><td>▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▅▅██</td></tr><tr><td>_timestamp</td><td>▁▅▅██</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">tough-sweep-3</strong>: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/runs/mx66bv25\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/runs/mx66bv25</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4w18k5ry with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnlatent: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">splendid-sweep-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/sweeps/4t8iicmb</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/pmccaffrey6/cs_230_vae/runs/4w18k5ry\" target=\"_blank\">https://wandb.ai/pmccaffrey6/cs_230_vae/runs/4w18k5ry</a><br/>\n",
       "                Run data is saved locally in <code>/home/pathinformatics/jupyter_projects/vamb/stanford_cs230_project/wandb/run-20210312_124112-4w18k5ry</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 1\tLoss: 6.617038\tCE: 6.9429825\tSSE: 138.622148\tKLD: 6574.4154\tBatchsize: 256\n",
      "\tEpoch: 2\tLoss: 2.507680\tCE: 2.0788482\tSSE: 124.260435\tKLD: 7164.1227\tBatchsize: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb_on = True\n",
    "sweep_on = True\n",
    "\n",
    "\n",
    "if wandb_on:\n",
    "    \n",
    "    #beta_vae = DISENTANGLED_BETA_VAE(nsamples=rpkms.shape[1], config=config)\n",
    "    #wandb.watch(beta_vae)\n",
    "    #beta_vae.trainmodel(dataloader, batchsteps=None)\n",
    "\n",
    "    if sweep_on:\n",
    "        sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {\n",
    "          'name': 'loss',\n",
    "          'goal': 'minimize'   \n",
    "            },\n",
    "            'parameters': {\n",
    "                'nepochs': {\n",
    "                    'values': [5]\n",
    "                },\n",
    "                'dropout': {\n",
    "                    'values': [0.2, 0.4, 0.6]\n",
    "                },\n",
    "                'learning_rate': {\n",
    "                    'values': [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "                },\n",
    "                'alpha': {\n",
    "                    'values': [0.15]\n",
    "                },\n",
    "                'beta': {\n",
    "                    'values': [1, 200, 400, 800]\n",
    "                },\n",
    "                'nlatent': {\n",
    "                    'values': [32, 64]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "        def train():\n",
    "            config_defaults = {\n",
    "                'nepochs': 5,\n",
    "                'dropout': 0.2,\n",
    "                'learning_rate': 1e-3,\n",
    "                'alpha': 0.15,\n",
    "                'beta': 200,\n",
    "                'nlatent': 32\n",
    "            }\n",
    "            \n",
    "            wandb.init(project='cs_230_vae', entity='pmccaffrey6', config=config_defaults)\n",
    "            \n",
    "            config = wandb.config\n",
    "            beta_vae = DISENTANGLED_BETA_VAE(nsamples=rpkms.shape[1], config=config)\n",
    "            wandb.watch(beta_vae)\n",
    "            beta_vae.trainmodel(dataloader, batchsteps=None)\n",
    "            print('rundir:', wandb.run.dir)\n",
    "            torch.save(beta_vae.state_dict(), os.path.join(wandb.run.dir, \"model.h5\"))\n",
    "            \n",
    "\n",
    "        sweep_id = wandb.sweep(sweep_config, entity=\"pmccaffrey6\", project=\"cs_230_vae\")\n",
    "        wandb.agent(sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best run volcanic-sweep-7 with 1.3094314575195312% validation accuracy\n",
      "best_params_dict: {'beta': {'value': 200, 'desc': None}, 'alpha': {'value': 0.15, 'desc': None}, 'dropout': {'value': 0.6, 'desc': None}, 'nepochs': {'value': 5, 'desc': None}, 'nlatent': {'value': 32, 'desc': None}, 'learning_rate': {'value': 1e-05, 'desc': None}}\n"
     ]
    }
   ],
   "source": [
    "# GET THE BEST MODEL'S PARAMS FROM THE WANDB API\n",
    "api = wandb.Api()\n",
    "print()\n",
    "sweep = api.sweep(f\"pmccaffrey6/cs_230_vae/{sweep_id}\")\n",
    "\n",
    "runs = sorted(sweep.runs, key=lambda run: run.summary.get(\"loss\", 0), reverse=True)\n",
    "loss = runs[0].summary.get(\"loss\", 0)\n",
    "\n",
    "best_run = runs[0].name\n",
    "\n",
    "print(f\"Best run {best_run} with {loss}% validation accuracy\")\n",
    "best_params_dict = json.loads(runs[0].json_config)\n",
    "print('best_params_dict:', best_params_dict)\n",
    "\n",
    "def convert(dictionary):\n",
    "    return namedtuple('GenericDict', dictionary.keys())(**dictionary)\n",
    "\n",
    "best_params = convert( {key:best_params_dict[key]['value'] for key in best_params_dict.keys()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='./model.h5' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs[0].file(\"model.h5\").download(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape: (1342, 32)\n"
     ]
    }
   ],
   "source": [
    "beta_vae = DISENTANGLED_BETA_VAE(nsamples=rpkms.shape[1], config=best_params)\n",
    "beta_vae.load_state_dict(torch.load('model.h5'))\n",
    "\n",
    "latent = beta_vae.encode(dataloader)\n",
    "print(\"Latent shape:\", latent.shape)\n",
    "\n",
    "latent_output_path = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs/latent_space.npy')\n",
    "with open(latent_output_path, 'wb') as outfile:\n",
    "    np.save(outfile, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leverage Other VAMB Tools for Clustering and Post Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_mapping_table = pd.read_csv(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/gsa_mapping.tsv\"), sep='\\t')\n",
    "\n",
    "contig_mapping_output_path = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs/encoding_mapping.tsv')    \n",
    "\n",
    "contig_mapping_table[contig_mapping_table['#anonymous_contig_id'].isin(contignames)].reset_index().drop('index', axis=1).set_index(\n",
    "    '#anonymous_contig_id').reindex(contignames).to_csv(contig_mapping_output_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: S0C34345 (of type: <class 'numpy.str_'> )\n",
      "Type of values: <class 'set'>\n",
      "First element of value: S0C34345 of type: <class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "filtered_labels = [n for (n,m) in zip(contignames, mask) if m]\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=filtered_labels)\n",
    "clusters = dict(cluster_iterator)\n",
    "\n",
    "medoid, contigs = next(iter(clusters.items()))\n",
    "print('First key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before splitting and filtering: 1163\n",
      "Number of bins after splitting and filtering: 3\n"
     ]
    }
   ],
   "source": [
    "def filterclusters(clusters, lengthof):\n",
    "    filtered_bins = dict()\n",
    "    for medoid, contigs in clusters.items():\n",
    "        binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "        if binsize >= 200000:\n",
    "            filtered_bins[medoid] = contigs\n",
    "    \n",
    "    return filtered_bins\n",
    "        \n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "filtered_bins = filterclusters(vamb.vambtools.binsplit(clusters, 'C'), lengthof)\n",
    "print('Number of bins before splitting and filtering:', len(clusters))\n",
    "print('Number of bins after splitting and filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamb_outputs_base = os.path.join(BASE_DIR, 'example_input_data/new_simulations/camisim_outputs/vamb_outputs')\n",
    "\n",
    "if not os.path.exists(vamb_outputs_base):\n",
    "    os.mkdir(vamb_outputs_base)\n",
    "    \n",
    "\n",
    "# This writes a .tsv file with the clusters and corresponding sequences\n",
    "with open(os.path.join(vamb_outputs_base, 'clusters_dvae.tsv'), 'w') as file:\n",
    "    vamb.vambtools.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "keptcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "\n",
    "\n",
    "# decompress fasta.gz if present\n",
    "fasta_path = os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta.gz\")\n",
    "if os.path.exists(fasta_path) and not os.path.exists(fasta_path.replace('.fasta.gz','.fasta')):\n",
    "    !gzip -dk $fasta_path\n",
    "\n",
    "\n",
    "with open(os.path.join(BASE_DIR, f\"example_input_data/new_simulations/camisim_outputs/{EXAMPLE_FASTA_FILE}/contigs/anonymous_gsa.fasta\"), 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=keptcontigs)\n",
    "\n",
    "\n",
    "bindir = os.path.join(vamb_outputs_base, 'dvae_bins')\n",
    "if not os.path.exists(bindir):\n",
    "    os.mkdir(bindir)\n",
    "vamb.vambtools.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vamb_env",
   "language": "python",
   "name": "vamb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
